# Table of Contents

<details>
<summary>▾ Expand ▴ Collapse</summary>

* [_start:](#_start)

* [setu_arch](#setup_arch)
    * [setup_machine_fdt](#setup_machine_fdt)
    * [arm64_memblock_init](#arm64_memblock_init)
    * [paging_init](#paging_init)
    * [bootmem_init](#bootmem_init)

* [numa](#numa)
    * [numa_init](#numa_init)
    * [node](#node)
    * [zone](#zone)
        * [zone_sizes_init](#zone_sizes_init)
        * [build_all_zonelists](#build_all_zonelists)
    * [page](#page)

* [sparsemem](#sparsemem)
    * [sparse_init](#sparse_init)
    * [vmemmap_populate](#vmemmap_populate)
    * [sparse_add_section](#sparse_add_section)
    * [sparse_remove_section](#sparse_remove_section)

* [mm_core_init](#mm_core_init)

* [memblock](#memblock)
    * [memblock_add](#memblock_add)
    * [memmap_remove](#memmap_remove)
    * [memmap_free](#memmap_free)
    * [memblock_free_all](#memblock_free_all)
* [hotplug](#hotplug)
    * [add_memory](#add_memory)
    * [remove_memory](#remove_memory)
    * [memory_subsys](#memory_subsys)
        * [memory_block_online](#memory_block_online)
        * [memory_block_offline](#memory_block_offline)

* [segment](#segment)
* [paging](#paging)
* [user virtual space](#user-virtual-space)
* [kernel virtual space](#kernel-virtual-space)

* [alloc_pages](#alloc_pages)
    * [prepare_alloc_pages](#prepare_alloc_pages)
    * [get_page_from_freelist](#get_page_from_freelist)
        * [rmqueue](#rmqueue)
        * [rmqueue_pcplist](#rmqueue_pcplist)
        * [rmqueue_fallback](#rmqueue_fallback)
        * [node_reclaim](#node_reclaim)
    * [alloc_pages_slowpath](#alloc_pages_slowpath)
        * [alloc_pages_direct_compact](#alloc_pages_direct_compact)
        * [alloc_pages_direct_reclaim](#alloc_pages_direct_reclaim)
        * [alloc_pages_may_oom](#alloc_pages_may_oom)

* [free_pages](#free_pages)
    * [free_frozen_pages](#free_frozen_pages)
    * [free_one_page](#free_one_page)

* [kmem_cache](#kmem_cache)
    * [kmem_cache_create](#kmem_cache_create)
        * [calculate_sizes](#calculate_sizes)
    * [kmem_cache_destroy](#kmem_cache_destroy)

* [slab](#slab)
    * [slab_alloc](#slab_alloc)
* [slub](#slub)
    * [slub_alloc](#slub_alloc)
    * [slub_free](#slub_free)
* [kernel mapping](#kernel-mapping)
* [kmalloc](#kmalloc)
    * [kmalloc_caches](#kmalloc_caches)
* [kmap_atomic](#kmap_atomic)
* [vmalloc](#vmalloc)
* [cma](#cma)
    * [rmem_cma_setup](#rmem_cma_setup)
    * [cma_init_reserved_areas](#cma_init_reserved_areas)
    * [cma_alloc](#cma_alloc)
* [rmap](#rmap)

* [brk](#brk)

* [pgd_mapping](#pgd_mapping)
    * [create_pgd_mapping](#create_pgd_mapping)
    * [remove_pgd_mapping](#remove_pgd_mapping)

* [mmap](#mmap)
    * [get_unmapped_area](#get_unmapped_area)
    * [mmap_region](#mmap_region)
    * [mm_populate](#mm_populate)

* [page_fault](#page_fault)
    * [do_pte_missing](#do_pte_missing)
        * [do_anonymous_page](#do_anonymous_page)
        * [do_fault](#do_fault)
            * [do_read_fault](#do_read_fault)
            * [do_cow_fault](#do_cow_fault)
            * [do_sharaed_fault](#do_shared_fault)
    * [do_swap_page](#do_swap_page)
    * [do_wp_page](#do_wp_page)
    * [do_numa_page](#do_numa_page)
    * [do_kernel_fault](#do_kernel_fault)
    * [hugetlb_fault](#hugetlb_fault)

* [munmap](#munmap)

* [mmu_gather](#mmu_gather)
    * [tlb_gather_mmu](#tlb_gather_mmu)
    * [unmap_vmas](#unmap_vmas)
    * [free_pgtables](#free_pgtables)
    * [tlb_finish_mmu](#tlb_finish_mmu)

* [mremap](#mremap)
    * [anon_vma_prepare](#anon_vma_prepare)
    * [anon_vma_fork](#anon_vma_fork)
    * [try_to_unmap](#try_to_unmap)

* [page_reclaim](#page_reclaim)
    * [lru](#lru)
    * [folio_batch](#folio_batch)
    * [workingset](#workingset)
    * [throttle_direct_reclaim](#throttle_direct_reclaim)
    * [shrink_node](#shrink_node)
        * [shrink_node_memcgs](#shrink_node_memcgs)
            * [get_scan_count](#get_scan_count)
        * [shrink_active_list](#shrink_active_list)
            * [isolate_lru_folios](#isolate_lru_folios)
            * [move_folios_to_lru](#move_folios_to_lru)
        * [shrink_inactive_list](#shrink_inactive_list)
            * [shrink_folio_list](#shrink_folio_list)
                * [remove_mapping](#remove_mapping)
        * [shrink_slab](#shrink_slab)
            * [shrink_slab_memcg](#shrink_slab_memcg)
    * [vmpressure](#vmpressure)

* [page_compact](#page_compact)
    * [compact_finished](#compact_finished)
    * [isolate_migratepages](#isolate_migratepages)
        * [fast_find_migrateblock](#fast_find_migrateblock)
        * [isolate_migratepages_block](#isolate_migratepages_block)
    * [isolate_freepages](#isolate_freepages)
        * [fast_isolate_freepages](#fast_isolate_freepages)
        * [isolate_freepages_block](#isolate_freepages_block)

* [page_migrate](#page_migrate)
    * [migreate_pages_batch](#migreate_pages_batch)
        * [migrate_folio_unmap](#migrate_folio_unmap)
            * [try_to_migrate](#try_to_migrate)
        * [migrate_folio_move](#migrate_folio_move)
            * [migrate_folio](#migrate_folio)
        * [remove_migration_ptes](#remove_migration_ptes)

* [kcompactd](#kcompactd)
* [kswapd](#kswapd)

* [swap](#swap)
    * [swapon](#swapon)
        * [setup_swap_map_and_extents](#setup_swap_map_and_extents)
    * [swapoff](#swapoff)
        * [swap_discard_work](#swap_discard_work)
    * [folio_alloc_swap](#folio_alloc_swap)
    * [add_to_swap](#add_to_swap)
        * [get_swap_pages](#get_swap_pages)
        * [swap_alloc_cluster](#swap_alloc_cluster)
        * [scan_swap_map_slots](#scan_swap_map_slots)
            * [scan_swap_map_try_ssd_cluster](#scan_swap_map_try_ssd_cluster)
    * [swap_free](#swap_free)
    * [folio_free_swap](#folio_free_swap)

* [fork](#fork)
    * [copy_page_range](#copy_page_range)

* [out_of_memory](#out_of_memory)
    * [oom_reaper](#oom_reaper)
    * [select_bad_process](#select_bad_process)
    * [oom_kill_process](#oom_kill_process)

* [exit_mm](#exit_mm)

* [dma]
    * [dma_alloc_coherent](#dma_alloc_coherent)
        * [dma_direct_alloc](#dma_direct_alloc)
        * [iommu_dma_alloc](#iommu_dma_alloc)

* [hugetlb]
    * [arm64_hugetlb_cma_reserve](#arm64_hugetlb_cma_alloc)
    * [hugetlb_init](#hugetlb_init)
        * [gather_bootmem_prealloc_parallel](#gather_bootmem_prealloc_parallel)
    * [hugatble_hstat_alloc_pages](#hugetlb_hstate_alloc_pages)
        * [hugetlb_gigantic_pages_alloc_boot](#hugetlb_gigantic_pages_alloc_boot)
        * [hugetlb_pages_alloc_boot](#hugetlb_pages_alloc_boot)
            * [alloc_gigantic_folio](#alloc_gigantic_folio)
                * [alloc_contig_range_noprof](#alloc_contig_range_noprof)
            * [alloc_buddy_hugetlb_folio](#alloc_buddy_hugetlb_folio)
    * [alloc_hugetlb_folio](#alloc_hugetlb_folio)
        * [dequeue_hugetlb_folio_vma](#dequeue_hugetlb_folio_vma)
        * [alloc_buddy_hugetlb_folio_with_mpol](#alloc_buddy_hugetlb_folio_with_mpol)
        * [hugetlb_vmemmap_optimize_folio](#hugetlb_vmemmap_optimize_folio)
    * [hugetlbfs]
        * [hugetlbfs_create](#hugetlbfs_create)
        * [hugetlbfs_file_map](#hugetlb_file_map)
        * [hugetlb_reserve_pages](#hugetlb_reserve_pages)
    * [khugepaged](#khugepaged)
        * [hpage_collapse_scan_file](#hpage_collapse_scan_file)
            * [collpase_file](#collpase_file)
        * [collapse_pte_mapped_thp](#collapse_pte_mapped_thp)
        * [hpage_collapse_scan_pmd](#hpage_collapse_scan_pmd)
            * [collapse_huge_page](#collapse_huge_page)
    * [split_huge_page](#split_huge_page)

</details>

![](../images/kernel/kernel-structual.svg)

# Doc

* [git.mm](https://git.kernel.org/pub/scm/linux/kernel/git/akpm/mm.git/)
* [lore.mm](https://lore.kernel.org/linux-mm/)
* [:link: Linux - Memory Management Documentation](https://docs.kernel.org/mm/index.html)
    * [Memory Layout on AArch64 Linux](https://docs.kernel.org/arch/arm64/memory.html)

* Folio
    * [LWN Series - Folio](https://lwn.net/Kernel/Index/#Memory_management-Folios)
    * [YouTube - Large Pages in the Linux Kernel - 2020.12.03 Matthew Wilcox, Oracle](https://www.youtube.com/watch?v=hoSpvGxXgNg)
    * [YouTuBe - Folios - 2022.6.20 Matthew Wilcox](https://www.youtube.com/watch?v=URTuP6wXYPA)
    * [YouTube - Memory Folios - 2022.6.23 Matthew Wilcox, Oracle](https://www.youtube.com/watch?v=nknQML80w3E)
    * [论好名字的重要性： Linux内核page到folio的变迁](https://mp.weixin.qq.com/s/4XnyOCSQwf6NGXY8RIAI0A)
* [LWN - Compund Page](https://lwn.net/Kernel/Index/#Memory_management-Compound_pages)
* [wowo tech - memory management :cn:](http://www.wowotech.net/sort/memory_management)
    * [ARM64 Kernel Image Mapping](http://www.wowotech.net/memory_management/436.html)
    * [Memory Model: FLATE, SPARSE](http://www.wowotech.net/memory_management/memory_model.html)
    * [Fix Map](http://www.wowotech.net/memory_management/fixmap.html)
    * [TLB Flush](http://www.wowotech.net/memory_management/tlb-flush.html)
    * [Memory Init :one:](http://www.wowotech.net/memory_management/mm-init-1.html) ⊙ [:two: - identity mapping & kernel image mapping ](http://www.wowotech.net/memory_management/__create_page_tables_code_analysis.html) ⊙ [:three: - Memory Layout](http://www.wowotech.net/memory_management/memory-layout.html) ⊙ [:four: - mapping](http://www.wowotech.net/memory_management/mem_init_3.html)
    * [CMA](http://www.wowotech.net/memory_management/cma.html)
    * [Dynamic DMA mapping Guide](http://www.wowotech.net/memory_management/DMA-Mapping-api.html)
    * [Page Reclaim](http://www.wowotech.net/memory_management/page_reclaim_basic.html)
    * [Rmap](http://www.wowotech.net/memory_management/reverse_mapping.html)
    * [copy_from_user](http://www.wowotech.net/memory_management/454.html)
    * [Consistent memory model](http://www.wowotech.net/memory_management/456.html)
    * [Cache Memory](http://www.wowotech.net/memory_management/458.html)
* [LoyenWang :cn:]()
    * [ARM V8 MMU and mem mapping](https://www.cnblogs.com/LoyenWang/p/11406693.html)
    * [Physical memory init](https://www.cnblogs.com/LoyenWang/p/11440957.html)
    * [paging_init](https://www.cnblogs.com/LoyenWang/p/11483948.html)
    * [sparse memory model](https://www.cnblogs.com/LoyenWang/p/11523678.html)
    * [zone_sizes_init](https://www.cnblogs.com/LoyenWang/p/11568481.html)
    * [zoned page frame allocator](https://www.cnblogs.com/LoyenWang/p/11626237.html)
    * [buddy system](https://www.cnblogs.com/LoyenWang/p/11666939.html)
    * [watermark](https://www.cnblogs.com/LoyenWang/p/11708255.html)
    * [memory compaction](https://www.cnblogs.com/LoyenWang/p/11746357.html)
    * [memory reclaim](https://www.cnblogs.com/LoyenWang/p/11827153.html)
    * [slub allocator](https://www.cnblogs.com/LoyenWang/p/11922887.html)
    * [vmap vmalloc](https://www.cnblogs.com/LoyenWang/p/11965787.html)
    * [vma malloc mmap ](https://www.cnblogs.com/LoyenWang/p/12037658.html)
    * [page fault](https://www.cnblogs.com/LoyenWang/p/12116570.html)
    * [rmap](https://www.cnblogs.com/LoyenWang/p/12164683.html)
    * [cma](https://www.cnblogs.com/LoyenWang/p/12182594.html)

* [内存管理2：ARM64 linux虚拟内存布局是怎样的](https://zhuanlan.zhihu.com/p/407063888)

* Cache
    * [深入学习Cache系列 :one: :link:](https://mp.weixin.qq.com/s/0_TRtwVxWW2B-izGFwykOw) ⊙ [:two: :link:](https://mp.weixin.qq.com/s/zN1BlEL378YSSKlcL96MGQ) ⊙ [:three: :link:](https://mp.weixin.qq.com/s/84BAFSCjasJBFDiN-XXUMg)

* [深入理解Linux内核之mmu-gather操作](https://blog.csdn.net/youzhangjing_/article/details/127703682)

*  bin的技术小屋
    * [深入理解 Linux 虚拟内存管理](https://mp.weixin.qq.com/s/uWadcBxEgctnrgyu32T8sQ)
    * [深入理解 Linux 物理内存管理](https://mp.weixin.qq.com/s/Cn-oX0W5DrI2PivaWLDpPw)
    * [深入理解 Linux 物理内存分配全链路实现](https://mp.weixin.qq.com/s/llZXDRG99NUXoMyIAf00ig)
    * [深入理解 Linux 伙伴系统的设计与实现](https://mp.weixin.qq.com/s/e28oT6vE7cOD5M8pukn_jw)
    * [深度解读 Linux 内核级通用内存池 - kmalloc](https://mp.weixin.qq.com/s/atHXeXxx0L63w99RW7bMHg)
    * [深度解读 Linux 虚拟物理内存映射](https://mp.weixin.qq.com/s/FzTBx32ABR0Vtpq50pwNSA)
    * [深度解读 Linux mmap](https://mp.weixin.qq.com/s/AUsgFOaePwVsPozC3F6Wjw)

*  深度 Linux
    * [探索三大分配器: bootmem, buddy, slab](https://mp.weixin.qq.com/s/Ki1z8na9-oVw1tk7PcdVrg)
    * [Linux内存回收机制：系统性能的幕后守护者](https://mp.weixin.qq.com/s/DwYGCwdr4Qb_bFvW5c1-pA)
    * [探秘IOMMU：从概念到原理的深度解析](https://mp.weixin.qq.com/s/XFwaEzFgN5FVDn5wheWhkA)

* Kernel Exploring
    * [内存管理的不同粒度](https://richardweiyang-2.gitbook.io/kernel-exploring/00-memory_a_bottom_up_view/13-physical-layer-partition)

        ```
                    +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
        slub        | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |
                    +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
        page        |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
                    +---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+
        page_block  |       |       |       |       |       |       |       |       |
                    +-------+-------+-------+-------+-------+-------+-------+-------+
        mem_section |               |               |               |               |
                    +---------------+---------------+---------------+---------------+
        memory_block|                               |                               |
                    +-------------------------------+-------------------------------+
        memblock    |                                                               |
                    +---------------------------------------------------------------+
        e820        |                                                               |
                    +---------------------------------------------------------------+
        ```

        * **Migrate Types**: Each pageblock has a single migrate type (e.g., MIGRATE_UNMOVABLE), set via **set_pageblock_migratetype**(). This ties to gfp_migratetype() we discussed.
        * **Zonelist**: Pageblocks exist within zones (e.g., ZONE_NORMAL). The allocator walks the zonelist, and compaction operates on pageblocks within those zones.
        * **__gfp_pfmemalloc_flags**: If ALLOC_NO_WATERMARKS is returned, the allocator might split a pageblock from a reserve pool (e.g., MIGRATE_HIGHATOMIC).
        * **Buddy System**: Pageblocks are a higher-level abstraction over the buddy’s free lists, aiding bulk management.

* [Linux watermark 内存水位](https://mp.weixin.qq.com/s/vCUfMgLAXshWhWa65SN_wA)

---

# _start:

* ![](../images/kernel/ker-start.svg)

```s
/* arch/arm64/kernel/head.S */
_start:
_primary_entry
    bl record_mmu_state

    /* Preserve the arguments passed by the bootloader in x0 .. x3 */
    bl preserve_boot_args

    bl create_idmap

    bl __cpu_setup

    b __primary_switch {
        adrp x1, reserved_pg_dir
        adrp x2, init_idmap_pg_dir
        bl __enable_mmu

        bl clear_page_tables
        bl create_kernel_mapping

        adrp x1, init_pg_dir
        load_ttbr1 x1, x1, x2 /* install x1 as a TTBR1 page table */

        x0 = __pa(KERNEL_START)

        bl __primary_switched {
            adr_l x4, init_task
            init_cpu_task x4, x5, x6

            adr_l x8, vectors /* load VBAR_EL1 with virtual */
            msr vbar_el1, x8 /* vector table address */

            /* Save the offset between
             * the kernel virtual and physical mappings*/
            ldr_l       x4, _text
            sub         x4, x4, x0
            str_l       x4, kimage_voffset, x5

            bl set_cpu_boot_mode_flag

            bl __pi_memset

            mov x0, x21    // pass FDT address in x0
            bl early_fdt_map
                /* populate pud, pmd, pte, p4d */
                early_fixmap_init()
                    __p4d_populate(p4dp, __pa_symbol(bm_pud))
                    __pud_populate(pudp, __pa_symbol(bm_pmd))
                    __pmd_populate(pmdp, __pa_symbol(bm_pte))

                /* map phys & virt */
                early_fdt_ptr = fixmap_remap_fdt(dt_phys, size)
                    create_mapping_noalloc()

            mov x0, x20 /* pass the full boot status */
            bl init_feature_override  /* Parse cpu feature overrides */

            bl start_kernel
        }
    }
```

```c
/* init/main.c */
start_kernel()
    setup_arch(&command_line) {
        setup_initial_init_mm()
        early_fixmap_init()
        early_ioremap_init();
        setup_machine_fdt(__fdt_pointer);
        arm64_memblock_init() {

        }
        paging_init() {
            /* map kernel: text, rodata init, data, bss*/
            map_kernel(pgdp);
            /* map all the memory banks */
            map_mem(pgdp);
            pgd_clear_fixmap();
            cpu_replace_ttbr1(lm_alias(swapper_pg_dir), init_idmap_pg_dir);
            init_mm.pgd = swapper_pg_dir;
            memblock_phys_free(__pa_symbol(init_pg_dir));
            memblock_allow_resize();
            create_idmap();
        }

        acpi_table_upgrade();

        bootmem_init() {
            dma_pernuma_cma_reserve();
            sparse_init();
            zone_sizes_init();
            dma_contiguous_reserve(arm64_dma_phys_limit);
            reserve_crashkernel();
        }
    }

    setup_per_cpu_areas() {

    }

    mm_core_init() {
        build_all_zonelists(NULL);
            --->
        page_alloc_init_cpuhp();
        page_ext_init_flatmem();
        mem_debugging_and_hardening_init();
        kfence_alloc_pool();
        report_meminit();
        kmsan_init_shadow();
        stack_depot_early_init();
        mem_init();
        mem_init_print_info();
        kmem_cache_init();

        page_ext_init_flatmem_late();
        kmemleak_init();
        ptlock_cache_init();
        pgtable_cache_init();
        debug_objects_mem_init();
        vmalloc_init();
        /* If no deferred init page_ext now, as vmap is fully initialized */
        if (!deferred_struct_pages)
            page_ext_init();
        /* Should be run before the first non-init thread is created */
        init_espfix_bsp();
        /* Should be run after espfix64 is set up. */
        pti_init();
        kmsan_init_runtime();
        mm_cache_init();
    }

    kmem_cache_init_late() {
        flushwq = alloc_workqueue("slub_flushwq", WQ_MEM_RECLAIM, 0);
    }

    setup_per_cpu_pageset() {

    }

    anon_vma_init() {

    }

    thread_stack_cache_init() {
        thread_stack_cache = kmem_cache_create_usercopy(
            "thread_stack",
            THREAD_SIZE, THREAD_SIZE, 0, 0,
            THREAD_SIZE, NUL
        );
    }

    proc_caches_init() {

    }

    vfs_caches_init() {

    }

    pagecache_init() {
        for (i = 0; i < PAGE_WAIT_TABLE_SIZE; i++)
            init_waitqueue_head(&folio_wait_table[i]);
        page_writeback_init();
    }
```

# setup_arch

```c
setup_arch(&command_line);
    setup_initial_init_mm(_stext, _etext, _edata, _end) {
        init_mm.start_code = (unsigned long)start_code;
        init_mm.end_code = (unsigned long)end_code;
        init_mm.end_data = (unsigned long)end_data;
        init_mm.brk = (unsigned long)brk;
    }

    early_fixmap_init();

    early_ioremap_init();

    setup_machine_fdt(__fdt_pointer);
        --->
    arm64_memblock_init();
        --->
    paging_init();
        --->
    acpi_table_upgrade();

    bootmem_init();
        --->
```

## early_fixmap_init

![](../images/kernel/mem-fix-map.svg)

```c
static pte_t bm_pte[NR_BM_PTE_TABLES][PTRS_PER_PTE] __page_aligned_bss;
static pmd_t bm_pmd[PTRS_PER_PMD] __page_aligned_bss __maybe_unused;
static pud_t bm_pud[PTRS_PER_PUD] __page_aligned_bss __maybe_unused;

void __init early_fixmap_init(void)
{
    unsigned long addr = FIXADDR_TOT_START;
    unsigned long end = FIXADDR_TOP;

    pgd_t *pgdp = pgd_offset_k(addr) {
        pgd_offset(&init_mm, (address)) {
            pgd_offset_pgd((mm)->pgd, (address)) {
                return (pgd + pgd_index(address));
            }
        }
    }
    p4d_t *p4dp = p4d_offset_kimg(pgdp, addr) {
        if (!pgtable_l5_enabled()) {
            return pgd_to_folded_p4d(pgdp, addr);
        }
        return (p4d_t *)__phys_to_kimg(p4d_offset_phys(pgdp, addr)) {
            return ((unsigned long)((x) + kimage_voffset))
        }
    }

    early_fixmap_init_pud(p4dp, addr, end) {
        p4d_t p4d = READ_ONCE(*p4dp);
        pud_t *pudp;

        if (p4d_none(p4d)) {
            __p4d_populate(p4dp, __pa_symbol(bm_pud), P4D_TYPE_TABLE | P4D_TABLE_AF) {
                set_p4d(p4dp, __p4d(__phys_to_p4d_val(pudp) | prot));
            }
        }

        pudp = pud_offset_kimg(p4dp, addr);
        early_fixmap_init_pmd(pudp, addr, end) {
            unsigned long next;
            pud_t pud = READ_ONCE(*pudp);
            pmd_t *pmdp;

            if (pud_none(pud))
                __pud_populate(pudp, __pa_symbol(bm_pmd), PUD_TYPE_TABLE | PUD_TABLE_AF);

            pmdp = pmd_offset_kimg(pudp, addr);
            do {
                next = pmd_addr_end(addr, end);
                early_fixmap_init_pte(pmdp, addr) {
                    pmd_t pmd = READ_ONCE(*pmdp);
                    pte_t *ptep;

                    if (pmd_none(pmd)) {
                        ptep = bm_pte[BM_PTE_TABLE_IDX(addr)];
                        __pmd_populate(pmdp, __pa_symbol(ptep), PMD_TYPE_TABLE | PMD_TABLE_AF) {
                            set_pmd(pmdp, __pmd(__phys_to_pmd_val(ptep) | prot));
                        }
                    }
                }
            } while (pmdp++, addr = next, addr != end);
        }
    }
}
```

## setup_machine_fdt

```c
setup_machine_fdt(__fdt_pointer)
    void *dt_virt = fixmap_remap_fdt(dt_phys, &size, PAGE_KERNEL) {
        const u64 dt_virt_base = __fix_to_virt(FIX_FDT);
        dt_phys_base = round_down(dt_phys, PAGE_SIZE);
        offset = dt_phys % PAGE_SIZE;
        dt_virt = (void *)dt_virt_base + offset;

        create_mapping_noalloc(dt_phys_base, dt_virt_base, PAGE_SIZE, prot);
            __create_pgd_mapping();

        *size = fdt_totalsize(dt_virt);
        if (*size > MAX_FDT_SIZE)
            return NULL;

        if (offset + *size > PAGE_SIZE) {
            create_mapping_noalloc(dt_phys_base, dt_virt_base,
                        offset + *size, prot);
        }

        return dt_virt;
    }
    if (dt_virt)
        memblock_reserve(dt_phys, size);

    early_init_dt_scan(dt_virt) {
        status = early_init_dt_verify(params);
        early_init_dt_scan_nodes() {

        }
    }

    const char * name = of_flat_dt_get_machine_name();

    /* Early fixups are done, map the FDT as read-only now */
    fixmap_remap_fdt(dt_phys, &size, PAGE_KERNEL_RO) {

    }

    name = of_flat_dt_get_machine_name();

```

## arm64_memblock_init

```c
arm64_memblock_init() {
    s64 linear_region_size = PAGE_END - _PAGE_OFFSET(vabits_actual);

    /* Remove memory above our supported physical address size */
    memblock_remove(1ULL << PHYS_MASK_SHIFT, ULLONG_MAX);

    /* Select a suitable value for the base of physical memory. */
    memstart_addr = round_down(memblock_start_of_DRAM(), ARM64_MEMSTART_ALIGN);

    /* Remove the memory that we will not be able to cover with the
     * linear mapping. Take care not to clip the kernel which may be
     * high in memory. */
    memblock_remove(max_t(u64, memstart_addr + linear_region_size,
            __pa_symbol(_end)), ULLONG_MAX);
    if (memstart_addr + linear_region_size < memblock_end_of_DRAM()) {
        /* ensure that memstart_addr remains sufficiently aligned */
        memstart_addr = round_up(memblock_end_of_DRAM() - linear_region_size,
                    ARM64_MEMSTART_ALIGN);
        memblock_remove(0, memstart_addr);
    }

    /* Register the kernel text, kernel data, initrd, and initial
     * pagetables with memblock. */
    memblock_reserve(__pa_symbol(_stext), _end - _stext);

    /* Remove the memory that we will not be able to cover
     * with the linear mapping. */
    memblock_remove(max_t(u64, memstart_addr + linear_region_size, __pa_symbol(_end)), ULLONG_MAX);

    if (IS_ENABLED(CONFIG_BLK_DEV_INITRD) && phys_initrd_size) {
        /* the generic initrd code expects virtual addresses */
        initrd_start = __phys_to_virt(phys_initrd_start);
        initrd_end = initrd_start + phys_initrd_size;
    }

    early_init_fdt_scan_reserved_mem() {

    }

    high_memory = __va(memblock_end_of_DRAM() - 1) + 1;
}
```


## paging_init

```c
/* arch/arm64/mm/mmu.c */
paging_init() {
    /* The linear map, maps all the memory banks */
    map_mem(swapper_pg_dir) {
        memblock_mark_nomap(kernel_start, kernel_end - kernel_start);
        for_each_mem_range(i, &start, &end) {
            __map_memblock(pgdp, start, end, pgprot_tagged(PAGE_KERNEL), flags) {
                __create_pgd_mapping(pgdp, start, __phys_to_virt(start), end - start, prot, early_pgtable_alloc, flags) {
                    --->
                }
            }
        }
        __map_memblock(pgdp, kernel_start, kernel_end, PAGE_KERNEL, NO_CONT_MAPPINGS);
        memblock_clear_nomap(kernel_start, kernel_end - kernel_start);
    }

    memblock_allow_resize();

    create_idmap();

    declare_kernel_vmas() {
        static struct vm_struct vmlinux_seg[KERNEL_SEGMENT_COUNT];

        declare_vma(&vmlinux_seg[0], _stext, _etext, VM_NO_GUARD);
        declare_vma(&vmlinux_seg[1], __start_rodata, __inittext_begin, VM_NO_GUARD);
        declare_vma(&vmlinux_seg[2], __inittext_begin, __inittext_end, VM_NO_GUARD);
        declare_vma(&vmlinux_seg[3], __initdata_begin, __initdata_end, VM_NO_GUARD);
        declare_vma(&vmlinux_seg[4]/*vma*/, _data/*start*/, _end/*end*/, 0/*vm_flags*/) {
            phys_addr_t pa_start = __pa_symbol(va_start);
            unsigned long size = va_end - va_start;

            BUG_ON(!PAGE_ALIGNED(pa_start));
            BUG_ON(!PAGE_ALIGNED(size));

            if (!(vm_flags & VM_NO_GUARD))
                size += PAGE_SIZE;

            vma->addr       = va_start;
            vma->phys_addr  = pa_start;
            vma->size       = size;
            vma->flags      = VM_MAP | vm_flags;
            vma->caller     = __builtin_return_address(0);

            vm_area_add_early(vma/*vm*/) {
                struct vm_struct *tmp, **p;

                BUG_ON(vmap_initialized);
                for (p = &vmlist; (tmp = *p) != NULL; p = &tmp->next) {
                    if (tmp->addr >= vm->addr) {
                        BUG_ON(tmp->addr < vm->addr + vm->size);
                        break;
                    } else
                        BUG_ON(tmp->addr + tmp->size > vm->addr);
                }
                vm->next = *p;
                *p = vm;
            }
        }
    }
}

static struct vm_struct *vmlist __initdata;
```

## bootmem_init

```c
bootmem_init() {
    min = PFN_UP(memblock_start_of_DRAM());
    max = PFN_DOWN(memblock_end_of_DRAM());

    early_memtest(min << PAGE_SHIFT, max << PAGE_SHIFT);

    max_pfn = max_low_pfn = max;
    min_low_pfn = min;

    arch_numa_init();
        --->

    sparse_init();
        --->

    zone_sizes_init();
        --->

    dma_contiguous_reserve(arm64_dma_phys_limit);

    reserve_crashkernel();
}
```

# numa

![](../images/kernel/mem-physic-numa-2.png)

![](../images/kernel/mem-physic-numa-1.png)

```
              +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
 slub         | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |
              +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
 page         |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
              +---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+
 page_block   |       |       |       |       |       |       |       |       |
              +-------+-------+-------+-------+-------+-------+-------+-------+
 mem_section  |               |               |               |               |
              +---------------+---------------+---------------+---------------+
 memory_block |                               |                               |
              +-------------------------------+-------------------------------+
 memblock     |                                                               |
              +---------------------------------------------------------------+
 e820         |                                                               |
              +---------------------------------------------------------------+
```


## numa_init

* start_kernel -> setup_arch -> bootmem_init -> arch_numa_init

```c
arch_numa_init() {
    if (!numa_off) {
        if (!acpi_disabled && !numa_init(arch_acpi_numa_init))
            return;
        if (acpi_disabled && !numa_init(of_numa_init))
            return;
    }

    numa_init(dummy_numa_init) {
        nodes_clear(numa_nodes_parsed);
        nodes_clear(node_possible_map);
        nodes_clear(node_online_map);

        ret = numa_alloc_distance() {
            size = nr_node_ids * nr_node_ids * sizeof(numa_distance[0]);
            numa_distance = memblock_alloc(size, PAGE_SIZE);
            numa_distance_cnt = nr_node_ids;

            /* fill with the default distances */
            for (i = 0; i < numa_distance_cnt; i++) {
                for (j = 0; j < numa_distance_cnt; j++) {
                    numa_distance[i * numa_distance_cnt + j] = i == j
                        ? LOCAL_DISTANCE : REMOTE_DISTANCE;
                }
            }
        }
        if (ret < 0)
            return ret;

        ret = init_func(); arch_acpi_numa_init() {
            acpi_numa_init() {
                int i, fake_pxm, cnt = 0;

                if (acpi_disabled)
                    return -EINVAL;

                /* SRAT: System Resource Affinity Table */
                ret = acpi_table_parse(ACPI_SIG_SRAT, acpi_parse_srat) {
                    struct acpi_table_header *table = NULL;

                    if (acpi_disabled)
                        return -ENODEV;

                    if (!id || !handler)
                        return -EINVAL;

                    if (strncmp(id, ACPI_SIG_MADT, 4) == 0)
                        acpi_get_table(id, acpi_apic_instance, &table);
                    else
                        acpi_get_table(id, 0, &table);

                    if (table) {
                        handler(table); acpi_parse_srat() {
                            struct acpi_table_srat *srat = (struct acpi_table_srat *)table;

                            acpi_srat_revision = srat->header.revision;

                            /* Real work done in acpi_table_parse_srat below. */
                            return 0;
                        }
                        acpi_put_table(table);
                        return 0;
                    } else
                        return -ENODEV;
                }
                if (!ret) {
                    struct acpi_subtable_proc srat_proc[4];

                    memset(srat_proc, 0, sizeof(srat_proc));
                    srat_proc[0].id = ACPI_SRAT_TYPE_CPU_AFFINITY;
                    srat_proc[0].handler = acpi_parse_processor_affinity;

                    srat_proc[1].id = ACPI_SRAT_TYPE_X2APIC_CPU_AFFINITY;
                    srat_proc[1].handler = acpi_parse_x2apic_affinity;

                    srat_proc[2].id = ACPI_SRAT_TYPE_GICC_AFFINITY;
                    srat_proc[2].handler = acpi_parse_gicc_affinity;

                    srat_proc[3].id = ACPI_SRAT_TYPE_GENERIC_AFFINITY;
                    srat_proc[3].handler = acpi_parse_gi_affinity;

                    acpi_table_parse_entries_array(
                        ACPI_SIG_SRAT,
                        sizeof(struct acpi_table_srat),
                        srat_proc, ARRAY_SIZE(srat_proc), 0
                    );

                    cnt = acpi_table_parse_srat(ACPI_SRAT_TYPE_MEMORY_AFFINITY,
                        acpi_parse_memory_affinity, 0);
                    acpi_parse_memory_affinity() {
                        struct acpi_srat_mem_affinity *memory_affinity;

                        memory_affinity = (struct acpi_srat_mem_affinity *)header;

                        acpi_table_print_srat_entry(&header->common);

                        /* let architecture-dependent part to do it */
                        ret = acpi_numa_memory_affinity_init(memory_affinity) {
                            u64 start, end;
                            u32 hotpluggable;
                            int node, pxm;

                            hotpluggable = IS_ENABLED(CONFIG_MEMORY_HOTPLUG)
                                && (ma->flags & ACPI_SRAT_MEM_HOT_PLUGGABLE);

                            start = ma->base_address;
                            end = start + ma->length;
                            pxm = ma->proximity_domain;
                            if (acpi_srat_revision <= 1)
                                pxm &= 0xff;

                            numa_add_memblk(node, start, end)
                                --->

                            node = acpi_map_pxm_to_node(pxm);
                            node_set(node, numa_nodes_parsed);

                            /* Mark hotplug range in memblock. */
                            memblock_mark_hotplug(start, ma->length);
                            max_possible_pfn = max(max_possible_pfn, PFN_UP(end - 1));

                            return 0;
                        }
                        if (!)
                            parsed_numa_memblks++;
                        return 0;
                    }
                }

                /* SLIT: System Locality Information Table */
                acpi_table_parse(ACPI_SIG_SLIT, acpi_parse_slit);

                /* fake_pxm is the next unused PXM value after SRAT parsing */
                for (i = 0, fake_pxm = -1; i < MAX_NUMNODES; i++) {
                    if (node_to_pxm_map[i] > fake_pxm)
                        fake_pxm = node_to_pxm_map[i];
                }
                fake_pxm++;
                acpi_table_parse_cedt(ACPI_CEDT_TYPE_CFMWS, acpi_parse_cfmws, &fake_pxm);

                if (cnt < 0)
                    return cnt;
                else if (!parsed_numa_memblks)
                    return -ENOENT;
                return 0;
            }
        }

        ret = numa_register_nodes() {
            for_each_node_mask(nid, numa_nodes_parsed) {
                unsigned long start_pfn, end_pfn;

                get_pfn_range_for_nid(nid, &start_pfn, &end_pfn);
                setup_node_data(nid, start_pfn, end_pfn) {
                    const size_t nd_size = roundup(sizeof(pg_data_t), SMP_CACHE_BYTES);
                    u64 nd_pa;
                    void *nd;
                    int tnid;

                    nd_pa = memblock_phys_alloc_try_nid(nd_size, SMP_CACHE_BYTES, nid);
                    nd = __va(nd_pa);
                    tnid = early_pfn_to_nid(nd_pa >> PAGE_SHIFT);

                    node_data[nid] = nd;
                    memset(NODE_DATA(nid), 0, sizeof(pg_data_t));
                    NODE_DATA(nid)->node_id = nid;
                    NODE_DATA(nid)->node_start_pfn = start_pfn;
                    NODE_DATA(nid)->node_spanned_pages = end_pfn - start_pfn;
                }

                node_set_online(nid) {
                    node_set_state(nid, N_ONLINE);
                    nr_online_nodes = num_node_state(N_ONLINE);
                }
            }

            /* Setup online nodes to actual nodes*/
            node_possible_map = numa_nodes_parsed;
        }
        setup_node_to_cpumask_map();
    }
}
```

## node

```c
/* drivers/base/arch_numa.c */
struct pglist_data *node_data[MAX_NUMNODES] __read_mostly;
EXPORT_SYMBOL(node_data);
nodemask_t numa_nodes_parsed __initdata;
static int cpu_to_node_map[NR_CPUS] = { [0 ... NR_CPUS-1] = NUMA_NO_NODE };

static int numa_distance_cnt;
static u8 *numa_distance;
bool numa_off;

typedef struct pglist_data {
    struct zone node_zones[MAX_NR_ZONES];
    /* backup area if current node run out
     * link zones in Movable->highmem->normal->dma direction */
    struct zonelist node_zonelists[MAX_ZONELISTS];
    int nr_zones;
    struct page *node_mem_map;
    unsigned long node_start_pfn;     /* start page number of this node */
    unsigned long node_present_pages; /* total number of physical pages */
    unsigned long node_spanned_pages; /* total size of physical page range, including holes */
    int node_id;
} pg_data_t;

struct zonelist {
    struct zoneref {
        struct zone *zone;  /* Pointer to actual zone */
        int zone_idx;       /* zone_idx(zoneref->zone) */
    } _zonerefs[MAX_ZONES_PER_ZONELIST + 1];
};

#define MAX_ZONES_PER_ZONELIST (MAX_NUMNODES * MAX_NR_ZONES)

enum zone_type {
    ZONE_DMA,
    ZONE_DMA32,
    ZONE_NORMAL,    /* direct mmapping area */
    ZONE_HIGHMEM,   /* only for 32 bit system */
    /* make memory offlining/unplug more likely to succeed,
     * and to locally limit unmovable allocations */
    ZONE_MOVABLE,
    ZONE_DEVICE,
    __MAX_NR_ZONES
};
```

* [Chapter 2 Describing Physical Memory](https://www.kernel.org/doc/gorman/html/understand/understand005.html)

## zone

| **Type** | **Definition**  | **Usage** |
| :-: | :-: | :-: |
| `MIGRATE_UNMOVABLE`   | Pages that cannot be moved to another physical location. | Used for stable, critical allocations like kernel data or pinned DMA buffers. |
| `MIGRATE_MOVABLE`     | Pages that can be migrated to another physical location. | Helps defragment memory for large allocations (e.g., user-space pages with `__GFP_MOVABLE`). |
| `MIGRATE_RECLAIMABLE` | Pages that can be freed but not moved. | Reclaimed under memory pressure (e.g., slab caches with `__GFP_RECLAIMABLE`). |
| `MIGRATE_PCPTYPES`    | Marker for the number of types in per-CPU page lists. | Defines scope of fast pcp caches (`UNMOVABLE`, `MOVABLE`, `RECLAIMABLE`). |
| `MIGRATE_HIGHATOMIC`  | Reserve pool for high-priority atomic allocations. | Emergency pool for critical needs (e.g., `GFP_ATOMIC` with `ALLOC_HIGHATOMIC`). |
| `MIGRATE_CMA`         | Pages reserved for the Contiguous Memory Allocator. | Supports large, contiguous allocations for devices like GPUs (movable). |
| `MIGRATE_ISOLATE`     | |

```c
/* The zone fallback order is MOVABLE -> HIGHMEM -> NORMAL -> DMA32 -> DMA */

Node 0                 Node 1
[CPU 0]                [CPU 1]
  |                      |
  v                      v
+----------------+     +----------------+
| ZONE_NORMAL    |     | ZONE_NORMAL    |
| (3.984 GB)     |     | (3.984 GB)     |
| Buddy Free     |     | Buddy Free     |
| Lists:         |     | Lists:         |
| - UNMOVABLE    |     | - UNMOVABLE    |
| - MOVABLE      |     | - MOVABLE      |
| - RECLAIMABLE  |     | - RECLAIMABLE  |
| - HIGHATOMIC   |     | - HIGHATOMIC   |
+----------------+     +----------------+
| ZONE_DMA       |     | ZONE_DMA       |
| (16 MB)        |     | (16 MB)        |
| Buddy Free     |     | Buddy Free     |
| Lists (similar)|     | Lists (similar)|
+----------------+     +----------------+
  |                      |
  +--- Zonelist -       -+---- Zonelist --+
  |    Node 0            |    Node 1      |
  v                      v                |
+----------------+     +----------------+ |
| 1. N0 NORMAL   |     | 1. N1 NORMAL   | |
| 2. N0 DMA      |     | 2. N1 DMA      | | Priority Order
| 3. N1 NORMAL   |     | 3. N0 NORMAL   | | for Allocation
| 4. N1 DMA      |     | 4. N0 DMA      | |
+----------------+     +----------------+ |
Dist: 10 (local)       Dist: 10 (local)   |
      20 (remote)            20 (remote)  v
```

```c
struct zone {
    /* zone watermarks, access with *_wmark_pages(zone) macros */
    unsigned long _watermark[NR_WMARK];
    /* temporarily increases the effective low watermark
     * for a memory zone to trigger more aggressive reclaim or compaction
     * to prevent allocation failures for high-order pages  */
    unsigned long watermark_boost;

    unsigned long nr_reserved_highatomic;
    unsigned long nr_free_highatomic;

    long lowmem_reserve[MAX_NR_ZONES];

    struct pglist_data  *zone_pgdat;
    struct per_cpu_pageset *pageset; /* hot/cold page */

    unsigned long    *pageblock_flags /* track migratetype of each pageblock */

    unsigned long    zone_start_pfn;
    unsigned long    managed_pages; /* managed_pages = present_pages - reserved_pages */
    unsigned long    spanned_pages; /* spanned_pages = zone_end_pfn - zone_start_pfn */
    unsigned long    present_pages; /* present_pages = spanned_pages - absent_pages(pages in holes) */

    const char    *name;
    /* free areas of different sizes */
    struct free_area  free_area[MAX_PAGE_ORDER];
    /* zone flags, see below */
    unsigned long    flags;

    /* Primarily protects free_area */
    spinlock_t    lock;

    atomic_long_t   vm_stat[NR_VM_ZONE_STAT_ITEMS];
    atomic_long_t   vm_numa_event[NR_VM_NUMA_EVENT_ITEMS];
};

struct per_cpu_pageset {
    struct per_cpu_pages pcp;
    s8 expire;
    u16 vm_numa_stat_diff[NR_VM_NUMA_STAT_ITEMS];
};

struct per_cpu_pages {
    int count; /* number of pages in the list */
    int high;  /* high watermark, emptying needed */
    int batch; /* chunk size for buddy add/remove */

    /* Lists of pages, one per migrate type stored on the pcp-lists */
    struct list_head lists[MIGRATE_PCPTYPES];
};

enum migratetype {
    MIGRATE_UNMOVABLE,
    MIGRATE_MOVABLE,
    MIGRATE_RECLAIMABLE,
    MIGRATE_PCPTYPES,  /* the number of types on the pcp lists */
    MIGRATE_HIGHATOMIC = MIGRATE_PCPTYPES,
    MIGRATE_CMA,
    MIGRATE_ISOLATE,
    MIGRATE_TYPES
};

#define PB_migratetype_bits 3
/* Bit indices that affect a whole block of pages */
enum pageblock_bits {
    PB_migrate,
    PB_migrate_end = PB_migrate + PB_migratetype_bits - 1,
    /* 3 bits required for migrate types */
    PB_migrate_skip,/* If set the block is skipped by compaction */

    NR_PAGEBLOCK_BITS
};
```

### zone_sizes_init

* <code style="color : yellowgreen">start_kernel</code> -> <code style="color : yellowgreen">setup_arch</code> -> <code style="color : yellowgreen">bootmem_init</code> -> <code style="color : yellowgreen">zone_sizes_init</code>

```c
zone_sizes_init() {
    /* Initialise all pg_data_t and zone data */
    free_area_init(max_zone_pfns) {

        find_zone_movable_pfns_for_nodes();

        for_each_mem_pfn_range(i, MAX_NUMNODES, &start_pfn, &end_pfn, &nid) {
            subsection_map_init(start_pfn, end_pfn - start_pfn);
        }

        setup_nr_node_ids();
        for_each_node(nid) {
            pg_data_t *pgdat;

            if (!node_online(nid)) {

                pgdat = arch_alloc_nodedata(nid) {

                }

                arch_refresh_nodedata(nid, pgdat);
                free_area_init_memoryless_node(nid);

                continue;
            }

            pgdat = NODE_DATA(nid);
            free_area_init_node(nid) {
                get_pfn_range_for_nid(nid, &start_pfn, &end_pfn);

                pgdat->node_id = nid;
                pgdat->node_start_pfn = start_pfn;
                pgdat->per_cpu_nodestats = NULL;

                calculate_node_totalpages(pgdat, start_pfn, end_pfn) {
                    spanned = zone_spanned_pages_in_node();
                    absent = zone_absent_pages_in_node()
                    if (size)
                        zone->zone_start_pfn = zone_start_pfn;
                    else
                        zone->zone_start_pfn = 0;
                    zone->spanned_pages = size;
                    zone->present_pages = real_size;

                }

                alloc_node_mem_map() { #ifdef CONFIG_FLATMEM
                    start = pgdat->node_start_pfn & ~(MAX_ORDER_NR_PAGES - 1);
                    offset = pgdat->node_start_pfn - start;

                    end = pgdat_end_pfn(pgdat);
                    end = ALIGN(end, MAX_ORDER_NR_PAGES);
                    size =  (end - start) * sizeof(struct page);
                    map = memmap_alloc(size);

                    pgdat->node_mem_map = map + offset;

                    if (pgdat == NODE_DATA(0)) {
                        mem_map = NODE_DATA(0)->node_mem_map;
                        if (page_to_pfn(mem_map) != pgdat->node_start_pfn)
                            mem_map -= offset;
                    }
                }

                /* Set up the zone data structures */
                free_area_init_core(pgdat) {
                    /* calculate the freesize of pgdat */
                    zone_init_internals(zone, j, nid, freesize);
                    set_pageblock_order();
                    setup_usemap(zone) {
                        unsigned long usemapsize = usemap_size(
                            zone->zone_start_pfn, zone->spanned_pages) {

                            unsigned long usemapsize;
                            zonesize += zone_start_pfn & (pageblock_nr_pages-1);
                            usemapsize = roundup(zonesize, pageblock_nr_pages);
                            usemapsize = usemapsize >> pageblock_order;
                            usemapsize *= NR_PAGEBLOCK_BITS;
                            usemapsize = roundup(usemapsize, BITS_PER_LONG);

                            return usemapsize / BITS_PER_BYTE;
                        }
                        zone->pageblock_flags = NULL;
                        if (usemapsize) {
                            zone->pageblock_flags = memblock_alloc_node(
                                usemapsize, SMP_CACHE_BYTES, zone_to_nid(zone)
                            );
                        }
                    }
                    init_currently_empty_zone();
                }
                lru_gen_init_pgdat(pgdat);
            }

            /* Any memory on that node */
            if (pgdat->node_present_pages)
                node_set_state(nid, N_MEMORY);
            check_for_memory(pgdat, nid);
        }

        memmap_init() {
            for_each_mem_pfn_range(i, MAX_NUMNODES, &start_pfn, &end_pfn, &nid) {
                struct pglist_data *node = NODE_DATA(nid);

                for (j = 0; j < MAX_NR_ZONES; j++) {
                    struct zone *zone = node->node_zones + j;

                    /* return zone->present_pages */
                    if (!populated_zone(zone))
                        continue;

                    memmap_init_zone_range(zone, start_pfn, end_pfn, &hole_pfn) {
                        memmap_init_range() {
                            for (pfn = start_pfn; pfn < end_pfn; ) {
                                if (context == MEMINIT_EARLY) {
                                    if (overlap_memmap_init(zone, &pfn))
                                        continue;
                                    if (defer_init(nid, pfn, zone_end_pfn)) {
                                        deferred_struct_pages = true;
                                        break;
                                    }
                                }

                                page = pfn_to_page(pfn); /* vmemmap + pfn */
                                __init_single_page(page, pfn, zone, nid) {
                                    mm_zero_struct_page(page); /* memset(0) */
                                    set_page_links(page, zone, nid, pfn);
                                    init_page_count(page); /* 1 */
                                    page_mapcount_reset(page);
                                    page_cpupid_reset_last(page);
                                    page_kasan_tag_reset(page);

                                    INIT_LIST_HEAD(&page->lru);
                                }
                                if (context == MEMINIT_HOTPLUG)
                                    __SetPageReserved(page);

                                if (pageblock_aligned(pfn)) {
                                    set_pageblock_migratetype(page, migratetype);
                                    cond_resched();
                                }

                                pfn++;
                            }
                        }
                    }
                    zone_id = j;
                }
            }
        }
    }
}
```

### build_all_zonelists

* start_kernel -> mm_core_init -> build_all_zonelists

```c
build_all_zonelists(NULL) {
    __build_all_zonelists(NULL) {
        for_each_node(nid) {
            pg_data_t *pgdat = NODE_DATA(nid);

            build_zonelists(pgdat) {
                static int node_order[MAX_NUMNODES];
                int node, nr_nodes = 0;
                nodemask_t used_mask = NODE_MASK_NONE;
                int local_node, prev_node;

                /* NUMA-aware ordering of nodes */
                local_node = pgdat->node_id;
                prev_node = local_node;

                memset(node_order, 0, sizeof(node_order));
                find_next_best_node(local_node, &used_mask) {
                    for_each_node_state(n, N_MEMORY) {
                        /* Don't want a node to appear more than once */
                        if (node_isset(n, *used_node_mask))
                            continue;

                        /* Use the distance array to find the distance */
                        val = node_distance(node, n) {
                            return numa_distance[from * numa_distance_cnt + to];
                        }

                        /* Penalize nodes under us ("prefer the next node") */
                        val += (n < node);

                        /* Give preference to headless and unused nodes */
                        if (!cpumask_empty(cpumask_of_node(n)))
                            val += PENALTY_FOR_NODE_WITH_CPUS;

                        /* Slight preference for less loaded node */
                        val *= MAX_NUMNODES;
                        val += node_load[n];

                        if (val < min_val) {
                            min_val = val;
                            best_node = n;
                        }
                    }

                    if (best_node >= 0)
                        node_set(best_node, *used_node_mask);

                    return best_node;
                }
                while ((node = find_next_best_node()) >= 0) {
                    if (node_distance(local_node, node)
                        != node_distance(local_node, prev_node))
                        node_load[node] += 1;

                    node_order[nr_nodes++] = node;
                    prev_node = node;
                }

                build_zonelists_in_node_order(pgdat, node_order, nr_nodes) {
                    struct zoneref *zonerefs;
                    int i;

                    zonerefs = pgdat->node_zonelists[ZONELIST_FALLBACK]._zonerefs;

                    for (i = 0; i < nr_nodes; i++) {
                        int nr_zones;

                        pg_data_t *node = NODE_DATA(node_order[i]);

                        nr_zones = build_zonerefs_node(node, zonerefs) {
                            struct zone *zone;
                            enum zone_type zone_type = MAX_NR_ZONES;
                            int nr_zones = 0;

                            do {
                                zone_type--;
                                zone = pgdat->node_zones + zone_type;
                                if (populated_zone(zone)) {
                                    zoneref_set_zone(zone, &zonerefs[nr_zones++]) {
                                        zoneref->zone = zone;
                                        zoneref->zone_idx = zone_idx(zone);
                                    }
                                }
                            } while (zone_type);

                            return nr_zones;
                        }
                        zonerefs += nr_zones;
                    }
                    zonerefs->zone = NULL;
                    zonerefs->zone_idx = 0;
                }

                build_thisnode_zonelists(pgdat) {
                    struct zoneref *zonerefs;
                    int nr_zones;

                    zonerefs = pgdat->node_zonelists[ZONELIST_NOFALLBACK]._zonerefs;
                    nr_zones = build_zonerefs_node(pgdat, zonerefs);
                        --->
                    zonerefs += nr_zones;
                    zonerefs->zone = NULL;
                    zonerefs->zone_idx = 0;
                }
            }
        }
    }
    for_each_possible_cpu(cpu)
        per_cpu_pages_init(&per_cpu(boot_pageset, cpu), &per_cpu(boot_zonestats, cpu));

    mminit_verify_zonelist();
    cpuset_init_current_mems_allowed();
}
```

## page

![](../images/kernel/mem-folio-layout.png)

* [LWN Index  - struct_page](https://lwn.net/Kernel/Index/#Memory_management-struct_page)
    * [Pulling slabs out of struct page](https://lwn.net/Articles/871982/)
    * [Struct slab comes to 5.17 ](https://lwn.net/Articles/881039/)
    * [The proper time to split struct page](https://lwn.net/Articles/937839/)

* [Linux Compound Page](https://mp.weixin.qq.com/s/kj0UjP7QSynIb_KHov490g)
* [OPPO - Linux Large Folios 大页在社区和产品的现状和未来](https://mp.weixin.qq.com/s/Jp0TgD2p91m6mUczRdTh0Q)

| **Context** | <span style="color:yellowgreen">index Usage</span> | <span style="color:yellowgreen">private Usage</span> |
| :-: | :-: | :-: |
| **File-Backed Pages** | page offset within the file (measured in pages). | filesystem-specific metadata (e.g., journaling info). |
| **Anonymous Pages** |  | |
| **Swap Cache Pages** | `swp_entry_t` (location in swap space). |  |
| **Device-Backed Pages** | device-specific offsets or identifiers. | device-specific data (e.g., DMA or driver info). |
| **Buffer Pages** | | Buffer head pointer |
| **Writeback Pages** | May store the offset for tracking page in writeback operations. | flags or state related to writeback processes. |
| **VMA (File-Backed)** | Offset within the file, relative to the VMA’s vm_pgoff, used for address translation. | Typically 0 or buffer-related data (e.g., buffer_head) if the page is part of a file mapping.
| **VMA (Anonymous)** | Offset within the VMA (relative to start), used during fault handling; less meaningful unless swapped. | 0 unless swapped, then holds swp_entry_t when in swap cache.
| **Networking** | Rarely used. | custom metadata for network-related operations (e.g., zero-copy). |
| **Buddy Pages** | | Buddy order |
| **Slab Pages** | | kmem_cache pointer |
| **Huge Pages** | offset of the huge page in its mapping. | metadata about the huge page (e.g., reference count). |
| **Compound Head** | Offset within the compound page mapping | metadata for managing compound pages (e.g., ref count). |
| **Compound Tail** | Compound head pointer | Offset to head |

---
**Page Table Flags in 4KB Granule**
|Flag|Bits|Purpose|Common Values|
| :-: | :-: | :-: | :-: |
|PTE_VALID|0|Valid entry|1 (valid), 0 (invalid)|
|PTE_TABLE|1|Table vs. page|0 (page at PTE level)|
|PTE_MT|2-4|Memory type (AttrIndx)|0-7 (e.g., Normal, Device)|
|PTE_NS|5|Non-secure|1 (non-secure)|
|PTE_AP|6-7|Access permissions|00, 01, 10, 11|
|PTE_SH|8-9|Shareability|11 (inner shareable)|
|PTE_AF|10|Access flag|1 (accessed)|
|PTE_NG|11|Not global|1 (user), 0 (kernel)|
|Physical Addr|12-47|Physical page address|Page frame number|
|PTE_DIRTY|(SW)|Software dirty bit|1 (dirty)|
|PTE_YOUNG|(SW)|Software access bit|1 (recently accessed)|

```c
struct page {
    /* Atomic flags + zone number, some possibly updated asynchronously */
    unsigned long flags;

    /* Five words (20/40 bytes) are available in this union.
    * WARNING: bit 0 of the first word is used for PageTail(). That
    * means the other users of this union MUST NOT use the bit to
    * avoid collision and false-positive PageTail(). */
    union {
        struct {    /* Page cache and anonymous pages */
            /**
            * @lru: Pageout list, eg. active_list protected by
            * lruvec->lru_lock.  Sometimes used as a generic list
            * by the page owner. */
            union {
                struct list_head lru;

                /* Or, for the Unevictable "LRU list" slot */
                struct {
                    /* Always even, to negate PageTail */
                    void *__filler;
                    /* Count page's or folio's mlocks */
                    unsigned int mlock_count;
                };

                /* Or, free page */
                struct list_head buddy_list;
                struct list_head pcp_list;
                struct {
                    struct llist_node pcp_llist;
                    unsigned int order;
                };
            };
            /* See page-flags.h for PAGE_MAPPING_FLAGS */
            struct address_space *mapping;
            union {
                /* 1. anon mapping: page offset in user virtual address space
                 * 2. file mapping: page offset in file
                 * 3. migration type
                 * 4. swap_entry_t */
                pgoff_t __folio_index;  /* Our offset within mapping. */
                unsigned long share;    /* share count for fsdax */
            };
            /**
            * @private: Mapping-private opaque data.
            * Usually used for buffer_heads if PagePrivate.
            * Used for swp_entry_t if swapcache flag set.
            * Indicates order in the buddy system if PageBuddy. */
            unsigned long private;
        };
        struct {    /* page_pool used by netstack */
            /**
            * @pp_magic: magic value to avoid recycling non
            * page_pool allocated pages. */
            unsigned long pp_magic;
            struct page_pool *pp;
            unsigned long _pp_mapping_pad;
            unsigned long dma_addr;
            atomic_long_t pp_ref_count;
        };
        struct {    /* Tail pages of compound page */
            unsigned long compound_head;    /* Bit zero is set */
        };
        struct {    /* ZONE_DEVICE pages */
            /* The first word is used for compound_head or folio
            * pgmap */
            void *_unused_pgmap_compound_head;
            void *zone_device_data;
            /* ZONE_DEVICE private pages are counted as being
            * mapped so the next 3 words hold the mapping, index,
            * and private fields from the source anonymous or
            * page cache page while the page is migrated to device
            * private memory.
            * ZONE_DEVICE MEMORY_DEVICE_FS_DAX pages also
            * use the mapping, index, and private fields when
            * pmem backed DAX files are mapped. */
        };

        /** @rcu_head: You can use this to free a page by RCU. */
        struct rcu_head rcu_head;
    };

    union { /* This union is 4 bytes in size. */
        /* For head pages of typed folios, the value stored here
        * allows for determining what this page is used for. The
        * tail pages of typed folios will not store a type
        * (page_type == _mapcount == -1).
        *
        * See page-flags.h for a list of page types which are currently
        * stored here.
        *
        * Owners of typed folios may reuse the lower 16 bit of the
        * head page page_type field after setting the page type,
        * but must reset these 16 bit to -1 before clearing the
        * page type. */
        unsigned int page_type;

        /* For pages that are part of non-typed folios for which mappings
        * are tracked via the RMAP, encodes the number of times this page
        * is directly referenced by a page table.
        *
        * Note that the mapcount is always initialized to -1, so that
        * transitions both from it and to it can be tracked, using
        * atomic_inc_and_test() and atomic_add_negative(-1). */
        atomic_t _mapcount;
    };

    /* Usage count. *DO NOT USE DIRECTLY*. See page_ref.h */
    atomic_t _refcount;

#ifdef CONFIG_MEMCG
    unsigned long memcg_data;
#elif defined(CONFIG_SLAB_OBJ_EXT)
    unsigned long _unused_slab_obj_exts;
#endif

    /* On machines where all RAM is mapped into kernel address space,
    * we can simply calculate the virtual address. On machines with
    * highmem some memory is mapped into kernel virtual memory
    * dynamically, so we need a place to store that address.
    * Note that this field could be 16 bits on x86 ... ;)
    *
    * Architectures with slow multiplication can define
    * WANT_PAGE_VIRTUAL in asm/page.h */
#if defined(WANT_PAGE_VIRTUAL)
    void *virtual;  /* Kernel virtual address (NULL if
                    not kmapped, ie. highmem) */
#endif /* WANT_PAGE_VIRTUAL */

#ifdef LAST_CPUPID_NOT_IN_PAGE_FLAGS
    int _last_cpupid;
#endif

#ifdef CONFIG_KMSAN
    /* KMSAN metadata for this page:
    *  - shadow page: every bit indicates whether the corresponding
    *    bit of the original page is initialized (0) or not (1);
    *  - origin page: every 4 bytes contain an id of the stack trace
    *    where the uninitialized value was created. */
    struct page *kmsan_shadow;
    struct page *kmsan_origin;
#endif
} _struct_page_alignment;
```

# sparsemem

* [Physical Memory Model: FLATE - SPARSE](https://docs.kernel.org/mm/memory-model.html)

[Pic Source](https://zhuanlan.zhihu.com/p/220068494)
![](../images/kernel/mem-sparsemem-arch.png)

![](../images/kernel/mem-sparsemem-phys-pfn-virt.svg)

```c
#ifdef CONFIG_SPARSEMEM_EXTREME
    #define SECTIONS_PER_ROOT   (PAGE_SIZE / sizeof (struct mem_section))
#else
    #define SECTIONS_PER_ROOT    1
#endif

#ifdef CONFIG_SPARSEMEM_EXTREME
    struct mem_section **mem_section;
#else
    struct mem_section mem_section[NR_SECTION_ROOTS][SECTIONS_PER_ROOT];
#endif


#if MAX_NUMNODES <= 256
    static u8 section_to_node_table[NR_MEM_SECTIONS];
#else
    static u16 section_to_node_table[NR_MEM_SECTIONS];
#endif
```

```c
#define __is_lm_address(addr) (((u64)(addr) - PAGE_OFFSET) < (PAGE_END - PAGE_OFFSET))

/* PHYS_OFFSET - the physical address of the start of memory. */
#define PHYS_OFFSET ({ VM_BUG_ON(memstart_addr & 1); memstart_addr; })

#define __lm_to_phys(addr) (((addr) - PAGE_OFFSET) + PHYS_OFFSET)
#define __kimg_to_phys(addr) ((addr) - kimage_voffset)

#define __virt_to_phys_nodebug(x) ({ \
    phys_addr_t __x = (phys_addr_t)(__tag_reset(x)); \
    __is_lm_address(__x) ? __lm_to_phys(__x) : __kimg_to_phys(__x); \
})

#define __virt_to_phys(x)   __virt_to_phys_nodebug(x)
#define __pa(x) __virt_to_phys((unsigned long)(x))

#define __phys_to_virt(x)   ((unsigned long)((x) - PHYS_OFFSET) | PAGE_OFFSET)
#define __phys_to_kimg(x)   ((unsigned long)((x) + kimage_voffset))

#define page_to_phys(page)  (__pfn_to_phys(page_to_pfn(page)))
#define phys_to_page(phys)  (pfn_to_page(__phys_to_pfn(phys)))

#if defined(CONFIG_FLATMEM)

    #ifndef ARCH_PFN_OFFSET
    #define ARCH_PFN_OFFSET     (0UL)
    #endif

    #define __pfn_to_page(pfn) (mem_map + ((pfn) - ARCH_PFN_OFFSET))
    #define __page_to_pfn(page) ((unsigned long)((page) - mem_map) \
        + ARCH_PFN_OFFSET)

#elif defined(CONFIG_SPARSEMEM_VMEMMAP)

    #define vmemmap ((struct page *)VMEMMAP_START - (memstart_addr >> PAGE_SHIFT))

    /* memmap is virtually contiguous.  */
    #define __pfn_to_page(pfn)      (vmemmap + (pfn))
    #define __page_to_pfn(page)    (unsigned long)((page) - vmemmap)

#elif defined(CONFIG_SPARSEMEM)

    /* Note: section's mem_map is encoded to reflect its start_pfn.
     * section[i].section_mem_map == mem_map's address - start_pfn; */
    #define __page_to_pfn(pg) \
    ({ const struct page *__pg = (pg); \
        int __sec = page_to_section(__pg); \
        (unsigned long)(__pg - __section_mem_map_addr(__nr_to_section(__sec))); \
    })

    #define __pfn_to_page(pfn) \
    ({ unsigned long __pfn = (pfn); \
        struct mem_section *__sec = __pfn_to_section(__pfn); \
        __section_mem_map_addr(__sec) + __pfn; \
    })

    struct page *__section_mem_map_addr(struct mem_section *section) {
        unsigned long map = section->section_mem_map;
        map &= SECTION_MAP_MASK;
        return (struct page *)map;
    }
#endif /* CONFIG_FLATMEM/SPARSEMEM */
```

## sparse_init

```c
int sparse_init(void) {
    /* Mark all memblocks as present */
    memblocks_present() {
        for_each_mem_pfn_range() {
            memory_present(nid, start, end) {
                if (unlikely(!mem_section)) {
                    mem_section = memblock_alloc(size, align);
                }

                mminit_validate_memmodel_limits(&start, &end);
                for (pfn = start; pfn < end; pfn += PAGES_PER_SECTION) {
                    unsigned long section = pfn_to_section_nr(pfn);
                    struct mem_section *ms;

                    sparse_index_init(section, nid) {
                        unsigned long root = SECTION_NR_TO_ROOT(section_nr) {
                            return ((sec) / SECTIONS_PER_ROOT);
                        }
                        struct mem_section *section;

                        if (mem_section[root])
                            return 0;

                        section = sparse_index_alloc(nid) {
                            if (slab_is_available()) {
                                section = kzalloc_node(array_size, GFP_KERNEL, nid);
                            } else {
                                section = memblock_alloc_node(array_size, SMP_CACHE_BYTES, nid);
                            }
                            return section;
                        }
                        mem_section[root] = section;
                    }
                    set_section_nid(section, nid) {
                        section_to_node_table[section_nr] = nid;
                    }

                    ms = __nr_to_section(section) {
                        unsigned long root = SECTION_NR_TO_ROOT(nr);

                        if (unlikely(root >= NR_SECTION_ROOTS))
                            return NULL;

                    #ifdef CONFIG_SPARSEMEM_EXTREME
                        if (!mem_section || !mem_section[root])
                            return NULL;
                    #endif
                        return &mem_section[root][nr & SECTION_ROOT_MASK];
                    }
                    if (!ms->section_mem_map) {
                        ms->section_mem_map = sparse_encode_early_nid(nid) | SECTION_IS_ONLINE {
                                (nid << SECTION_NID_SHIFT) | SECTION_IS_ONLINE;
                            }
                        __section_mark_present(ms, section) {
                            ms->section_mem_map |= SECTION_MARKED_PRESENT
                        }
                    }
                }
            }
        }
    }

    pnum_begin = first_present_section_nr();
    nid_begin = sparse_early_nid(__nr_to_section(pnum_begin));

    for_each_present_section_nr(pnum_begin + 1, pnum_end) {
        int nid = sparse_early_nid(__nr_to_section(pnum_end)) {
            return (section->section_mem_map >> SECTION_NID_SHIFT);
        }

        if (nid == nid_begin) {
            map_count++;
            continue;
        }
        /* Init node with sections in range [pnum_begin, pnum_end) */
        sparse_init_nid(nid_begin, pnum_begin, pnum_end, map_count);
        nid_begin = nid;
        pnum_begin = pnum_end;
        map_count = 1;
    }

    /* cover the last node */
    sparse_init_nid(nid_begin, pnum_begin, pnum_end, map_count) {
        struct mem_section_usage *usage;
        unsigned long pnum;
        struct page *map;

        usage = sparse_early_usemaps_alloc_pgdat_section(
            NODE_DATA(nid), mem_section_usage_size() * map_count
        );

        sparse_buffer_init(map_count * section_map_size(), nid) {
            sparsemap_buf = memmap_alloc(size, section_map_size(), addr, nid, true);
            sparsemap_buf_end = sparsemap_buf + size;
        }

        for_each_present_section_nr(pnum_begin, pnum) {
            unsigned long pfn = section_nr_to_pfn(pnum);

            if (pnum >= pnum_end)
                break;

            /* 1. sparse-vmemmap.c */
            map = __populate_section_memmap(pfn, PAGES_PER_SECTION, nid, NULL, NULL) {
                /* get virt addr of this page in vmemmap area */
                unsigned long start = (unsigned long) pfn_to_page(pfn) {
                    return (pfn) + vmemmap {
                        return ((struct page *)VMEMMAP_START - (memstart_addr >> PAGE_SHIFT));
                    }
                }
                unsigned long end = start + nr_pages * sizeof(struct page);

                if (vmemmap_can_optimize(altmap, pgmap))
                    r = vmemmap_populate_compound_pages(pfn, start, end, nid, pgmap);
                else {
                    r = vmemmap_populate(start, end, nid, altmap);
                        --->
                }
            }

            /* sparse.c */
            map = __populate_section_memmap(pfn, PAGES_PER_SECTION, nid, NULL, NULL) {
                unsigned long size = section_map_size();
                struct page *map = sparse_buffer_alloc(size);
                phys_addr_t addr = __pa(MAX_DMA_ADDRESS);

                map = memmap_alloc(size, size, addr, nid, false);
            }

            check_usemap_section_nr(nid, usage);
            sparse_init_one_section(__nr_to_section(pnum), pnum, map, usage, SECTION_IS_EARLY) {
                ms->section_mem_map &= ~SECTION_MAP_MASK;
                ms->section_mem_map |= sparse_encode_mem_map(mem_map, pnum) | SECTION_HAS_MEM_MAP | flags;
                ms->usage = usage;
            }
            usage = (void *) usage + mem_section_usage_size();
        }
        sparse_buffer_fini() {
            unsigned long size = sparsemap_buf_end - sparsemap_buf;

            if (sparsemap_buf && size > 0)
                sparse_buffer_free(size);
            sparsemap_buf = NULL;
        }
    }
}
```

## vmemmap_populate

```c
vmemmap_populate(unsigned long start, unsigned long end, int node, struct vmem_altmap *altmap) {
    if (!IS_ENABLED(CONFIG_ARM64_4K_PAGES)) {
        return vmemmap_populate_basepages(start, end, node, altmap, NULL/*reuse*/) {
            return vmemmap_populate_range(start, end, node, altmap, -1, 0);
        }
    } else {
        return vmemmap_populate_hugepages(start, end, node, altmap);
    }
}

static int __meminit vmemmap_populate_range(unsigned long start,
                        unsigned long end, int node,
                        struct vmem_altmap *altmap,
                        unsigned long ptpfn,
                        unsigned long flags)
{
    unsigned long addr = start;
    pte_t *pte;

    for (; addr < end; addr += PAGE_SIZE) {
        pte = vmemmap_populate_address(addr, node, altmap, ptpfn, flags) {
            pgd_t *pgd;
            p4d_t *p4d;
            pud_t *pud;
            pmd_t *pmd;
            pte_t *pte;

            pgd = vmemmap_pgd_populate(addr, node);
            if (!pgd)
                return NULL;
            p4d = vmemmap_p4d_populate(pgd, addr, node);
            if (!p4d)
                return NULL;
            pud = vmemmap_pud_populate(p4d, addr, node);
            if (!pud)
                return NULL;

            pmd = vmemmap_pmd_populate(pud, addr, node) {
                pmd_t *pmd = pmd_offset(pud, addr);
                if (pmd_none(*pmd)) {
                    void *p = vmemmap_alloc_block_zero(PAGE_SIZE, node);
                    if (!p)
                        return NULL;
                    kernel_pte_init(p);
                    pmd_populate_kernel(&init_mm, pmd, p) {
                        VM_BUG_ON(mm && mm != &init_mm);
                        __pmd_populate(pmdp, __pa(ptep), PMD_TYPE_TABLE | PMD_TABLE_AF | PMD_TABLE_UXN) {
                            set_pmd(pmdp, __pmd(__phys_to_pmd_val(ptep) | prot));
                        }
                    }
                }
                return pmd;
            }
            if (!pmd)
                return NULL;

            pte = vmemmap_pte_populate(pmd, addr, node, altmap, ptpfn, flags) {
                pte_t *pte = pte_offset_kernel(pmd, addr);
                if (pte_none(ptep_get(pte))) {
                    pte_t entry;
                    void *p;

                    if (ptpfn == (unsigned long)-1) {
                        p = vmemmap_alloc_block_buf(PAGE_SIZE, node, altmap);
                        if (!p)
                            return NULL;
                        ptpfn = PHYS_PFN(__pa(p));
                    } else {
                        if (flags & VMEMMAP_POPULATE_PAGEREF)
                            get_page(pfn_to_page(ptpfn));
                    }
                    entry = pfn_pte(ptpfn, PAGE_KERNEL);
                    set_pte_at(&init_mm, addr, pte, entry);
                }
                return pte;
            }
            if (!pte)
                return NULL;
            vmemmap_verify(pte, node, addr, addr + PAGE_SIZE);

            return pte;
        }
        if (!pte)
            return -ENOMEM;
    }

    return 0;
}
```

```c
int __meminit vmemmap_populate_hugepages(unsigned long start, unsigned long end,
                     int node, struct vmem_altmap *altmap)
{
    unsigned long addr;
    unsigned long next;
    pgd_t *pgd;
    p4d_t *p4d;
    pud_t *pud;
    pmd_t *pmd;

    for (addr = start; addr < end; addr = next) {
        next = pmd_addr_end(addr, end);

        pgd = vmemmap_pgd_populate(addr, node) {
            pgd_t *pgd = pgd_offset_k(addr);
            if (pgd_none(*pgd)) {
                void *p = vmemmap_alloc_block_zero(PAGE_SIZE, node);
                if (!p)
                    return NULL;
                pgd_populate(&init_mm, pgd, p);
            }
        }
        if (!pgd)
            return -ENOMEM;

        p4d = vmemmap_p4d_populate(pgd, addr, node)
        if (!p4d)
            return -ENOMEM;

        pud = vmemmap_pud_populate(p4d, addr, node) {
            p4d_t *p4d = p4d_offset(pgd, addr);
            if (p4d_none(*p4d)) {
                void *p = vmemmap_alloc_block_zero(PAGE_SIZE, node);
                if (!p)
                    return NULL;
                pud_init(p);
                p4d_populate(&init_mm, p4d, p);
            }
            return p4d;
        }
        if (!pud)
            return -ENOMEM;

        pmd = pmd_offset(pud, addr);
        if (pmd_none(READ_ONCE(*pmd))) {
            void *p;

            p = vmemmap_alloc_block_buf(PMD_SIZE, node, altmap);
            if (p) {
                vmemmap_set_pmd(pmd, p, node, addr, next) {
                    pmd_set_huge(pmdp, __pa(p), __pgprot(PROT_SECT_NORMAL)) {
                        prot = mk_pmd_sect_prot(prot) {
                            return __pgprot((pgprot_val(prot) & ~PMD_TYPE_MASK) | PMD_TYPE_SECT);
                        }
                        pmd_t new_pmd = pfn_pmd(__phys_to_pfn(phys), prot);

                        /* Only allow permission changes for now */
                        if (!pgattr_change_is_safe(READ_ONCE(pmd_val(*pmdp)), pmd_val(new_pmd)))
                            return 0;

                        VM_BUG_ON(phys & ~PMD_MASK);
                        set_pmd(pmdp, new_pmd);
                        return 1;
                    }
                }
                continue;
            } else if (altmap) {
                /* No fallback: In any case we care about, the
                 * altmap should be reasonably sized and aligned
                 * such that vmemmap_alloc_block_buf() will always
                 * succeed. For consistency with the PTE case,
                 * return an error here as failure could indicate
                 * a configuration issue with the size of the altmap. */
                return -ENOMEM;
            }
        } else if (vmemmap_check_pmd(pmd, node, addr, next))
            continue;
        if (vmemmap_populate_basepages(addr, next, node, altmap))
            return -ENOMEM;
    }
    return 0;
}
```

## sparse_add_section

```c
sparse_add_section(int nid, unsigned long start_pfn,
        unsigned long nr_pages, struct vmem_altmap *altmap,
        struct dev_pagemap *pgmap)
{
    unsigned long section_nr = pfn_to_section_nr(start_pfn);
    struct mem_section *ms;
    struct page *memmap;
    int ret;

    ret = sparse_index_init(section_nr, nid);
    if (ret < 0)
        return ret;

    memmap = section_activate(nid, start_pfn, nr_pages, altmap, pgmap) {
        __populate_section_memmap();
            --->
    }
    if (IS_ERR(memmap))
        return PTR_ERR(memmap);

    /* Poison uninitialized struct pages in order to catch invalid flags
     * combinations. */
    page_init_poison(memmap, sizeof(struct page) * nr_pages);

    ms = __nr_to_section(section_nr);
    set_section_nid(section_nr, nid);
    __section_mark_present(ms, section_nr);

    /* Align memmap to section boundary in the subsection case */
    if (section_nr_to_pfn(section_nr) != start_pfn)
        memmap = pfn_to_page(section_nr_to_pfn(section_nr));
    sparse_init_one_section(ms, section_nr, memmap, ms->usage, 0) {
        ms->section_mem_map &= ~SECTION_MAP_MASK;
        ms->section_mem_map |= sparse_encode_mem_map(mem_map, pnum)
            | SECTION_HAS_MEM_MAP | flags;
        ms->usage = usage;
    }

    return 0;
}
```

## sparse_remove_section

```c
void sparse_remove_section(unsigned long pfn, unsigned long nr_pages,
        struct vmem_altmap *altmap)
{
    struct mem_section *ms = __pfn_to_section(pfn);

    if (WARN_ON_ONCE(!valid_section(ms)))
        return;

    section_deactivate(pfn, nr_pages, altmap) {
        struct mem_section *ms = __pfn_to_section(pfn);
        bool section_is_early = early_section(ms) {
            return (section && (section->section_mem_map & SECTION_IS_EARLY));
        }
        struct page *memmap = NULL;
        bool empty;

        if (clear_subsection_map(pfn, nr_pages))
            return;

        empty = is_subsection_map_empty(ms);
        if (empty) {
            unsigned long section_nr = pfn_to_section_nr(pfn);

            ms->section_mem_map &= ~SECTION_HAS_MEM_MAP;

            if (!PageReserved(virt_to_page(ms->usage))) {
                kfree_rcu(ms->usage, rcu);
                WRITE_ONCE(ms->usage, NULL);
            }
            memmap = sparse_decode_mem_map(ms->section_mem_map, section_nr);
        }

        if (!section_is_early) {
            depopulate_section_memmap(pfn, nr_pages, altmap);
        } else if (memmap) {
            free_map_bootmem(memmap) {
                unsigned long start = (unsigned long)memmap;
                unsigned long end = (unsigned long)(memmap + PAGES_PER_SECTION);

                vmemmap_free(start, end, NULL) {
                    unmap_hotplug_range(start, end, true, altmap);
                    free_empty_tables(start, end, VMEMMAP_START, VMEMMAP_END);
                }
            }
        }

        if (empty)
            ms->section_mem_map = (unsigned long)NULL;
    }
}
```

# alloc_pages

![](../images/kernel/mem-alloc_pages.svg)

* [Freezing out the page reference count](https://lwn.net/Articles/1000654/)

watermark | free area
--- | ---
![](../images/kernel/mem-alloc_pages.png) | ![](../images/kernel/mem-free_area.png)

---

![](../images/kernel/mem-alloc-watermark.png)

```c
#define ALLOC_OOM            ALLOC_NO_WATERMARKS /* performed during OOM handling */
#define ALLOC_NON_BLOCK     0x10 /* Caller cannot block. Allow access
                                * to 25% of the min watermark or
                                * 62.5% if __GFP_HIGH is set. */
#define ALLOC_MIN_RESERVE   0x20 /* __GFP_HIGH set. Allow access to 50%
                                * of the min watermark. */
#define ALLOC_CPUSET        0x40 /* check for correct cpuset */
#define ALLOC_CMA           0x80 /* allow allocations from CMA areas */
#define ALLOC_NOFRAGMENT    0x100 /* avoid mixing pageblock types */
#define ALLOC_HIGHATOMIC    0x200 /* Allows access to MIGRATE_HIGHATOMIC */
#define ALLOC_KSWAPD        0x800 /* allow waking of kswapd, __GFP_KSWAPD_RECLAIM set */

/* Flags that allow allocations below the min watermark. */
#define ALLOC_RESERVES (ALLOC_NON_BLOCK|ALLOC_MIN_RESERVE|ALLOC_HIGHATOMIC|ALLOC_OOM)
```

```c
#define alloc_pages(...)    alloc_hooks(alloc_pages_noprof(__VA_ARGS__))

static inline struct page *alloc_pages_noprof(gfp_t gfp_mask, unsigned int order)
{
    return alloc_pages_node_noprof(numa_node_id(), gfp_mask, order) {
        if (nid == NUMA_NO_NODE)
            nid = numa_mem_id();

        return __alloc_pages_node_noprof(nid, gfp_mask, order) {
            VM_BUG_ON(nid < 0 || nid >= MAX_NUMNODES);
            warn_if_node_offline(nid, gfp_mask);

            return __alloc_pages_noprof(gfp_mask, order, nid, NULL) {
                struct page *page;

                page = __alloc_frozen_pages_noprof(gfp, order, preferred_nid, nodemask);
                if (page) {
                    set_page_refcounted(page) {
                        set_page_count(page, 1);
                    }
                }
                return page;
            }
        }
    }
}

struct page *__alloc_frozen_pages_noprof(gfp_t gfp, unsigned int order,
        int preferred_nid, nodemask_t *nodemask)
{
    struct page *page;
    unsigned int alloc_flags = ALLOC_WMARK_LOW;
    gfp_t alloc_gfp; /* The gfp_t that was actually used for allocation */
    struct alloc_context ac = { };

    if (WARN_ON_ONCE_GFP(order > MAX_PAGE_ORDER, gfp))
        return NULL;

    gfp &= gfp_allowed_mask;
    gfp = current_gfp_context(gfp) {
        unsigned int pflags = READ_ONCE(current->flags);

        if (pflags & PF_MEMALLOC_NOIO)
            flags &= ~(__GFP_IO | __GFP_FS);
        else if (pflags & PF_MEMALLOC_NOFS)
            flags &= ~__GFP_FS;

        if (pflags & PF_MEMALLOC_PIN)
            flags &= ~__GFP_MOVABLE;

        return flags;
    }
    alloc_gfp = gfp;
    ret = prepare_alloc_pages(gfp, order, preferred_nid, nodemask, &ac, &alloc_gfp, &alloc_flags);
        --->
    if (!ret)
        return NULL;

   /* Forbid the first pass from falling back to types that fragment
    * memory until all local zones are considered. */
    alloc_flags |= alloc_flags_nofragment(ac.preferred_zoneref->zone, gfp) {
        unsigned int alloc_flags = (__force int) (gfp_mask & __GFP_KSWAPD_RECLAIM);
        if (defrag_mode) {
            alloc_flags |= ALLOC_NOFRAGMENT;
            return alloc_flags;
        }
    }

    /* First allocation attempt */
    page = get_page_from_freelist(alloc_gfp, order, alloc_flags, &ac);
    if (likely(page))
        goto out;

    alloc_gfp = gfp;
    ac.spread_dirty_pages = false;

    /* Restore the original nodemask if it was potentially replaced with
        * &cpuset_current_mems_allowed to optimize the fast-path attempt. */
    ac.nodemask = nodemask;

    page = __alloc_pages_slowpath(alloc_gfp, order, &ac);

out:
    if (memcg_kmem_online() && (gfp & __GFP_ACCOUNT) && page &&
        unlikely(__memcg_kmem_charge_page(page, gfp, order) != 0)) {
        __free_pages(page, order);
        page = NULL;
    }

    kmsan_alloc_page(page, order, alloc_gfp);

    return page;
}
```

## prepare_alloc_pages

```c
bool prepare_alloc_pages(gfp_t gfp_mask, unsigned int order,
    int preferred_nid, nodemask_t *nodemask,
    struct alloc_context *ac, gfp_t *alloc_gfp,
    unsigned int *alloc_flags)
{
    ac->highest_zoneidx = gfp_zone(gfp_mask) {
        #define GFP_ZONE_TABLE ( \
              (ZONE_NORMAL << 0 * GFP_ZONES_SHIFT) \
            | (OPT_ZONE_DMA << ___GFP_DMA * GFP_ZONES_SHIFT) \
            | (OPT_ZONE_HIGHMEM << ___GFP_HIGHMEM * GFP_ZONES_SHIFT) \
            | (OPT_ZONE_DMA32 << ___GFP_DMA32 * GFP_ZONES_SHIFT) \
            | (ZONE_NORMAL << ___GFP_MOVABLE * GFP_ZONES_SHIFT) \
            | (OPT_ZONE_DMA << (___GFP_MOVABLE | ___GFP_DMA) * GFP_ZONES_SHIFT) \
            | (ZONE_MOVABLE << (___GFP_MOVABLE | ___GFP_HIGHMEM) * GFP_ZONES_SHIFT)\
            | (OPT_ZONE_DMA32 << (___GFP_MOVABLE | ___GFP_DMA32) * GFP_ZONES_SHIFT)\
        )

        enum zone_type z;
        int bit = (__force int) (flags & GFP_ZONEMASK);

        z = (GFP_ZONE_TABLE >> (bit * GFP_ZONES_SHIFT)) & ((1 << GFP_ZONES_SHIFT) - 1);

        return z;
    }

    ac->zonelist = node_zonelist(preferred_nid, gfp_mask) {
        return NODE_DATA(nid)->node_zonelists + gfp_zonelist(flags) {
            #ifdef CONFIG_NUMA
                if (unlikely(flags & __GFP_THISNODE))
                    return ZONELIST_NOFALLBACK;
            #endif
                return ZONELIST_FALLBACK;
        }
    }

    ac->nodemask = nodemask;
    /* MIGRATE_UNMOVABLE (0): Pages can’t be moved (default if no flags match).
     *      Input: GFP_KERNEL (typically __GFP_RECLAIM | __GFP_ZERO,
     *          no __GFP_MOVABLE or __GFP_RECLAIMABLE):
     *      gfp_flags & GFP_MOVABLE_MASK = 0
     *      0 >> 3 = 0
     *      Result: MIGRATE_UNMOVABLE
     * MIGRATE_MOVABLE (1): Pages can be migrated (if __GFP_MOVABLE is set).
     * MIGRATE_RECLAIMABLE (2): Pages can be reclaimed (if __GFP_RECLAIMABLE is set).
     * MIGRATE_HIGHATOMIC (3): A special case.
     *      Input: __GFP_MOVABLE | __GFP_RECLAIMABLE (e.g., 0x8 | 0x10 = 0x18):
     *      gfp_flags & GFP_MOVABLE_MASK = 0x18
     *      0x18 >> 3 = 3
     *      Result: MIGRATE_HIGHATOMIC (but triggers the VM_WARN_ON). */
    ac->migratetype = gfp_migratetype(gfp_mask) {
        enum {
            ___GFP_DMA_BIT          = 0,
            ___GFP_HIGHMEM_BIT      = 1,
            ___GFP_DMA32_BIT        = 2,
            ___GFP_MOVABLE_BIT      = 3,
            ___GFP_RECLAIMABLE_BIT  = 4,
            ___GFP_HIGH_BIT         = 5,
        };
        #define GFP_MOVABLE_MASK (__GFP_RECLAIMABLE|__GFP_MOVABLE)
        #define GFP_MOVABLE_SHIFT 3
        return (gfp_flags & GFP_MOVABLE_MASK) >> GFP_MOVABLE_SHIFT;
    }

    if (cpusets_enabled()) {
        *alloc_gfp |= __GFP_HARDWALL;
        if (in_task() && !ac->nodemask)
            ac->nodemask = &cpuset_current_mems_allowed;
        else
            *alloc_flags |= ALLOC_CPUSET;
    }

    might_alloc(gfp_mask);

    ret = should_fail_alloc_page(gfp_mask, order) {
        int flags = 0;

        if (order < fail_page_alloc.min_order)
            return false;
        if (gfp_mask & __GFP_NOFAIL)
            return false;
        if (fail_page_alloc.ignore_gfp_highmem && (gfp_mask & __GFP_HIGHMEM))
            return false;
        if (fail_page_alloc.ignore_gfp_reclaim && (gfp_mask & __GFP_DIRECT_RECLAIM))
            return false;

        /* See comment in __should_failslab() */
        if (gfp_mask & __GFP_NOWARN)
            flags |= FAULT_NOWARN;

        return should_fail_ex(&fail_page_alloc.attr, 1 << order, flags) {
            bool stack_checked = false;

            if (in_task()) {
                unsigned int fail_nth = READ_ONCE(current->fail_nth);

                if (fail_nth) {
                    if (!fail_stacktrace(attr))
                        return false;

                    stack_checked = true;
                    fail_nth--;
                    WRITE_ONCE(current->fail_nth, fail_nth);
                    if (!fail_nth)
                        goto fail;

                    return false;
                }
            }

            /* No need to check any other properties if the probability is 0 */
            if (attr->probability == 0)
                return false;

            if (attr->task_filter && !fail_task(attr, current))
                return false;

            if (atomic_read(&attr->times) == 0)
                return false;

            if (!stack_checked && !fail_stacktrace(attr))
                return false;

            if (atomic_read(&attr->space) > size) {
                atomic_sub(size, &attr->space);
                return false;
            }

            if (attr->interval > 1) {
                attr->count++;
                if (attr->count % attr->interval)
                    return false;
            }

            if (attr->probability <= get_random_u32_below(100))
                return false;

        fail:
            if (!(flags & FAULT_NOWARN))
                fail_dump(attr);

            if (atomic_read(&attr->times) != -1)
                atomic_dec_not_zero(&attr->times);

            return true;
        }
    }
    if (ret)
        return false;

    *alloc_flags = gfp_to_alloc_flags_cma(gfp_mask, *alloc_flags) {
        #ifdef CONFIG_CMA
            if (gfp_migratetype(gfp_mask) == MIGRATE_MOVABLE)
                alloc_flags |= ALLOC_CMA;
        #endif
            return alloc_flags;
    }

    /* Dirty zone balancing only done in the fast path */
    ac->spread_dirty_pages = (gfp_mask & __GFP_WRITE);

    /* The preferred zone is used for statistics but crucially it is
     * also used as the `starting point` for the zonelist iterator. It
     * may get reset for allocations that ignore memory policies. */
    ac->preferred_zoneref = first_zones_zonelist(
        ac->zonelist, ac->highest_zoneidx, ac->nodemask
    );

    return true;
}
```

## get_page_from_freelist

![](../images/kernel/mem-get_page_from_freelist.png)

```c
struct page * get_page_from_freelist(
    gfp_t gfp_mask, unsigned int order, int alloc_flags,
    const struct alloc_context *ac)
{
    struct zoneref *z;
    struct zone *zone;
    struct pglist_data *last_pgdat = NULL;
    bool last_pgdat_dirty_ok = false;
    bool no_fallback;

retry:
    /* Scan zonelist, looking for a zone with enough free.
     * See also cpuset_node_allowed() comment in kernel/cgroup/cpuset.c. */
    no_fallback = alloc_flags & ALLOC_NOFRAGMENT;
    z = ac->preferred_zoneref;
    for_next_zone_zonelist_nodemask(zone, z, ac->highest_zoneidx, ac->nodemask) {
        struct page *page;
        unsigned long mark;

        if (cpusets_enabled() &&
            (alloc_flags & ALLOC_CPUSET) &&
            !__cpuset_zone_allowed(zone, gfp_mask))
                continue;

        if (ac->spread_dirty_pages) {
            if (last_pgdat != zone->zone_pgdat) {
                last_pgdat = zone->zone_pgdat;
                last_pgdat_dirty_ok = node_dirty_ok(zone->zone_pgdat) {
                    unsigned long limit = node_dirty_limit(pgdat);
                    unsigned long nr_pages = 0;

                    nr_pages += node_page_state(pgdat, NR_FILE_DIRTY);
                    nr_pages += node_page_state(pgdat, NR_WRITEBACK);

                    return nr_pages <= limit;
                }
            }

            if (!last_pgdat_dirty_ok)
                continue;
        }

        if (no_fallback && nr_online_nodes > 1 && zone != ac->preferred_zoneref->zone) {
            int local_nid;

            /* If moving to a remote node, retry but allow
             * fragmenting fallbacks. Locality is more important
             * than fragmentation avoidance. */
            local_nid = zone_to_nid(ac->preferred_zoneref->zone);
            if (zone_to_nid(zone) != local_nid) {
                alloc_flags &= ~ALLOC_NOFRAGMENT;
                goto retry;
            }
        }

        cond_accept_memory(zone, order, alloc_flags);

        /* Detect whether the number of free pages is below high
         * watermark.  If so, we will decrease pcp->high and free
         * PCP pages in free path to reduce the possibility of
         * premature page reclaiming.  Detection is done here to
         * avoid to do that in hotter free path. */
        if (test_bit(ZONE_BELOW_HIGH, &zone->flags))
            goto check_alloc_wmark;

        /* fast check high wmark */
        mark = high_wmark_pages(zone);
        ret = zone_watermark_fast(zone, order, mark, ac->highest_zoneidx, alloc_flags, gfp_mask);
        if (ret) {
            goto try_this_zone;
        } else
            set_bit(ZONE_BELOW_HIGH, &zone->flags);

/* check real mwark */
check_alloc_wmark:
        mark = wmark_pages(zone, alloc_flags & ALLOC_WMARK_MASK) {
            return (z->_watermark[i] + z->watermark_boost)
        }
        if (!zone_watermark_fast(zone, order, mark, ac->highest_zoneidx, alloc_flags, gfp_mask)) {
            int ret;

            if (cond_accept_memory(zone, order, alloc_flags))
                goto try_this_zone;

            if (deferred_pages_enabled()) {
                if (_deferred_grow_zone(zone, order))
                    goto try_this_zone;
            }

            /* Checked here to keep the fast path fast */
            BUILD_BUG_ON(ALLOC_NO_WATERMARKS < NR_WMARK);
            if (alloc_flags & ALLOC_NO_WATERMARKS)
                goto try_this_zone;

            ret = zone_allows_reclaim(ac->preferred_zoneref->zone, zone) {
                return node_distance(zone_to_nid(local_zone), zone_to_nid(zone))
                    <= node_reclaim_distance/*30*/;
            }
            ret1 = node_reclaim_enabled(void) {
                return node_reclaim_mode & (RECLAIM_ZONE|RECLAIM_WRITE|RECLAIM_UNMAP);
            }
            if (!ret1 || !ret)
                continue;

            ret = node_reclaim(zone->zone_pgdat, gfp_mask, order);
            switch (ret) {
            case NODE_RECLAIM_NOSCAN: /* did not scan */
                continue;
            case NODE_RECLAIM_FULL: /* scanned but unreclaimable */
                continue;
            default: /* did we reclaim enough */
                if (zone_watermark_ok(zone, order, mark, ac->highest_zoneidx, alloc_flags))
                    goto try_this_zone;

                continue;
            }
        }

try_this_zone:
        page = rmqueue(ac->preferred_zoneref->zone, zone, order, gfp_mask, alloc_flags, ac->migratetype);
        if (page) {
            prep_new_page(page, order, gfp_mask, alloc_flags) {
                post_alloc_hook(page, order, gfp_flags);

                if (order && (gfp_flags & __GFP_COMP)) {
                    prep_compound_page(page, order) {
                        int i;
                        int nr_pages = 1 << order;

                        __SetPageHead(page);
                        for (i = 1; i < nr_pages; i++) {
                            prep_compound_tail(page, i) {
                                struct page *p = head + tail_idx;

                                p->mapping = TAIL_MAPPING;
                                set_compound_head(p, head) {
                                    WRITE_ONCE(page->compound_head, (unsigned long)head + 1);
                                }
                                set_page_private(p, 0) ;
                            }
                        }

                        prep_compound_head(page, order) {
                            struct folio *folio = (struct folio *)page;

                            folio_set_order(folio, order) {
                                if (WARN_ON_ONCE(!order || !folio_test_large(folio)))
                                        return;
                                    folio->_flags_1 = (folio->_flags_1 & ~0xffUL) | order;
                                #ifdef NR_PAGES_IN_LARGE_FOLIO
                                    folio->_nr_pages = 1U << order;
                                #endif
                            }

                            atomic_set(&folio->_large_mapcount, -1);
                            if (IS_ENABLED(CONFIG_PAGE_MAPCOUNT))
                                atomic_set(&folio->_nr_pages_mapped, 0);
                            if (IS_ENABLED(CONFIG_MM_ID)) {
                                folio->_mm_ids = 0;
                                folio->_mm_id_mapcount[0] = -1;
                                folio->_mm_id_mapcount[1] = -1;
                            }
                            if (IS_ENABLED(CONFIG_64BIT) || order > 1) {
                                atomic_set(&folio->_pincount, 0);
                                atomic_set(&folio->_entire_mapcount, -1);
                            }
                            if (order > 1)
                                INIT_LIST_HEAD(&folio->_deferred_list);
                        }
                    }
                }

                if (alloc_flags & ALLOC_NO_WATERMARKS)
                    set_page_pfmemalloc(page);
                else
                    clear_page_pfmemalloc(page);
            }

            /* If this is a high-order atomic allocation then check
             * if the pageblock should be reserved for the future */
            if (unlikely(alloc_flags & ALLOC_HIGHATOMIC))
                reserve_highatomic_pageblock(page, zone);

            return page;
        } else {
            if (has_unaccepted_memory()) {
                if (try_to_accept_memory(zone, order)) {
                    goto try_this_zone;
                }
            }
        }
    }

    /* It's possible on a UMA machine to get through all zones that are
     * fragmented. If avoiding fragmentation, reset and try again. */
    if (no_fallback) {
        alloc_flags &= ~ALLOC_NOFRAGMENT;
        goto retry;
    }

    return NULL;
}
```

### zone_watermark_ok

```c
static inline bool zone_watermark_fast(
    struct zone *z, unsigned int order,
    unsigned long mark, int highest_zoneidx,
    unsigned int alloc_flags, gfp_t gfp_mask) {

    long free_pages;
    free_pages = zone_page_state(z, NR_FREE_PAGES) {
        return atomic_long_read(&zone->vm_stat[item]);
    }

    /* Fast check for order-0 only. If this fails then the reserves
     * need to be calculated. */
    if (!order) {
        long usable_free;
        long reserved;

        usable_free = free_pages;
        reserved = __zone_watermark_unusable_free(z, 0, alloc_flags);
            --->
        /* reserved may over estimate high-atomic reserves. */
        usable_free -= min(usable_free, reserved);
        if (usable_free > mark + z->lowmem_reserve[highest_zoneidx])
            return true;
    }

    /* Return true if free base pages are above 'mark' */
    ret = __zone_watermark_ok(z, order, mark, highest_zoneidx, alloc_flags, free_pages) {
        long min = mark;
        int o;

        free_pages -= __zone_watermark_unusable_free(z, order, alloc_flags) {
            long unusable_free = (1 << order) - 1;
            /* #define ALLOC_RESERVES (ALLOC_NON_BLOCK|ALLOC_MIN_RESERVE|ALLOC_HIGHATOMIC|ALLOC_OOM) */
            if (likely(!(alloc_flags & ALLOC_RESERVES))) {
                unusable_free += READ_ONCE(z->nr_free_highatomic);
            }
            if (!(alloc_flags & ALLOC_CMA)) {
                unusable_free += zone_page_state(z, NR_FREE_CMA_PAGES);
            }
            return unusable_free;
        }

        if (unlikely(alloc_flags & ALLOC_RESERVES)) {
            /* __GFP_HIGH set. Allow access to 50% of the min watermark. */
            if (alloc_flags & ALLOC_MIN_RESERVE) {
                min -= min / 2;
                if (alloc_flags & ALLOC_NON_BLOCK) {
                    min -= min / 4;
                }
            }
            if (alloc_flags & ALLOC_OOM) {
                min -= min / 2;
            }
        }

        if (free_pages <= min + z->lowmem_reserve[highest_zoneidx])
            return false;

        if (!order)
            return true;

        for (o = order; o < NR_PAGE_ORDERS; o++) {
            struct free_area *area = &z->free_area[o];
            int mt;

            if (!area->nr_free)
                continue;

            for (mt = 0; mt < MIGRATE_PCPTYPES; mt++) {
                if (!free_area_empty(area, mt))
                    return true;
            }
            if ((alloc_flags & ALLOC_CMA) && !free_area_empty(area, MIGRATE_CMA)) {
                return true;
            }
            if ((alloc_flags & (ALLOC_HIGHATOMIC|ALLOC_OOM)) &&
                !free_area_empty(area, MIGRATE_HIGHATOMIC)) {
                return true;
            }
        }
        return false;
    }
    if (ret) {
        return true;
    }

    /* Ignore watermark boosting for __GFP_HIGH order-0 allocations
     * when checking the min watermark. The min watermark is the
     * point where boosting is ignored so that kswapd is woken up
     * when below the low watermark. */
    if (unlikely(!order && (alloc_flags & ALLOC_MIN_RESERVE) && z->watermark_boost
        && ((alloc_flags & ALLOC_WMARK_MASK) == WMARK_MIN))) {

        mark = z->_watermark[WMARK_MIN];
        return __zone_watermark_ok(z, order, mark, highest_zoneidx,
                    alloc_flags, free_pages);
    }

    return false;
}
```

### rmqueue

```c
struct page *rmqueue(struct zone *preferred_zone,
            struct zone *zone, unsigned int order,
            gfp_t gfp_flags, unsigned int alloc_flags,
            int migratetype)
{
    struct page *page;

    if (likely(pcp_allowed_order(order)) {
        if (order <= PAGE_ALLOC_COSTLY_ORDER)
                return true;
        #ifdef CONFIG_TRANSPARENT_HUGEPAGE
            if (order == HPAGE_PMD_ORDER)
                return true;
        #endif
            return false;
    }) {
        page = rmqueue_pcplist(preferred_zone, zone, order, migratetype, alloc_flags);
        if (likely(page))
            goto out;
    }

    page = rmqueue_buddy(preferred_zone, zone, order, alloc_flags, migratetype) {
        struct page *page;
        unsigned long flags;

        do {
            page = NULL;
            if (unlikely(alloc_flags & ALLOC_TRYLOCK)) {
                if (!spin_trylock_irqsave(&zone->lock, flags))
                    return NULL;
            } else {
                spin_lock_irqsave(&zone->lock, flags);
            }

            if (alloc_flags & ALLOC_HIGHATOMIC) {
                page = __rmqueue_smallest(zone, order, MIGRATE_HIGHATOMIC);
            }
            if (!page) {
                enum rmqueue_mode rmqm = RMQUEUE_NORMAL;
                page = __rmqueue(zone, order, migratetype, alloc_flags, &rmqueue_mode) {
                    struct page *page;

                    if (IS_ENABLED(CONFIG_CMA)) {
                        if (alloc_flags & ALLOC_CMA &&
                            zone_page_state(zone, NR_FREE_CMA_PAGES) >
                            zone_page_state(zone, NR_FREE_PAGES) / 2) {

                            page = __rmqueue_cma_fallback(zone, order) {
                                return __rmqueue_smallest(zone, order, MIGRATE_CMA);
                            }
                            if (page)
                                return page;
                        }
                    }

                    switch (*mode) {
                    case RMQUEUE_NORMAL:
                        page = __rmqueue_smallest(zone, order, migratetype);
                        if (page)
                            return page;
                        fallthrough;
                    case RMQUEUE_CMA:
                        if (alloc_flags & ALLOC_CMA) {
                            page = __rmqueue_cma_fallback(zone, order) {
                                return __rmqueue_smallest(zone, order, MIGRATE_CMA);
                            }
                            if (page) {
                                *mode = RMQUEUE_CMA;
                                return page;
                            }
                        }
                        fallthrough;
                    case RMQUEUE_CLAIM:
                        page = __rmqueue_claim(zone, order, migratetype, alloc_flags);
                        if (page) {
                            /* Replenished preferred freelist, back to normal mode. */
                            *mode = RMQUEUE_NORMAL;
                            return page;
                        }
                        fallthrough;
                    case RMQUEUE_STEAL:
                        if (!(alloc_flags & ALLOC_NOFRAGMENT)) {
                            page = __rmqueue_steal(zone, order, migratetype) {
                                struct free_area *area;
                                int current_order;
                                struct page *page;
                                int fallback_mt;

                                for (current_order = order; current_order < NR_PAGE_ORDERS; current_order++) {
                                    area = &(zone->free_area[current_order]);
                                    fallback_mt = find_suitable_fallback(area, current_order,
                                                        start_migratetype, false);
                                    if (fallback_mt == -1)
                                        continue;

                                    page = get_page_from_free_area(area, fallback_mt);
                                    page_del_and_expand(zone, page, order, current_order, fallback_mt);
                                    return page;
                                }

                                return NULL;
                            }
                            if (page) {
                                *mode = RMQUEUE_STEAL;
                                return page;
                            }
                        }
                    }
                    return NULL;
                }

                if (!page && (alloc_flags & (ALLOC_OOM|ALLOC_NON_BLOCK))) {
                    page = __rmqueue_smallest(zone, order, MIGRATE_HIGHATOMIC);
                }

                if (!page) {
                    spin_unlock_irqrestore(&zone->lock, flags);
                    return NULL;
                }
            }

            spin_unlock_irqrestore(&zone->lock, flags);
        } while (check_new_pages(page, order));

        __count_zid_vm_events(PGALLOC, page_zonenum(page), 1 << order);
        zone_statistics(preferred_zone, zone, 1);

        return page;
    }

out:
    /* memory steal happens, wakeup kswapd to compact memory */
    if ((alloc_flags & ALLOC_KSWAPD)
        && unlikely(test_bit(ZONE_BOOSTED_WATERMARK, &zone->flags))) {

        clear_bit(ZONE_BOOSTED_WATERMARK, &zone->flags);
        wakeup_kswapd(zone, 0, 0, zone_idx(zone));
    }

    return page;
}
```

### rmqueue_smallest

```c
struct page *__rmqueue_smallest(struct zone *zone, unsigned int order, int migratetype)
{
    unsigned int current_order;
    struct free_area *area;
    struct page *page;

    /* Find a page of the appropriate size in the preferred list */
    for (current_order = order; current_order < NR_PAGE_ORDERS; ++current_order) {
        area = &(zone->free_area[current_order]);
        page = get_page_from_free_area(area, migratetype) {
            return list_first_entry_or_null(&area->free_list[migratetype], struct page, buddy_list);
        }
        if (!page)
            continue;

        page_del_and_expand(zone, page, order, current_order,migratetype) {
            int nr_pages = 1 << high;

            __del_page_from_free_list(page, zone, high, migratetype){
                int nr_pages = 1 << order;

                /* clear reported state and update reported page count */
                if (page_reported(page))
                    __ClearPageReported(page);

                list_del(&page->buddy_list);
                __ClearPageBuddy(page);
                set_page_private(page, 0);
                zone->free_area[order].nr_free--;

                if (order >= pageblock_order && !is_migrate_isolate(migratetype))
                    __mod_zone_page_state(zone, NR_FREE_PAGES_BLOCKS, -nr_pages);
            }
            nr_pages -= expand(zone, page, low, high, migratetype) {
                unsigned long size = 1 << high;

                while (high > low) {
                    high--;
                    size >>= 1;

                    ret = set_page_guard(zone, &page[size], high, migratetype) {
                        if (!debug_guardpage_enabled())
                            return false;
                        return __set_page_guard(zone, page, order) {
                            if (order >= debug_guardpage_minorder())
                                return false;

                            __SetPageGuard(page);
                            INIT_LIST_HEAD(&page->buddy_list);
                            set_page_private(page, order) {
                                page->private = private;
                            }

                            return true;
                        }
                    }
                    if (ret) /* continue when success */
                        continue;

                    __add_to_free_list(&page[size], zone, high/*order*/, migratetype, bool tail) {
                        struct free_area *area = &zone->free_area[order];
                        int nr_pages = 1 << order;

                        if (tail)
                            list_add_tail(&page->buddy_list, &area->free_list[migratetype]);
                        else
                            list_add(&page->buddy_list, &area->free_list[migratetype]);
                        area->nr_free++;

                        if (order >= pageblock_order && !is_migrate_isolate(migratetype))
                            __mod_zone_page_state(zone, NR_FREE_PAGES_BLOCKS, nr_pages);
                    }

                    set_buddy_order(&page[size], high) {
                        set_page_private(page, order) {
                            page->private = private;
                        }
                        __SetPageBuddy(page);
                    }
                }
            }
            account_freepages(zone, -nr_pages, migratetype);
        }

        return page;
    }

    return NULL;
}
```

### rmqueue_pcplist

```c
struct per_cpu_pages {
    spinlock_t lock;    /* Protects lists field */
    int count;          /* number of pages in the list */
    int high;           /* high watermark, emptying needed */
    int high_min;       /* min high watermark */
    int high_max;       /* max high watermark */
    int batch;          /* chunk size for buddy add/remove */
    u8 flags;           /* protected by pcp->lock */
    u8 alloc_factor;    /* batch scaling factor during allocate */
    u8 expire;          /* When 0, remote pagesets are drained */
    short free_count;   /* consecutive free count */

    /* Lists of pages, one per migrate type stored on the pcp-lists */
    struct list_head lists[NR_PCP_LISTS];
};
```

```c
struct page *rmqueue_pcplist(struct zone *preferred_zone,
            struct zone *zone, unsigned int order,
            int migratetype, unsigned int alloc_flags)
{
    struct per_cpu_pages *pcp;
    struct list_head *list;
    struct page *page;
    unsigned long __maybe_unused UP_flags;

    /* spin_trylock may fail due to a parallel drain or IRQ reentrancy. */
    pcp_trylock_prepare(UP_flags);
    pcp = pcp_spin_trylock(zone->per_cpu_pageset);
    if (!pcp) {
        pcp_trylock_finish(UP_flags);
        return NULL;
    }

    pcp->free_count >>= 1;
    ret = order_to_pindex(migratetype, order) {
        #ifdef CONFIG_TRANSPARENT_HUGEPAGE
            if (order > PAGE_ALLOC_COSTLY_ORDER) {
                VM_BUG_ON(order != pageblock_order);
                return NR_LOWORDER_PCP_LISTS;
            }
        #else
            VM_BUG_ON(order > PAGE_ALLOC_COSTLY_ORDER);
        #endif

        return (MIGRATE_PCPTYPES * order) + migratetype;
    }
    list = &pcp->lists[ret];

    page = __rmqueue_pcplist(zone, order, migratetype, alloc_flags, pcp, list) {
        struct page *page;

        do {
            if (list_empty(list)) {
                int batch = nr_pcp_alloc(pcp, zone, order);
                int alloced;

                alloced = rmqueue_bulk(zone, order, batch/*count*/, list, migratetype, alloc_flags) {
                    enum rmqueue_mode rmqm = RMQUEUE_NORMAL;
                    unsigned long flags;
                    int i;

                    if (unlikely(alloc_flags & ALLOC_TRYLOCK)) {
                        if (!spin_trylock_irqsave(&zone->lock, flags))
                            return 0;
                    } else {
                        spin_lock_irqsave(&zone->lock, flags);
                    }
                    for (i = 0; i < count; ++i) {
                        struct page *page = __rmqueue(zone, order, migratetype, alloc_flags, &rmqm);
                        if (unlikely(page == NULL))
                            break;

                        list_add_tail(&page->pcp_list, list);
                    }
                    spin_unlock_irqrestore(&zone->lock, flags);

                    return i;
                }

                pcp->count += alloced << order;
                if (unlikely(list_empty(list)))
                    return NULL;
            }

            page = list_first_entry(list, struct page, pcp_list);
            list_del(&page->pcp_list);
            pcp->count -= 1 << order;
        } while (check_new_pages(page, order));

        return page;
    }

    pcp_spin_unlock(pcp);
    pcp_trylock_finish(UP_flags);
    if (page) {
        __count_zid_vm_events(PGALLOC, page_zonenum(page), 1 << order);
        zone_statistics(preferred_zone, zone, 1);
    }
    return page;
}
```

### rmqueue_claim

```c
static int fallbacks[MIGRATE_TYPES][MIGRATE_PCPTYPES - 1] = {
    [MIGRATE_UNMOVABLE]   = { MIGRATE_RECLAIMABLE, MIGRATE_MOVABLE   },
    [MIGRATE_MOVABLE]     = { MIGRATE_RECLAIMABLE, MIGRATE_UNMOVABLE },
    [MIGRATE_RECLAIMABLE] = { MIGRATE_UNMOVABLE,   MIGRATE_MOVABLE   },
};

bool __rmqueue_claim(struct zone *zone, int order, int start_migratetype,
                        unsigned int alloc_flags)
{
    struct free_area *area;
    int current_order;
    int min_order = order;
    struct page *page;
    int fallback_mt;
    bool can_steal;

    if (order < pageblock_order && alloc_flags & ALLOC_NOFRAGMENT)
        min_order = pageblock_order;

    /* Find the largest available free page in the other list. This roughly
     * approximates finding the pageblock with the most free pages, which
     * would be too costly to do exactly. */
    for (current_order = MAX_PAGE_ORDER; current_order >= min_order; --current_order) {
        area = &(zone->free_area[current_order]);
        fallback_mt = find_suitable_fallback(area, current_order,
            start_migratetype/*migratetype*/, bool claimable) {

            int i;
            int fallback_mt;

            ret = should_try_claim_block(order, migratetype) {
                if (order >= pageblock_order)
                    return true;
                if (order >= pageblock_order / 2)
                    return true;
                if (start_mt == MIGRATE_RECLAIMABLE || start_mt == MIGRATE_UNMOVABLE)
                    return true;

                if (page_group_by_mobility_disabled)
                    return true;
                return false;
            }
            if (claimable && !ret)
                return -2;

            if (area->nr_free == 0)
                return -1;

            *can_steal = false;
            for (i = 0; i < MIGRATE_PCPTYPES - 1 ; i++) {
                fallback_mt = fallbacks[migratetype][i];
                if (free_area_empty(area, fallback_mt))
                    continue;

                ret = can_steal_fallback(order, migratetype/*start_mt*/) {
                    if (order >= pageblock_order)
                        return true;

                    if (order >= pageblock_order / 2
                        || start_mt == MIGRATE_RECLAIMABLE
                        || start_mt == MIGRATE_UNMOVABLE
                        || page_group_by_mobility_disabled) {

                        return true;
                    }

                    return false;
                }
                if (ret) {
                    *can_steal = true;
                }

                if (!only_stealable)
                    return fallback_mt;

                if (*can_steal)
                    return fallback_mt;
            }

            return -1;
        }
        if (fallback_mt == -1)
            continue;

        /* Advanced into orders too low to claim, abort */
        if (fallback_mt == -2)
            break;

        page = get_page_from_free_area(area, fallback_mt);
        page = try_to_claim_block(zone, page, current_order, order,
            start_migratetype, fallback_mt, alloc_flags) {

            int free_pages, movable_pages, alike_pages;
            unsigned long start_pfn;

            /* Take ownership for orders >= pageblock_order */
            if (current_order >= pageblock_order) {
                unsigned int nr_added;

                del_page_from_free_list(page, zone, current_order, block_type);
                change_pageblock_range(page, current_order, start_type) {
                    int nr_pageblocks = 1 << (start_order - pageblock_order);

                    while (nr_pageblocks--) {
                        set_pageblock_migratetype(pageblock_page, migratetype);
                        pageblock_page += pageblock_nr_pages;
                    }
                }
                nr_added = expand(zone, page, order, current_order, start_type);
                account_freepages(zone, nr_added, start_type);
                return page;
            }

            /* Boost watermarks to increase reclaim pressure to reduce the
            * likelihood of future fallbacks. Wake kswapd now as the node
            * may be balanced overall and kswapd will not wake naturally. */
            if (boost_watermark(zone) && (alloc_flags & ALLOC_KSWAPD))
                set_bit(ZONE_BOOSTED_WATERMARK, &zone->flags);

            /* moving whole block can fail due to zone boundary conditions */
            ret = prep_move_freepages_block(zone, page, &start_pfn,
                &free_pages/* num_free */, &movable_pages/* num_movable */) {

                unsigned long pfn, start, end;

                pfn = page_to_pfn(page);
                start = pageblock_start_pfn(pfn);
                end = pageblock_end_pfn(pfn);

                if (!zone_spans_pfn(zone, start))
                    return false;
                if (!zone_spans_pfn(zone, end - 1))
                    return false;

                *start_pfn = start;

                if (num_free) {
                    *num_free = 0;
                    *num_movable = 0;
                    for (pfn = start; pfn < end;) {
                        page = pfn_to_page(pfn);
                        if (PageBuddy(page)) {
                            int nr = 1 << buddy_order(page);

                            *num_free += nr;
                            pfn += nr;
                            continue;
                        }
                        /* We assume that pages that could be isolated for
                        * migration are movable. But we don't actually try
                        * isolating, as that would be expensive. */
                        if (PageLRU(page) || page_has_movable_ops(page))
                            (*num_movable)++;
                        pfn++;
                    }
                }

                return true;
            }
            if (!ret)
                return NULL;

            /* Determine how many pages are compatible with our allocation.
            * For movable allocation, it's the number of movable pages which
            * we just obtained. For other types it's a bit more tricky. */
            if (start_type == MIGRATE_MOVABLE) {
                alike_pages = movable_pages;
            } else {
                /* If we are falling back a RECLAIMABLE or UNMOVABLE allocation
                * to MOVABLE pageblock, consider all non-movable pages as
                * compatible. If it's UNMOVABLE falling back to RECLAIMABLE or
                * vice versa, be conservative since we can't distinguish the
                * exact migratetype of non-movable pages. */
                if (block_type == MIGRATE_MOVABLE)
                    alike_pages = pageblock_nr_pages
                                - (free_pages + movable_pages);
                else
                    alike_pages = 0;
            }
            /* If a sufficient number of pages in the block are either free or of
            * compatible migratability as our allocation, claim the whole block. */
            if (free_pages + alike_pages >= (1 << (pageblock_order-1)) ||
                    page_group_by_mobility_disabled) {
                __move_freepages_block(zone, start_pfn, block_type, start_type);
                set_pageblock_migratetype(pfn_to_page(start_pfn), start_type);
                return __rmqueue_smallest(zone, order, start_type);
            }

            return NULL;
        }
        if (page) {
            return page;
        }
    }

    return NULL;
}
```

### node_reclaim

```c
int node_reclaim(struct pglist_data *pgdat, gfp_t gfp_mask, unsigned int order)
{
    int ret;

    /* Node reclaim reclaims unmapped file backed pages and
    * slab pages if we are over the defined limits.
    *
    * A small portion of unmapped file backed pages is needed for
    * file I/O otherwise pages read by file I/O will be immediately
    * thrown out if the node is overallocated. So we do not reclaim
    * if less than a specified percentage of the node is used by
    * unmapped file backed pages. */
    if (node_pagecache_reclaimable(pgdat) <= pgdat->min_unmapped_pages &&
        node_page_state_pages(pgdat, NR_SLAB_RECLAIMABLE_B) <=
        pgdat->min_slab_pages)
        return NODE_RECLAIM_FULL;

    /* Do not scan if the allocation should not be delayed. */
    if (!gfpflags_allow_blocking(gfp_mask) || (current->flags & PF_MEMALLOC))
        return NODE_RECLAIM_NOSCAN;

    /* Only run node reclaim on the local node or on nodes that do not
    * have associated processors. This will favor the local processor
    * over remote processors and spread off node memory allocations
    * as wide as possible. */
    if (node_state(pgdat->node_id, N_CPU) && pgdat->node_id != numa_node_id())
        return NODE_RECLAIM_NOSCAN;

    if (test_and_set_bit(PGDAT_RECLAIM_LOCKED, &pgdat->flags))
        return NODE_RECLAIM_NOSCAN;

    ret = __node_reclaim(pgdat, gfp_mask, order) {
        /* Minimum pages needed in order to stay on node */
        const unsigned long nr_pages = 1 << order;
        struct task_struct *p = current;
        unsigned int noreclaim_flag;
        struct scan_control sc = {
            .nr_to_reclaim = max(nr_pages, SWAP_CLUSTER_MAX),
            .gfp_mask = current_gfp_context(gfp_mask),
            .order = order,
            .priority = NODE_RECLAIM_PRIORITY,
            .may_writepage = !!(node_reclaim_mode & RECLAIM_WRITE),
            .may_unmap = !!(node_reclaim_mode & RECLAIM_UNMAP),
            .may_swap = 1,
            .reclaim_idx = gfp_zone(gfp_mask),
        };
        unsigned long pflags;

        cond_resched();
        psi_memstall_enter(&pflags);
        delayacct_freepages_start();
        fs_reclaim_acquire(sc.gfp_mask);
        /* We need to be able to allocate from the reserves for RECLAIM_UNMAP */
        noreclaim_flag = memalloc_noreclaim_save();
        set_task_reclaim_state(p, &sc.reclaim_state);

        if (node_pagecache_reclaimable(pgdat) > pgdat->min_unmapped_pages ||
            node_page_state_pages(pgdat, NR_SLAB_RECLAIMABLE_B) > pgdat->min_slab_pages) {
            /* Free memory by calling shrink node with increasing
            * priorities until we have enough memory freed. */
            do {
                shrink_node(pgdat, &sc);
                    --->
            } while (sc.nr_reclaimed < nr_pages && --sc.priority >= 0);
        }

        set_task_reclaim_state(p, NULL);
        memalloc_noreclaim_restore(noreclaim_flag);
        fs_reclaim_release(sc.gfp_mask);
        psi_memstall_leave(&pflags);
        delayacct_freepages_end();

        return sc.nr_reclaimed >= nr_pages;
    }
    clear_bit(PGDAT_RECLAIM_LOCKED, &pgdat->flags);

    if (!ret)
        count_vm_event(PGSCAN_ZONE_RECLAIM_FAILED);

    return ret;
}
```

### cond_accept_memory

### _deferred_grow_zone

## alloc_pages_slowpath

```c
struct page *
__alloc_pages_slowpath(gfp_t gfp_mask, unsigned int order,
                        struct alloc_context *ac)
{
    bool can_direct_reclaim = gfp_mask & __GFP_DIRECT_RECLAIM;
    const bool costly_order = order > PAGE_ALLOC_COSTLY_ORDER;
    struct page *page = NULL;
    unsigned int alloc_flags;
    unsigned long did_some_progress;
    enum compact_priority compact_priority;
    enum compact_result compact_result;
    int compaction_retries;
    int no_progress_loops;
    unsigned int cpuset_mems_cookie;
    unsigned int zonelist_iter_cookie;
    int reserve_flags;

restart:
    compaction_retries = 0;
    no_progress_loops = 0;
    compact_priority = DEF_COMPACT_PRIORITY;
    cpuset_mems_cookie = read_mems_allowed_begin();
    zonelist_iter_cookie = zonelist_iter_begin();

    /* The fast path uses conservative alloc_flags to succeed only until
     * kswapd needs to be woken up, and to avoid the cost of setting up
     * alloc_flags precisely. So we do that now. */
    alloc_flags = gfp_to_alloc_flags(gfp_mask, order) {
        unsigned int alloc_flags = ALLOC_WMARK_MIN | ALLOC_CPUSET;

        alloc_flags |= (__force int)
            (gfp_mask & (__GFP_HIGH | __GFP_KSWAPD_RECLAIM));

        if (!(gfp_mask & __GFP_DIRECT_RECLAIM)) {
            if (!(gfp_mask & __GFP_NOMEMALLOC)) {
                alloc_flags |= ALLOC_NON_BLOCK;

                if (order > 0) {
                    alloc_flags |= ALLOC_HIGHATOMIC;
                }
            }

            if (alloc_flags & ALLOC_MIN_RESERVE)
                alloc_flags &= ~ALLOC_CPUSET;
        } else if (unlikely(rt_task(current)) && in_task())
            alloc_flags |= ALLOC_MIN_RESERVE;

        alloc_flags = gfp_to_alloc_flags_cma(gfp_mask, alloc_flags);

        return alloc_flags;
    }

    /* We need to recalculate the starting point for the zonelist iterator
    * because we might have used different nodemask in the fast path, or
    * there was a cpuset modification and we are retrying - otherwise we
    * could end up iterating over non-eligible zones endlessly. */
    ac->preferred_zoneref = first_zones_zonelist(
        ac->zonelist, ac->highest_zoneidx, ac->nodemask);
    if (!ac->preferred_zoneref->zone)
        goto nopage;

    /* Check for insane configurations where the cpuset doesn't contain
    * any suitable zone to satisfy the request - e.g. non-movable
    * GFP_HIGHUSER allocations from MOVABLE nodes only. */
    if (cpusets_insane_config() && (gfp_mask & __GFP_HARDWALL)) {
        struct zoneref *z = first_zones_zonelist(
            ac->zonelist,
            ac->highest_zoneidx,
            &cpuset_current_mems_allowed);
        if (!z->zone)
            goto nopage;
    }

    if (alloc_flags & ALLOC_KSWAPD)
        wake_all_kswapds(order, gfp_mask, ac);

    /* The adjusted alloc_flags might result in immediate success, so try
     * that first */
    page = get_page_from_freelist(gfp_mask, order, alloc_flags, ac);
    if (page)
        goto got_pg;

    /* For costly allocations, try direct compaction first, as it's likely
     * that we have enough base pages and don't need to reclaim. For non-
     * movable high-order allocations, do that as well, as compaction will
     * try prevent permanent fragmentation by migrating from blocks of the
     * same migratetype.
     * Don't try this for allocations that are allowed to ignore
     * watermarks, as the ALLOC_NO_WATERMARKS attempt didn't yet happen. */
    if (can_direct_reclaim
        && (costly_order || (order > 0 && ac->migratetype != MIGRATE_MOVABLE))
        && !gfp_pfmemalloc_allowed(gfp_mask)) {

        page = __alloc_pages_direct_compact(gfp_mask, order,
                        alloc_flags, ac,
                        INIT_COMPACT_PRIORITY,
                        &compact_result);
        if (page)
            goto got_pg;

        /* Checks for costly allocations with __GFP_NORETRY, which
         * includes some THP page fault allocations */
        if (costly_order && (gfp_mask & __GFP_NORETRY)) {
            if (compact_result == COMPACT_SKIPPED || compact_result == COMPACT_DEFERRED)
                goto nopage;

            /* Looks like reclaim/compaction is worth trying, but
             * sync compaction could be very expensive, so keep
             * using async compaction. */
            compact_priority = INIT_COMPACT_PRIORITY;
        }
    }

retry:
    /* Deal with possible cpuset update races or zonelist updates to avoid
    * infinite retries. */
    if (check_retry_cpuset(cpuset_mems_cookie, ac) ||
        check_retry_zonelist(zonelist_iter_cookie))
        goto restart;

    /* Ensure kswapd doesn't accidentally go to sleep as long as we loop */
    if (alloc_flags & ALLOC_KSWAPD)
        wake_all_kswapds(order, gfp_mask, ac);

    /* "Process Flag: Memory Allocation" and signals that this task is special
     * when it comes to memory allocation—it’s allowed to bypass normal restrictions
     * and tap into reserved memory pools. */
    reserve_flags = __gfp_pfmemalloc_flags(gfp_mask) {
        if (unlikely(gfp_mask & __GFP_NOMEMALLOC))
            return 0;
        if (gfp_mask & __GFP_MEMALLOC)
            return ALLOC_NO_WATERMARKS;
        if (in_serving_softirq() && (current->flags & PF_MEMALLOC))
            return ALLOC_NO_WATERMARKS;
        if (!in_interrupt()) {
            if (current->flags & PF_MEMALLOC)
                return ALLOC_NO_WATERMARKS;
            else if (oom_reserves_allowed(current))
                return ALLOC_OOM;
        }

        return 0; /* normal mem alloc */
    }
    if (reserve_flags)
        alloc_flags = gfp_to_alloc_flags_cma(gfp_mask, reserve_flags) |
                    (alloc_flags & ALLOC_KSWAPD) {
    #ifdef CONFIG_CMA
        if (gfp_migratetype(gfp_mask) == MIGRATE_MOVABLE)
            alloc_flags |= ALLOC_CMA;
    #endif
        return alloc_flags;
    }

    /* Reset the nodemask and zonelist iterators if memory policies can be
     * ignored. These allocations are high priority and system rather than
     * user oriented. */
    if (!(alloc_flags & ALLOC_CPUSET) || reserve_flags) {
        ac->nodemask = NULL;
        ac->preferred_zoneref = first_zones_zonelist(ac->zonelist,
                    ac->highest_zoneidx, ac->nodemask);
    }

    /* Attempt with potentially adjusted zonelist and alloc_flags */
    page = get_page_from_freelist(gfp_mask, order, alloc_flags, ac);
    if (page)
        goto got_pg;

    /* Caller is not willing to reclaim, we can't balance anything */
    if (!can_direct_reclaim)
        goto nopage;

    /* Avoid recursion of direct reclaim */
    if (current->flags & PF_MEMALLOC)
        goto nopage;

    /* Try direct reclaim and then allocating */
    page = __alloc_pages_direct_reclaim(
        gfp_mask, order, alloc_flags, ac, &did_some_progress
    );
    if (page)
        goto got_pg;

    /* Try direct compaction and then allocating */
    page = __alloc_pages_direct_compact(
        gfp_mask, order, alloc_flags, ac, compact_priority, &compact_result
    );
    if (page)
        goto got_pg;

    /* Do not loop if specifically requested */
    if (gfp_mask & __GFP_NORETRY)
        goto nopage;

    /* Do not retry costly high order allocations unless they are
     * __GFP_RETRY_MAYFAIL */
    if (costly_order && !(gfp_mask & __GFP_RETRY_MAYFAIL))
        goto nopage;

    if (should_reclaim_retry(gfp_mask, order, ac, alloc_flags,
        did_some_progress > 0, &no_progress_loops))
        goto retry;

    /* It doesn't make any sense to retry for the compaction if the order-0
     * reclaim is not able to make any progress because the current
     * implementation of the compaction depends on the sufficient amount
     * of free memory (see __compaction_suitable) */
    if (did_some_progress > 0 && should_compact_retry(ac, order, alloc_flags,
        compact_result, &compact_priority, &compaction_retries))
        goto retry;


    /* Deal with possible cpuset update races or zonelist updates to avoid
     * a unnecessary OOM kill. */
    if (check_retry_cpuset(cpuset_mems_cookie, ac) ||
        check_retry_zonelist(zonelist_iter_cookie))
        goto restart;

    /* Reclaim has failed us, start killing things */
    page = __alloc_pages_may_oom(gfp_mask, order, ac, &did_some_progress);
    if (page)
        goto got_pg;

    /* Avoid allocations with no watermarks from looping endlessly */
    if (tsk_is_oom_victim(current) &&
        (alloc_flags & ALLOC_OOM ||
        (gfp_mask & __GFP_NOMEMALLOC)))
        goto nopage;

    /* Retry as long as the OOM killer is making progress */
    if (did_some_progress) {
        no_progress_loops = 0;
        goto retry;
    }

nopage:
    /* Deal with possible cpuset update races or zonelist updates to avoid
     * a unnecessary OOM kill. */
    if (check_retry_cpuset(cpuset_mems_cookie, ac) ||
        check_retry_zonelist(zonelist_iter_cookie))
        goto restart;

    /* Make sure that __GFP_NOFAIL request doesn't leak out and make sure
    * we always retry */
    if (gfp_mask & __GFP_NOFAIL) {
        /* All existing users of the __GFP_NOFAIL are blockable, so warn
        * of any new users that actually require GFP_NOWAIT */
        if (WARN_ON_ONCE_GFP(!can_direct_reclaim, gfp_mask))
            goto fail;

        /* PF_MEMALLOC request from this context is rather bizarre
         * because we cannot reclaim anything and only can loop waiting
         * for somebody to do a work for us */
        WARN_ON_ONCE_GFP(current->flags & PF_MEMALLOC, gfp_mask);

        /* non failing costly orders are a hard requirement which we
         * are not prepared for much so let's warn about these users
         * so that we can identify them and convert them to something
         * else. */
        WARN_ON_ONCE_GFP(costly_order, gfp_mask);

        /* Help non-failing allocations by giving some access to memory
         * reserves normally used for high priority non-blocking
         * allocations but do not use ALLOC_NO_WATERMARKS because this
         * could deplete whole memory reserves which would just make
         * the situation worse. */
        page = __alloc_pages_cpuset_fallback(gfp_mask, order, ALLOC_MIN_RESERVE, ac);
        if (page)
            goto got_pg;

        cond_resched();
        goto retry;
    }
fail:

got_pg:
    return page;
}
```

### alloc_pages_direct_reclaim

:arrow_right: [__perform_reclaim](#page_reclaim) for details.

```c
struct page *
__alloc_pages_direct_reclaim(gfp_t gfp_mask, unsigned int order,
        unsigned int alloc_flags, const struct alloc_context *ac,
        unsigned long *did_some_progress)
{
    struct page *page = NULL;
    unsigned long pflags;
    bool drained = false;

    psi_memstall_enter(&pflags);
    *did_some_progress = __perform_reclaim(gfp_mask, order, ac);
    if (unlikely(!(*did_some_progress)))
        goto out;

retry:
    page = get_page_from_freelist(gfp_mask, order, alloc_flags, ac);

    /* If an allocation failed after direct reclaim, it could be because
    * pages are pinned on the per-cpu lists or in high alloc reserves.
    * Shrink them and try again */
    if (!page && !drained) {
        unreserve_highatomic_pageblock(ac, false);
        drain_all_pages(NULL);
        drained = true;
        goto retry;
    }
out:
    psi_memstall_leave(&pflags);

    return page;
}
```

### alloc_pages_direct_compact

:arrow_right: [compact_zone_order](#page_compact) for details.

```c
struct page *
__alloc_pages_direct_compact(gfp_t gfp_mask, unsigned int order,
        unsigned int alloc_flags, const struct alloc_context *ac,
        enum compact_priority prio, enum compact_result *compact_result)
{
    struct page *page = NULL;
    unsigned long pflags;
    unsigned int noreclaim_flag;

    if (!order)
        return NULL;

    *compact_result = try_to_compact_pages(gfp_mask, order, alloc_flags, ac, prio, &page) {
        int may_perform_io = (__force int)(gfp_mask & __GFP_IO);
        struct zoneref *z;
        struct zone *zone;
        enum compact_result rc = COMPACT_SKIPPED;

        if (!may_perform_io)
            return COMPACT_SKIPPED;

        /* Compact each zone in the list */
        for_each_zone_zonelist_nodemask(zone, z, ac->zonelist, ac->highest_zoneidx, ac->nodemask) {
            enum compact_result status;

            if (prio > MIN_COMPACT_PRIORITY && compaction_deferred(zone, order)) {
                rc = max_t(enum compact_result, COMPACT_DEFERRED, rc);
                continue;
            }

            status = compact_zone_order(zone, order, gfp_mask, prio, alloc_flags, ac->highest_zoneidx, capture);
            rc = max(status, rc);

            /* The allocation should succeed, stop compacting */
            if (status == COMPACT_SUCCESS) {
                /* We think the allocation will succeed in this zone,
                * but it is not certain, hence the false. The caller
                * will repeat this with true if allocation indeed
                * succeeds in this zone. */
                compaction_defer_reset(zone, order, false);

                break;
            }

            if (prio != COMPACT_PRIO_ASYNC && (status == COMPACT_COMPLETE ||
                        status == COMPACT_PARTIAL_SKIPPED))
                /* We think that allocation won't succeed in this zone
                * so we defer compaction there. If it ends up
                * succeeding after all, it will be reset. */
                defer_compaction(zone, order);

            /* We might have stopped compacting due to need_resched() in
            * async compaction, or due to a fatal signal detected. In that
            * case do not try further zones */
            if ((prio == COMPACT_PRIO_ASYNC && need_resched())
                        || fatal_signal_pending(current))
                break;
        }

        return rc;
    }

    if (*compact_result == COMPACT_SKIPPED)
        return NULL;

    if (page)
        prep_new_page(page, order, gfp_mask, alloc_flags);

    if (!page)
        page = get_page_from_freelist(gfp_mask, order, alloc_flags, ac);

    if (page) {
        struct zone *zone = page_zone(page);
        zone->compact_blockskip_flush = false;
        compaction_defer_reset(zone, order, true);
        count_vm_event(COMPACTSUCCESS);
        return page;
    }

    return NULL;
}
```

### alloc_pages_may_oom

:arrow_right: [out_of_memory](#out_of_memory) for details.

```c
struct page * __alloc_pages_may_oom(gfp_t gfp_mask, unsigned int order,
    const struct alloc_context *ac, unsigned long *did_some_progress)
{
    struct oom_control oc = {
        .zonelist = ac->zonelist,
        .nodemask = ac->nodemask,
        .memcg = NULL,
        .gfp_mask = gfp_mask,
        .order = order,
    };
    struct page *page;

    *did_some_progress = 0;

    if (!mutex_trylock(&oom_lock)) {
        *did_some_progress = 1;
        schedule_timeout_uninterruptible(1);
        return NULL;
    }

    /* Go through the zonelist yet one more time, keep very high watermark
     * here, this is only to catch a parallel oom killing, we must fail if
     * we're still under heavy pressure. But make sure that this reclaim
     * attempt shall not depend on __GFP_DIRECT_RECLAIM && !__GFP_NORETRY
     * allocation which will never fail due to oom_lock already held. */
    page = get_page_from_freelist((gfp_mask | __GFP_HARDWALL) &
                    ~__GFP_DIRECT_RECLAIM, order,
                    ALLOC_WMARK_HIGH|ALLOC_CPUSET, ac);
    if (page)
        goto out;

    /* Coredumps can quickly deplete all memory reserves */
    if (current->flags & PF_DUMPCORE)
        goto out;
    /* The OOM killer will not help higher order allocs */
    if (order > PAGE_ALLOC_COSTLY_ORDER)
        goto out;
    /* We have already exhausted all our reclaim opportunities without any
    * success so it is time to admit defeat. We will skip the OOM killer
    * because it is very likely that the caller has a more reasonable
    * fallback than shooting a random task.
    *
    * The OOM killer may not free memory on a specific node. */
    if (gfp_mask & (__GFP_RETRY_MAYFAIL | __GFP_THISNODE))
        goto out;
    /* The OOM killer does not needlessly kill tasks for lowmem */
    if (ac->highest_zoneidx < ZONE_NORMAL)
        goto out;
    if (pm_suspended_storage())
        goto out;
    /* XXX: GFP_NOFS allocations should rather fail than rely on
     * other request to make a forward progress.
     * We are in an unfortunate situation where out_of_memory cannot
     * do much for this context but let's try it to at least get
     * access to memory reserved if the current task is killed (see
     * out_of_memory). Once filesystems are ready to handle allocation
     * failures more gracefully we should just bail out here. */

    /* Exhausted what can be done so it's blame time */
    if (out_of_memory(&oc) ||
        WARN_ON_ONCE_GFP(gfp_mask & __GFP_NOFAIL, gfp_mask)) {
        *did_some_progress = 1;

        /* Help non-failing allocations by giving them access to memory
        * reserves */
        if (gfp_mask & __GFP_NOFAIL)
            page = __alloc_pages_cpuset_fallback(gfp_mask, order,
                    ALLOC_NO_WATERMARKS, ac);
    }
out:
    mutex_unlock(&oom_lock);
    return page;
}
```

# free_pages

![](../images/kernel/mem-free_pages.svg)

```c
void free_pages(unsigned long addr, unsigned int order)
{
    if (addr != 0) {
        VM_BUG_ON(!virt_addr_valid((void *)addr));
        __free_pages(virt_to_page((void *)addr), order) {
            /* get PageHead before we drop reference */
            int head = PageHead(page);
            struct alloc_tag *tag = pgalloc_tag_get(page);

            if (put_page_testzero(page)) {
                free_frozen_pages(page, order);
            } else if (!head) {
                pgalloc_tag_sub_pages(tag, (1 << order) - 1);
                while (order-- > 0) {
                    free_frozen_pages(page + (1 << order), order);
                }
            }
        }
    }
}
```

## free_frozen_pages

```c
void free_frozen_pages(struct page *page, unsigned int order)
{
    unsigned long __maybe_unused UP_flags;
    struct per_cpu_pages *pcp;
    struct zone *zone;
    unsigned long pfn = page_to_pfn(page);
    int migratetype, pcpmigratetype;

    if (!pcp_allowed_order(order)) {
        __free_pages_ok(page, order, FPI_NONE);
        return;
    }

    if (!free_pages_prepare(page, order))
        return;

    zone = page_zone(page);
    migratetype = get_pfnblock_migratetype(page, pfn);
    if (unlikely(migratetype >= MIGRATE_PCPTYPES)) {
        if (unlikely(is_migrate_isolate(migratetype))) {
            free_one_page(page_zone(page), page, pfn, order, migratetype, FPI_NONE);
            return;
        }
        pcpmigratetype = MIGRATE_MOVABLE;
    }

    if (unlikely((fpi_flags & FPI_TRYLOCK) && IS_ENABLED(CONFIG_PREEMPT_RT) && (in_nmi() || in_hardirq()))) {
        add_page_to_zone_llist(zone, page, order);
        return;
    }

    pcp_trylock_prepare(UP_flags);
    pcp = pcp_spin_trylock(zone->per_cpu_pageset);
    if (pcp) {
        free_frozen_page_commit(zone, pcp, page, pcpmigratetype, order) {
            int high, batch;
            int pindex;
            bool free_high = false;

            pcp->alloc_factor >>= 1;
            __count_vm_events(PGFREE, 1 << order);
            pindex = order_to_pindex(migratetype, order);
            list_add(&page->pcp_list, &pcp->lists[pindex]);
            pcp->count += 1 << order;

            batch = READ_ONCE(pcp->batch);
            if (order && order <= PAGE_ALLOC_COSTLY_ORDER) {
                free_high = (pcp->free_count >= batch
                    && (pcp->flags & PCPF_PREV_FREE_HIGH_ORDER)
                    && (!(pcp->flags & PCPF_FREE_HIGH_BATCH) || pcp->count >= READ_ONCE(batch)));
                pcp->flags |= PCPF_PREV_FREE_HIGH_ORDER;
            } else if (pcp->flags & PCPF_PREV_FREE_HIGH_ORDER) {
                pcp->flags &= ~PCPF_PREV_FREE_HIGH_ORDER;
            }

            if (pcp->free_count < (batch << CONFIG_PCP_BATCH_SCALE_MAX))
                pcp->free_count += (1 << order);

            high = nr_pcp_high(pcp, zone, batch, free_high);
            /* pcp overflow: free pcp pages to buddy */
            if (pcp->count >= high) {
                free_pcppages_bulk(zone, nr_pcp_free(pcp, batch, high, free_high), pcp, pindex) {
                    count = min(pcp->count, count);

                    /* Ensure requested pindex is drained first. */
                    pindex = pindex - 1;

                    spin_lock_irqsave(&zone->lock, flags);
                    isolated_pageblocks = has_isolate_pageblock(zone);

                    while (count > 0) {
                        struct list_head *list;
                        int nr_pages;

                        /* Remove pages from lists in a round-robin fashion. */
                        do {
                            if (++pindex > NR_PCP_LISTS - 1)
                                pindex = 0;
                            list = &pcp->lists[pindex];
                        } while (list_empty(list));

                        order = pindex_to_order(pindex);
                        nr_pages = 1 << order;
                        do {
                            int mt;

                            page = list_last_entry(list, struct page, pcp_list);
                            mt = get_pcppage_migratetype(page);

                            /* must delete to avoid corrupting pcp list */
                            list_del(&page->pcp_list);
                            count -= nr_pages;
                            pcp->count -= nr_pages;

                            /* MIGRATE_ISOLATE page should not go to pcplists */
                            VM_BUG_ON_PAGE(is_migrate_isolate(mt), page);
                            /* Pageblock could have been isolated meanwhile */
                            if (unlikely(isolated_pageblocks))
                                mt = get_pageblock_migratetype(page);

                            __free_one_page(page, page_to_pfn(page), zone, order, mt, FPI_NONE);
                        } while (count > 0 && !list_empty(list));
                    }
                }
                if (test_bit(ZONE_BELOW_HIGH, &zone->flags) &&
                    zone_watermark_ok(zone, 0, high_wmark_pages(zone), ZONE_MOVABLE, 0))
                    clear_bit(ZONE_BELOW_HIGH, &zone->flags);
            }
        }
        pcp_spin_unlock(pcp);
    } else {
        free_one_page(zone, page, pfn, order, migratetype, FPI_NONE);
    }
    pcp_trylock_finish(UP_flags);
}
```

## free_one_page

```c
void __free_pages_ok(struct page *page, unsigned int order, fpi_t fpi_flags)
{
    unsigned long pfn = page_to_pfn(page);
    struct zone *zone = page_zone(page);

    if (free_pages_prepare(page, order))
        free_one_page(zone, page, pfn, order, fpi_flags);
}
```

```c
bool free_pages_prepare(struct page *page,
            unsigned int order)
{
    int bad = 0;
    bool skip_kasan_poison = should_skip_kasan_poison(page);
    bool init = want_init_on_free();
    bool compound = PageCompound(page);
    struct folio *folio = page_folio(page);

    VM_BUG_ON_PAGE(PageTail(page), page);

    trace_mm_page_free(page, order);
    kmsan_free_page(page, order);

    if (memcg_kmem_online() && PageMemcgKmem(page)) {
        __memcg_kmem_uncharge_page(page, order) {
            struct obj_cgroup *objcg = page_objcg(page);
            unsigned int nr_pages = 1 << order;

            if (!objcg)
                return;

            obj_cgroup_uncharge_pages(objcg, nr_pages);
            page->memcg_data = 0;
            obj_cgroup_put(objcg);
        }
    }

    /* In rare cases, when truncation or holepunching raced with
     * munlock after VM_LOCKED was cleared, Mlocked may still be
     * found set here.  This does not indicate a problem, unless
     * "unevictable_pgs_cleared" appears worryingly large. */
    if (unlikely(folio_test_mlocked(folio))) {
        long nr_pages = folio_nr_pages(folio);

        __folio_clear_mlocked(folio);
        zone_stat_mod_folio(folio, NR_MLOCK, -nr_pages);
        count_vm_events(UNEVICTABLE_PGCLEARED, nr_pages);
    }

    if (unlikely(PageHWPoison(page)) && !order) {
        /* Do not let hwpoison pages hit pcplists/buddy */
        reset_page_owner(page, order);
        page_table_check_free(page, order);
        pgalloc_tag_sub(page, 1 << order);

        /* The page is isolated and accounted for.
         * Mark the codetag as empty to avoid accounting error
         * when the page is freed by unpoison_memory(). */
        clear_page_tag_ref(page);
        return false;
    }

    VM_BUG_ON_PAGE(compound && compound_order(page) != order, page);

    /* Check tail pages before head page information is cleared to
     * avoid checking PageCompound for order-0 pages. */
    if (unlikely(order)) {
        int i;

        if (compound) {
            page[1].flags.f &= ~PAGE_FLAGS_SECOND;
#ifdef NR_PAGES_IN_LARGE_FOLIO
            folio->_nr_pages = 0;
#endif
        }
        for (i = 1; i < (1 << order); i++) {
            if (compound)
                bad += free_tail_page_prepare(page, page + i);
            if (is_check_pages_enabled()) {
                if (free_page_is_bad(page + i)) {
                    bad++;
                    continue;
                }
            }
            (page + i)->flags.f &= ~PAGE_FLAGS_CHECK_AT_PREP;
        }
    }
    if (folio_test_anon(folio)) {
        mod_mthp_stat(order, MTHP_STAT_NR_ANON, -1);
        folio->mapping = NULL;
    }
    if (unlikely(page_has_type(page)))
        /* Reset the page_type (which overlays _mapcount) */
        page->page_type = UINT_MAX;

    if (is_check_pages_enabled()) {
        if (free_page_is_bad(page))
            bad++;
        if (bad)
            return false;
    }

    page_cpupid_reset_last(page);
    page->flags.f &= ~PAGE_FLAGS_CHECK_AT_PREP;
    reset_page_owner(page, order);
    page_table_check_free(page, order);
    pgalloc_tag_sub(page, 1 << order);

    if (!PageHighMem(page)) {
        debug_check_no_locks_freed(page_address(page),
                       PAGE_SIZE << order);
        debug_check_no_obj_freed(page_address(page),
                       PAGE_SIZE << order);
    }

    kernel_poison_pages(page, 1 << order);

    /* As memory initialization might be integrated into KASAN,
     * KASAN poisoning and memory initialization code must be
     * kept together to avoid discrepancies in behavior.
     *
     * With hardware tag-based KASAN, memory tags must be set before the
     * page becomes unavailable via debug_pagealloc or arch_free_page. */
    if (!skip_kasan_poison) {
        kasan_poison_pages(page, order, init);

        /* Memory is already initialized if KASAN did it internally. */
        if (kasan_has_integrated_init())
            init = false;
    }
    if (init)
        kernel_init_pages(page, 1 << order);

    /* arch_free_page() can make the page's contents inaccessible.  s390
     * does this.  So nothing which can access the page's contents should
     * happen after this. */
    arch_free_page(page, order);

    debug_pagealloc_unmap_pages(page, 1 << order);

    return true;
}
```

```c
free_one_page(zone, page, pfn, order, migratetype, FPI_NONE) {
    unsigned long flags;

    struct llist_head *llhead;
    unsigned long flags;

    if (unlikely(fpi_flags & FPI_TRYLOCK)) {
        if (!spin_trylock_irqsave(&zone->lock, flags)) {
            add_page_to_zone_llist(zone, page, order) {
                /* Remember the order */
                page->private = order;
                /* Add the page to the free list */
                llist_add(&page->pcp_llist, &zone->trylock_free_pages);
            }
            return;
        }
    } else {
        spin_lock_irqsave(&zone->lock, flags);
    }

    /* The lock succeeded. Process deferred pages. */
    llhead = &zone->trylock_free_pages;
    if (unlikely(!llist_empty(llhead) && !(fpi_flags & FPI_TRYLOCK))) {
        struct llist_node *llnode;
        struct page *p, *tmp;

        llnode = llist_del_all(llhead);
        llist_for_each_entry_safe(p, tmp, llnode, pcp_llist) {
            unsigned int p_order = p->order;

            split_large_buddy(zone, p, page_to_pfn(p), p_order, fpi_flags);
            __count_vm_events(PGFREE, 1 << p_order);
        }
    }

    split_large_buddy(zone, page, pfn, order, fpi_flags) {
        unsigned long end = pfn + (1 << order);

        VM_WARN_ON_ONCE(!IS_ALIGNED(pfn, 1 << order));
        /* Caller removed page from freelist, buddy info cleared! */
        VM_WARN_ON_ONCE(PageBuddy(page));

        if (order > pageblock_order)
            order = pageblock_order;

        do {
            int mt = get_pfnblock_migratetype(page, pfn);

            __free_one_page(page, pfn, zone, order, mt, fpi) {
                --->
            }
            pfn += 1 << order;
            if (pfn == end)
                break;
            page = pfn_to_page(pfn);
        } while (1);
    }
    spin_unlock_irqrestore(&zone->lock, flags);

    __count_vm_events(PGFREE, 1 << order);
}
```

```c
__free_one_page(page, pfn, zone, order, migratetype, fpi_flags) {
    struct capture_control *capc = task_capc(zone);

    while (order < MAX_PAGE_ORDER) {
        int buddy_mt = migratetype;

        if (compaction_capture(capc, page, order, migratetype)) {
            __mod_zone_freepage_state(zone, -(1 << order), migratetype);
            return;
        }

        buddy = find_buddy_page_pfn(page, pfn, order, &buddy_pfn) {
            unsigned long __buddy_pfn = __find_buddy_pfn(pfn, order) {
                return page_pfn ^ (1 << order);
            }
            struct page *buddy;

            buddy = page + (__buddy_pfn - pfn);
            if (buddy_pfn) {
                *buddy_pfn = __buddy_pfn;
            }

            ret = page_is_buddy(page, buddy, order){
                if (!page_is_guard(buddy) && !PageBuddy(buddy))
                    return false;
                if (buddy_order(buddy) != order)
                    return false;
                if (page_zone_id(page) != page_zone_id(buddy))
                    return false;

                VM_BUG_ON_PAGE(page_count(buddy) != 0, buddy);

                return true;
            }
            if (ret) {
                return buddy;
            }
            return NULL;
        }
        if (!buddy)
            goto done_merging;

        if (unlikely(order >= pageblock_order)) {
            int buddy_mt = get_pfnblock_migratetype(buddy, buddy_pfn);

            if (migratetype != buddy_mt
                    && (!migratetype_is_mergeable(migratetype) ||
                        !migratetype_is_mergeable(buddy_mt)))
                goto done_merging;
        }

        if (page_is_guard(buddy))
            clear_page_guard(zone, buddy, order, migratetype);
        else
            del_page_from_free_list(buddy, zone, order);

        if (unlikely(buddy_mt != migratetype)) {
            set_pageblock_migratetype(buddy, migratetype);
        }

        combined_pfn = buddy_pfn & pfn;
        page = page + (combined_pfn - pfn);
        pfn = combined_pfn;
        order++;
    }

done_merging:
    set_buddy_order(page, order);

    if (fpi_flags & FPI_TO_TAIL)
        to_tail = true;
    else if (is_shuffle_order(order))
        to_tail = shuffle_pick_tail();
    else
        to_tail = buddy_merge_likely(pfn, buddy_pfn, page, order);

    if (to_tail)
        add_to_free_list_tail(page, zone, order, migratetype);
    else
        add_to_free_list(page, zone, order, migratetype);

    /* Notify page reporting subsystem of freed page */
    if (!(fpi_flags & FPI_SKIP_REPORT_NOTIFY)) {
        page_reporting_notify_free(order);
    }
}
```

# kmem_cache

![](../images/kernel/mem-kmem-cache-cpu-node.png)

```c
/* mm/slab_common.c */
enum slab_state slab_state;
LIST_HEAD(slab_caches);
DEFINE_MUTEX(slab_mutex);
struct kmem_cache *kmem_cache;

/* mm/slab.c */
struct kmem_cache kmem_cache_boot = {
    .name   = "kmem_cache",
    .size   = sizeof(struct kmem_cache),
    .flags  = SLAB_PANIC,
    .aligs  = ARCH_KMALLOC_MINALIGN,
};

struct kmem_cache {
#ifndef CONFIG_SLUB_TINY
    struct kmem_cache_cpu __percpu *cpu_slab;
#endif

    /* Used for retrieving partial slabs, etc. */
    slab_flags_t flags;
    unsigned long min_partial;
    unsigned int size;        /* Object size including metadata */
    unsigned int object_size;    /* Object size without metadata */
    struct reciprocal_value reciprocal_size;
    unsigned int offset;        /* Free pointer offset */

#ifdef CONFIG_SLUB_CPU_PARTIAL
    /* Number of per cpu partial objects to keep around */
    unsigned int cpu_partial;
    /* Number of per cpu partial slabs to keep around */
    unsigned int cpu_partial_slabs;
#endif

    /* high 16 bits store nr of page used by cache
     * low 16 bits store nr of obj in cache */
    struct kmem_cache_order_objects oo;
    struct kmem_cache_order_objects min;

    gfp_t allocflags;        /* gfp flags to use on each alloc */
    int refcount;            /* Refcount for slab cache destroy */
    void (*ctor)(void *object);    /* Object constructor */
    unsigned int inuse;        /* Offset to metadata */
    unsigned int align;        /* Alignment */
    unsigned int red_left_pad;    /* Left redzone padding size */
    const char *name;        /* Name (only for display!) */
    struct list_head list;        /* List of slab caches */

    struct kmem_cache_node *node[MAX_NUMNODES];
};

struct kmem_cache_node {
    spinlock_t          list_lock;
    unsigned long       nr_partial;
    struct list_head    partial;
};

struct kmem_cache_cpu {
    union {
        struct {
            void **freelist;    /* Pointer to next available object */
            unsigned long tid;    /* Globally unique transaction id */
        };
        freelist_aba_t freelist_tid;
    };
    struct slab *slab;    /* The slab from which we are allocating */

#ifdef CONFIG_SLUB_CPU_PARTIAL
    struct slab *partial;    /* Partially allocated frozen slabs */
#endif
    local_lock_t lock;    /* Protects the fields above */
};
```

## kmem_cache_create

```c
struct kmem_cache *
kmem_cache_create(const char *name, unsigned int size, unsigned int align,
        slab_flags_t flags, void (*ctor)(void *))
{
    return kmem_cache_create_usercopy(name, size, align, flags, 0, 0, ctor);
}

struct kmem_cache *
kmem_cache_create_usercopy(const char *name,
    unsigned int size, unsigned int align,
    slab_flags_t flags,
    unsigned int useroffset, unsigned int usersize,
    void (*ctor)(void *))
{
    struct kmem_cache *s = NULL;
    const char *cache_name;
    int err;

    mutex_lock(&slab_mutex);

    flags &= CACHE_CREATE_MASK;

    if (!usersize)
        s = __kmem_cache_alias(name, size, align, flags, ctor);

    cache_name = kstrdup_const(name, GFP_KERNEL);

    s = create_cache(cache_name, size,
        calculate_alignment(flags, align, size),
        flags, useroffset, usersize, ctor, NULL) {

        s = kmem_cache_zalloc(kmem_cache, GFP_KERNEL);

        s->name = name;
        s->size = s->object_size = object_size;
        s->align = align;
        s->ctor = ctor;
        s->useroffset = useroffset;
        s->usersize = usersize;

        __kmem_cache_create(s, flags) {
            err = kmem_cache_open(s, flags) {
                s->flags = kmem_cache_flags(s->size, flags, s->name);
                s->random = get_random_long();

                if (!calculate_sizes(s)) {
                    goto error;
                }

                s->min_partial = min_t(unsigned long, MAX_PARTIAL, ilog2(s->size) / 2);
                s->min_partial = max_t(unsigned long, MIN_PARTIAL, s->min_partial);

                set_cpu_partial(s) {
                #ifdef CONFIG_SLUB_CPU_PARTIAL
                    unsigned int nr_objects;
                    if (!kmem_cache_has_cpu_partial(s))
                        nr_objects = 0;
                    else if (s->size >= PAGE_SIZE)
                        nr_objects = 6;
                    else if (s->size >= 1024)
                        nr_objects = 24;
                    else if (s->size >= 256)
                        nr_objects = 52;
                    else
                        nr_objects = 120;

                    slub_set_cpu_partial(s, nr_objects) {
                        s->cpu_partial = nr_objects;
                        nr_slabs = DIV_ROUND_UP(nr_objects * 2, oo_objects(s->oo));
                        s->cpu_partial_slabs = nr_slabs;
                    }
                #endif
                }

                init_cache_random_seq(s);

                init_kmem_cache_nodes(s) {
                    int node;

                    for_each_node_mask(node, slab_nodes) {
                        struct kmem_cache_node *n;
                        struct node_barn *barn = NULL;

                        if (slab_state == DOWN) {
                            early_kmem_cache_node_alloc(node);
                            continue;
                        }

                        if (s->cpu_sheaves) {
                            barn = kmalloc_node(sizeof(*barn), GFP_KERNEL, node);

                            if (!barn)
                                return 0;
                        }

                        n = kmem_cache_alloc_node(kmem_cache_node, GFP_KERNEL, node);
                        if (!n) {
                            kfree(barn);
                            return 0;
                        }

                        init_kmem_cache_node(n, barn);

                        s->node[node] = n;
                    }
                    return 1;
                }

                alloc_kmem_cache_cpus(s) {
                    s->cpu_slab = __alloc_percpu(sizeof(struct kmem_cache_cpu), 2 * sizeof(void *));
                    init_kmem_cache_cpus(s) {
                        for_each_possible_cpu(cpu) {
                            c = per_cpu_ptr(s->cpu_slab, cpu);
                            local_lock_init(&c->lock);
                            c->tid = init_tid(cpu);
                        }
                    }
                }
            }

            err = sysfs_slab_add(s);

            if (s->flags & SLAB_STORE_USER)
                debugfs_slab_add(s);
        }

        s->refcount = 1;
        list_add(&s->list, &slab_caches);
        return s;
    }
    if (IS_ERR(s)) {
        err = PTR_ERR(s);
        kfree_const(cache_name);
    }

out_unlock:
    mutex_unlock(&slab_mutex);

    if (err) {
        return NULL;
    }
    return s;
}

struct kmem_cache *
__kmem_cache_alias(const char *name, unsigned int size, unsigned int align,
           slab_flags_t flags, void (*ctor)(void *))
{
    struct kmem_cache *s;

    s = find_mergeable(size, align, flags, name, ctor);
    if (s) {
        if (sysfs_slab_alias(s, name))
            pr_err("SLUB: Unable to add cache alias %s to sysfs\n",
                   name);

        s->refcount++;

        /* Adjust the object sizes so that we clear
         * the complete object on kzalloc. */
        s->object_size = max(s->object_size, size);
        s->inuse = max(s->inuse, ALIGN(size, sizeof(void *)));
    }

    return s;
}
```

### calculate_sizes

![](../images/kernel/mem-slub-layout.png)

```c
int calculate_sizes(struct kmem_cache *s) {
    slab_flags_t flags = s->flags;
    unsigned int size = s->object_size;
    unsigned int order;

/* 1. | object_size + ALIGN | */
    size = ALIGN(size, sizeof(void *));

#ifdef CONFIG_SLUB_DEBUG
    if ((flags & SLAB_POISON) && !(flags & SLAB_TYPESAFE_BY_RCU) && !s->ctor)
        s->flags |= __OBJECT_POISON;
    else
        s->flags &= ~__OBJECT_POISON;

/* 2. | object_size + ALIGN | RHT_RED_ZONE | */
    if ((flags & SLAB_RED_ZONE) && size == s->object_size)
        size += sizeof(void *);
#endif

    s->inuse = size;

    if (slub_debug_orig_size(s) ||
        (flags & (SLAB_TYPESAFE_BY_RCU | SLAB_POISON)) ||
        ((flags & SLAB_RED_ZONE) && s->object_size < sizeof(void *)) || s->ctor) {

/* 3.1 | object_size + ALIGN | RHT_RED_ZONE | free_ptr | */
        s->offset = size;
        size += sizeof(void *);
    } else {
/* 3.2 | object_size + ALIGN | RED_ZONE | */
        s->offset = ALIGN_DOWN(s->object_size / 2, sizeof(void *));
    }

#ifdef CONFIG_SLUB_DEBUG
    if (flags & SLAB_STORE_USER) {
/* 4. | object_size + ALIGN | RHT_RED_ZONE | free_ptr | track | track | */
        size += 2 * sizeof(struct track);

        /* Save the original kmalloc request size */
        if (flags & SLAB_KMALLOC) {
            size += sizeof(unsigned int);
        }
    }
#endif

    kasan_cache_create(s, &size, &s->flags);
#ifdef CONFIG_SLUB_DEBUG
    if (flags & SLAB_RED_ZONE) {
/* 5. | LFT_RED_ZONE + ALIGN | object_size + ALIGN | RHT_RED_ZONE | free_ptr | */
        size += sizeof(void *);

        s->red_left_pad = sizeof(void *);
        s->red_left_pad = ALIGN(s->red_left_pad, s->align);
        size += s->red_left_pad;
    }
#endif

/* 6. | LFT_RED_ZONE + ALIGN | object_size + ALIGN | RHT_RED_ZONE | free_ptr | ALIGN | */
    size = ALIGN(size, s->align);
    s->size = size;
    s->reciprocal_size = reciprocal_value(size);

    order = calculate_order(size) {
        unsigned int order;
        unsigned int min_objects;
        unsigned int max_objects;
        unsigned int min_order;

        min_objects = slub_min_objects;
        if (!min_objects) {
            unsigned int nr_cpus = num_present_cpus();
            if (nr_cpus <= 1)
                nr_cpus = nr_cpu_ids;
            min_objects = 4 * (fls(nr_cpus) + 1);
        }
        /* min_objects can't be 0 because get_order(0) is undefined */
        max_objects = max(order_objects(slub_max_order, size), 1U);
        min_objects = min(min_objects, max_objects);

        min_order = max_t(unsigned int, slub_min_order, get_order(min_objects * size));
        ret = order_objects(min_order, size) {
            return ((unsigned int)PAGE_SIZE << order) / size;
        }
        if (ret > MAX_OBJS_PER_PAGE) {
            return get_order(size * MAX_OBJS_PER_PAGE) - 1;
        }

        for (unsigned int fraction = 16; fraction > 1; fraction /= 2) {
            order = calc_slab_order(size, min_order, slub_max_order, fraction) {
                unsigned int order;

                for (order = min_order; order <= max_order; order++) {
                    unsigned int slab_size = (unsigned int)PAGE_SIZE << order;
                    unsigned int rem;

                    rem = slab_size % size;
                    if (rem <= slab_size / fract_leftover)
                        break;
                }

                return order;
            }

            if (order <= slub_max_order)
                return order;
        }

        order = get_order(size);
        if (order <= MAX_PAGE_ORDER)
            return order;
        return -ENOSYS;
    }

    if ((int)order < 0)
        return 0;

    s->allocflags = 0;
    if (order)
        s->allocflags |= __GFP_COMP;

    if (s->flags & SLAB_CACHE_DMA)
        s->allocflags |= GFP_DMA;

    if (s->flags & SLAB_CACHE_DMA32)
        s->allocflags |= GFP_DMA32;

    if (s->flags & SLAB_RECLAIM_ACCOUNT)
        s->allocflags |= __GFP_RECLAIMABLE;

    s->oo = oo_make(order, size);
    s->min = oo_make(get_order(size), size) {
        struct kmem_cache_order_objects x = {
            (order << OO_SHIFT) + order_objects(order, size)
        };
        return x;
    }

    return !!oo_objects(s->oo) {
        return x.x & OO_MASK;
    }
}
```

## kmem_cache_destroy

![](../images/kernel/mem-kmem_cache_destroy.png)

```c
void kmem_cache_destroy(struct kmem_cache *s)
{
    int err = -EBUSY;
    bool rcu_set;

    if (unlikely(!s) || !kasan_check_byte(s))
        return;

    cpus_read_lock();
    mutex_lock(&slab_mutex);

    rcu_set = s->flags & SLAB_TYPESAFE_BY_RCU;

    s->refcount--;
    if (s->refcount)
        goto out_unlock;

    err = shutdown_cache(s) {
        /* free asan quarantined objects */
        kasan_cache_shutdown(s);

        ret = __kmem_cache_shutdown(s) {
            int node;
            struct kmem_cache_node *n;

            flush_all_cpus_locked(s);
            /* Attempt to free all objects */
            for_each_kmem_cache_node(s, node, n) {
                free_partial(s, n) {
                    LIST_HEAD(discard);
                    struct slab *slab, *h;

                    list_for_each_entry_safe(slab, h, &n->partial, slab_list) {
                        if (!slab->inuse) {
                            remove_partial(n, slab);
                            list_add(&slab->slab_list, &discard);
                        } else {
                            list_slab_objects(s, slab);
                        }
                    }
                    spin_unlock_irq(&n->list_lock);

                    list_for_each_entry_safe(slab, h, &discard, slab_list) {
                        discard_slab(s, slab);
                    }
                }
                if (n->nr_partial || node_nr_slabs(n))
                    return 1;
            }
            return 0;
        }
        if (ret != 0) {
            return -EBUSY;
        }

        list_del(&s->list);

        if (s->flags & SLAB_TYPESAFE_BY_RCU) {
            list_add_tail(&s->list, &slab_caches_to_rcu_destroy);
            schedule_work(&slab_caches_to_rcu_destroy_work) {
                list_for_each_entry_safe(s, s2, &to_destroy, list) {
                    debugfs_slab_release(s);
                    kfence_shutdown_cache(s);
                    kmem_cache_release(s) {
                        slab_kmem_cache_release(s) {
                            __kmem_cache_release(s) {
                                cache_random_seq_destroy(s) {
                                    free_percpu(s->cpu_slab);

                                    free_kmem_cache_nodes(s) {
                                        for_each_kmem_cache_node(s, node, n) {
                                            s->node[node] = NULL;
                                            kmem_cache_free(kmem_cache_node, n);
                                        }
                                    }
                                }
                            }

                            kfree_const(s->name);

                            kmem_cache_free(kmem_cache, s) {
                                s = cache_from_obj(s, x);
                                if (!s)
                                    return;
                                slab_free(s, virt_to_slab(x), x, _RET_IP_);
                            }
                        }
                    }
                }
            }
        } else {
            kfence_shutdown_cache(s);
            debugfs_slab_release(s);
        }
    }

out_unlock:
    mutex_unlock(&slab_mutex);
    cpus_read_unlock();
    if (!err && !rcu_set) {
        kmem_cache_release(s);
    }
}
```


```c
void kmem_cache_init(void)
{
    static struct kmem_cache boot_kmem_cache, boot_kmem_cache_node;

    kmem_cache_node = &boot_kmem_cache_node;
    kmem_cache = &boot_kmem_cache;

    create_boot_cache(kmem_cache_node, "kmem_cache_node",
        sizeof(struct kmem_cache_node), SLAB_HWCACHE_ALIGN, 0, 0);

    register_hotmemory_notifier(&slab_memory_callback_nb);

    /* Able to allocate the per node structures */
    slab_state = PARTIAL;

    create_boot_cache(kmem_cache, "kmem_cache",
        offsetof(struct kmem_cache, node) +
            nr_node_ids * sizeof(struct kmem_cache_node *),
        SLAB_HWCACHE_ALIGN, 0, 0
    );

    kmem_cache = bootstrap(&boot_kmem_cache);
    kmem_cache_node = bootstrap(&boot_kmem_cache_node);

    /* Now we can use the kmem_cache to allocate kmalloc slabs */
    setup_kmalloc_cache_index_table();
    create_kmalloc_caches(0);

    /* Setup random freelists for each cache */
    init_freelist_randomization();

    cpuhp_setup_state_nocalls(CPUHP_SLUB_DEAD, "slub:dead", NULL,
          slub_cpu_dead);
}
```

## kmem_cache_free_bulk

```c
void kmem_cache_free_bulk(struct kmem_cache *s, size_t size, void **p)
{
    if (!size)
        return;

    /* freeing to sheaves is so incompatible with the detached freelist so
     * once we go that way, we have to do everything differently */
    if (s && s->cpu_sheaves) {
        free_to_pcs_bulk(s, size, p) {
            struct slub_percpu_sheaves *pcs;
            struct slab_sheaf *main, *empty;
            bool init = slab_want_init_on_free(s);
            unsigned int batch, i = 0;
            struct node_barn *barn;
            void *remote_objects[PCS_BATCH_MAX];
            unsigned int remote_nr = 0;
            int node = numa_mem_id();

        next_remote_batch:
            while (i < size) {
                struct slab *slab = virt_to_slab(p[i]);

                memcg_slab_free_hook(s, slab, p + i, 1);
                alloc_tagging_slab_free_hook(s, slab, p + i, 1);

                if (unlikely(!slab_free_hook(s, p[i], init, false))) {
                    p[i] = p[--size];
                    continue;
                }

                if (unlikely((IS_ENABLED(CONFIG_NUMA) && slab_nid(slab) != node)
                        || slab_test_pfmemalloc(slab))) {
                    remote_objects[remote_nr] = p[i];
                    p[i] = p[--size];
                    if (++remote_nr >= PCS_BATCH_MAX)
                        goto flush_remote;
                    continue;
                }

                i++;
            }

            if (!size)
                goto flush_remote;

        next_batch:
            if (!local_trylock(&s->cpu_sheaves->lock))
                goto fallback;

            pcs = this_cpu_ptr(s->cpu_sheaves);

            if (likely(pcs->main->size < s->sheaf_capacity))
                goto do_free;

            barn = get_barn(s);
            if (!barn)
                goto no_empty;

            if (!pcs->spare) {
                empty = barn_get_empty_sheaf(barn);
                if (!empty)
                    goto no_empty;

                pcs->spare = pcs->main;
                pcs->main = empty;
                goto do_free;
            }

            if (pcs->spare->size < s->sheaf_capacity) {
                swap(pcs->main, pcs->spare);
                goto do_free;
            }

            empty = barn_replace_full_sheaf(barn, pcs->main);
            if (IS_ERR(empty)) {
                stat(s, BARN_PUT_FAIL);
                goto no_empty;
            }

            stat(s, BARN_PUT);
            pcs->main = empty;

        do_free:
            main = pcs->main;
            batch = min(size, s->sheaf_capacity - main->size);

            memcpy(main->objects + main->size, p, batch * sizeof(void *));
            main->size += batch;

            local_unlock(&s->cpu_sheaves->lock);

            stat_add(s, FREE_PCS, batch);

            if (batch < size) {
                p += batch;
                size -= batch;
                goto next_batch;
            }

            if (remote_nr)
                goto flush_remote;

            return;

        no_empty:
            local_unlock(&s->cpu_sheaves->lock);

            /* if we depleted all empty sheaves in the barn or there are too
            * many full sheaves, free the rest to slab pages */
        fallback:
            __kmem_cache_free_bulk(s, size, p);

        flush_remote:
            if (remote_nr) {
                __kmem_cache_free_bulk(s, remote_nr, &remote_objects[0]);
                if (i < size) {
                    remote_nr = 0;
                    goto next_remote_batch;
                }
            }
        }
        return;
    }

    do {
        struct detached_freelist df;

        size = build_detached_freelist(s, size, p, &df);
        if (!df.slab)
            continue;

        slab_free_bulk(df.s, df.slab, df.freelist, df.tail, &p[size], df.cnt, _RET_IP_);
    } while (likely(size));
}
```

# slub

* [git.slab](https://git.kernel.org/pub/scm/linux/kernel/git/vbabka/slab.git)
* [Kenel Index Slab - LWN](https://lwn.net/Kernel/Index/#Memory_management-Slab_allocators)
    * [The SLUB allocator ](https://lwn.net/Articles/229984/)
* [Oracle Linux SLUB Allocator Internals and Debugging :one: :link:](https://blogs.oracle.com/linux/post/linux-slub-allocator-internals-and-debugging-1) ⊙ [:two: :link:](https://blogs.oracle.com/linux/post/linux-slub-allocator-internals-and-debugging-2) ⊙ [:three: - KASan :link:](https://blogs.oracle.com/linux/post/linux-slub-allocator-internals-and-debugging-3) ⊙ [:four: - KFENCE :link:](https://blogs.oracle.com/linux/post/linux-slub-allocator-internals-and-debugging-4)
* [[PATCH v8 00/23] SLUB percpu sheaves](https://lore.kernel.org/all/20250910-slub-percpu-caches-v8-14-ca3099d8352c@suse.cz/)
    * [benchmarks](https://lore.kernel.org/lkml/20250913000935.1021068-1-sudarsanm@google.com)
    * [LWN - Slabs, sheaves, and barns](https://lwn.net/Articles/1010667/) ⊙ [Slab allocator: sheaves and any-context allocations](https://lwn.net/Articles/1016001/)

* bin 的技术小屋
    * [80 张图带你一步一步推演 slab 内存池的设计与实现](https://mp.weixin.qq.com/s/yHF5xBm5yMXDAHmE_noeCg)
    * [slab 内存池的创建初始化流程](https://mp.weixin.qq.com/s/CcPUAeHY0i2XAVerAWCmLA)
    * [slab 内存分配全链路实现](https://mp.weixin.qq.com/s/bNAQmzeBLx2HObSNySmB-Q)
    * [slab 内存池回收内存以及销毁全流程](https://mp.weixin.qq.com/s/dHLqT6KtAPZWzq_SmQZVFA)

![](../images/kernel/mem-slub-structure.png)

![](../images/kernel/mem-slub-layout.png)

![](../images/kernel/mem-mng.svg)


```c
/* Reuses the bits in struct page */
struct slab {
    unsigned long __page_flags;

    struct kmem_cache *slab_cache;
    union {
        struct {
            union {
                struct list_head slab_list;
#ifdef CONFIG_SLUB_CPU_PARTIAL
                struct {
                    struct slab *next;
                     /* Nr of total free slabs, the first partial page
                      * represents the total nr of cache. */
                    int slabs; /* Nr of slabs left */
                };
#endif
            };
            /* Double-word boundary */
            union {
                struct {
                    void *freelist; /* first free object */
                    union {
                        unsigned long counters;
                        struct {
                           unsigned inuse:16;       /* used obj */
                            unsigned objects:15;    /* nr obj */
                            unsigned frozen:1;      /* on cpu cache */
                        };
                    };
                };
#ifdef system_has_freelist_aba
                freelist_aba_t freelist_counter {
                    struct {
                        void *freelist;
                        unsigned long counter;
                    };
                    freelist_full_t full;
                }
#endif
            };
        };
        struct rcu_head rcu_head;
    };
    unsigned int __unused;

    atomic_t __page_refcount;
    unsigned long obj_exts;
};

/* guard the struct slab has same member offset with struct page */
#define SLAB_MATCH(pg, sl) \
    static_assert(offsetof(struct page, pg) == offsetof(struct slab, sl))

SLAB_MATCH(flags, __page_flags);
SLAB_MATCH(compound_head, slab_cache); /* Ensure bit 0 is clear */
SLAB_MATCH(_refcount, __page_refcount);
SLAB_MATCH(memcg_data, obj_exts);
#undef SLAB_MATCH
```

## slub_alloc

![](../images/kernel/mem-slub.drawio.svg)

---

![](../images/kernel/mem-slab_alloc.png)

```c
static __fastpath_inline void *slab_alloc_node(struct kmem_cache *s, struct list_lru *lru,
        gfp_t gfpflags, int node, unsigned long addr, size_t orig_size)
{
    void *object;
    bool init = false;

    s = slab_pre_alloc_hook(s, gfpflags) {
        flags &= gfp_allowed_mask;

        might_alloc(flags) {
            fs_reclaim_acquire(gfp_mask);
            fs_reclaim_release(gfp_mask);

            if (current->flags & PF_MEMALLOC)
                return;

            ret = gfpflags_allow_blocking(gfp_mask) {
                return !!(gfp_flags & __GFP_DIRECT_RECLAIM)
            }
            might_sleep_if(ret) {
                if (cond) might_sleep();
            }
        }

        ret = should_failslab(s, flags) {
            int flags = 0;

            /* No fault-injection for bootstrap cache */
            if (unlikely(s == kmem_cache))
                return 0;

            if (gfpflags & __GFP_NOFAIL)
                return 0;

            if (failslab.ignore_gfp_reclaim && (gfpflags & __GFP_DIRECT_RECLAIM))
                return 0;

            if (failslab.cache_filter && !(s->flags & SLAB_FAILSLAB))
                return 0;

            /* In some cases, it expects to specify __GFP_NOWARN
            * to avoid printing any information(not just a warning),
            * thus avoiding deadlocks. See commit 6b9dbedbe349 for
            * details. */
            if (gfpflags & __GFP_NOWARN)
                flags |= FAULT_NOWARN;

            return should_fail_ex(&failslab.attr, s->object_size, flags) {
                bool stack_checked = false;

                if (in_task()) {
                    unsigned int fail_nth = READ_ONCE(current->fail_nth);

                    if (fail_nth) {
                        if (!fail_stacktrace(attr))
                            return false;

                        stack_checked = true;
                        fail_nth--;
                        WRITE_ONCE(current->fail_nth, fail_nth);
                        if (!fail_nth)
                            goto fail;

                        return false;
                    }
                }

                /* No need to check any other properties if the probability is 0 */
                if (attr->probability == 0)
                    return false;

                if (attr->task_filter && !fail_task(attr, current))
                    return false;

                if (atomic_read(&attr->times) == 0)
                    return false;

                if (!stack_checked && !fail_stacktrace(attr))
                    return false;

                if (atomic_read(&attr->space) > size) {
                    atomic_sub(size, &attr->space);
                    return false;
                }

                if (attr->interval > 1) {
                    attr->count++;
                    if (attr->count % attr->interval)
                        return false;
                }

                if (attr->probability <= fault_prandom_u32_below_100())
                    return false;

            fail:
                if (!(flags & FAULT_NOWARN))
                    fail_dump(attr);

                if (atomic_read(&attr->times) != -1)
                    atomic_dec_not_zero(&attr->times);

                return true;
            }
            ? -ENOMEM : 0;
        }
        if (unlikely(ret))
            return NULL;

        return s;
    }
    if (unlikely(!s))
        return NULL;

    object = kfence_alloc(s, orig_size, gfpflags);
    if (unlikely(object))
        goto out;

    if (s->cpu_sheaves)
        object = alloc_from_pcs(s, gfpflags, node);

    if (!object)
        object = __slab_alloc_node(s, gfpflags, node, addr, orig_size);

    maybe_wipe_obj_freeptr(s, object) {
        ret = freeptr_outside_object(s) {
            return s->offset >= s->inuse;
        }
        ret1 = slab_want_init_on_free(s) {
            if (static_branch_maybe(CONFIG_INIT_ON_FREE_DEFAULT_ON, &init_on_free))
                return !(c->ctor || (c->flags & (SLAB_TYPESAFE_BY_RCU | SLAB_POISON)));
            return false;
        }
        if (unlikely(ret1) && obj && !ret)
            memset((void *)((char *)kasan_reset_tag(obj) + s->offset),
                0, sizeof(void *));
    }
    init = slab_want_init_on_alloc(gfpflags, s);

out:
    /* When init equals 'true', like for kzalloc() family, only
     * @orig_size bytes might be zeroed instead of s->object_size
     * In case this fails due to memcg_slab_post_alloc_hook(),
     * object is set to NULL */
     slab_post_alloc_hook(s, lru, gfpflags, 1, &object, init, orig_size);

    return object;
}
```

### alloc_from_pcs

```c
void *alloc_from_pcs(struct kmem_cache *s, gfp_t gfp, int node)
{
    struct slub_percpu_sheaves *pcs;
    bool node_requested;
    void *object;

#ifdef CONFIG_NUMA
    if (static_branch_unlikely(&strict_numa) && node == NUMA_NO_NODE) {
        struct mempolicy *mpol = current->mempolicy;

        if (mpol) {
            /* Special BIND rule support. If the local node
             * is in permitted set then do not redirect
             * to a particular node.
             * Otherwise we apply the memory policy to get
             * the node we need to allocate on. */
            if (mpol->mode != MPOL_BIND ||
                    !node_isset(numa_mem_id(), mpol->nodes))

                node = mempolicy_slab_node();
        }
    }
#endif

    node_requested = IS_ENABLED(CONFIG_NUMA) && node != NUMA_NO_NODE;

    /* We assume the percpu sheaves contain only local objects although it's
     * not completely guaranteed, so we verify later. */
    if (unlikely(node_requested && node != numa_mem_id()))
        return NULL;

    if (!local_trylock(&s->cpu_sheaves->lock))
        return NULL;

    pcs = this_cpu_ptr(s->cpu_sheaves);

    if (unlikely(pcs->main->size == 0)) {
        pcs = __pcs_replace_empty_main(s, pcs, gfp) {
            struct slab_sheaf *empty = NULL;
            struct slab_sheaf *full;
            struct node_barn *barn;
            bool can_alloc;

            lockdep_assert_held(this_cpu_ptr(&s->cpu_sheaves->lock));

            if (pcs->spare && pcs->spare->size > 0) {
                swap(pcs->main, pcs->spare);
                return pcs;
            }

            barn = get_barn(s);
            if (!barn) {
                local_unlock(&s->cpu_sheaves->lock);
                return NULL;
            }

            full = barn_replace_empty_sheaf(barn, pcs->main) {
                struct slab_sheaf *full = NULL;
                unsigned long flags;

                if (!data_race(barn->nr_full))
                    return NULL;

                spin_lock_irqsave(&barn->lock, flags);

                if (likely(barn->nr_full)) {
                    full = list_first_entry(&barn->sheaves_full, struct slab_sheaf, barn_list);
                    list_del(&full->barn_list);
                    list_add(&empty->barn_list, &barn->sheaves_empty);
                    barn->nr_full--;
                    barn->nr_empty++;
                }

                spin_unlock_irqrestore(&barn->lock, flags);

                return full;
            }

            if (full) {
                stat(s, BARN_GET);
                pcs->main = full;
                return pcs;
            }

            stat(s, BARN_GET_FAIL);

            can_alloc = gfpflags_allow_blocking(gfp);

            if (can_alloc) {
                if (pcs->spare) {
                    empty = pcs->spare;
                    pcs->spare = NULL;
                } else {
                    empty = barn_get_empty_sheaf(barn);
                }
            }

            local_unlock(&s->cpu_sheaves->lock);

            if (!can_alloc)
                return NULL;

            if (empty) {
                if (!refill_sheaf(s, empty, gfp | __GFP_NOMEMALLOC)) {
                    full = empty;
                } else {
                    /* we must be very low on memory so don't bother
                    * with the barn */
                    free_empty_sheaf(s, empty);
                }
            } else {
                full = alloc_full_sheaf(s, gfp);
            }

            if (!full)
                return NULL;

            /* we can reach here only when gfpflags_allow_blocking
            * so this must not be an irq */
            local_lock(&s->cpu_sheaves->lock);
            pcs = this_cpu_ptr(s->cpu_sheaves);

            /* If we are returning empty sheaf, we either got it from the
            * barn or had to allocate one. If we are returning a full
            * sheaf, it's due to racing or being migrated to a different
            * cpu. Breaching the barn's sheaf limits should be thus rare
            * enough so just ignore them to simplify the recovery. */

            if (pcs->main->size == 0) {
                barn_put_empty_sheaf(barn, pcs->main);
                pcs->main = full;
                return pcs;
            }

            if (!pcs->spare) {
                pcs->spare = full;
                return pcs;
            }

            if (pcs->spare->size == 0) {
                barn_put_empty_sheaf(barn, pcs->spare);
                pcs->spare = full;
                return pcs;
            }

            barn_put_full_sheaf(barn, full);
            stat(s, BARN_PUT);

            return pcs;
        }
        if (unlikely(!pcs))
            return NULL;
    }

    object = pcs->main->objects[pcs->main->size - 1];

    if (unlikely(node_requested)) {
        /* Verify that the object was from the node we want. This could
         * be false because of cpu migration during an unlocked part of
         * the current allocation or previous freeing process. */
        if (page_to_nid(virt_to_page(object)) != node) {
            local_unlock(&s->cpu_sheaves->lock);
            return NULL;
        }
    }

    pcs->main->size--;

    local_unlock(&s->cpu_sheaves->lock);

    stat(s, ALLOC_PCS);

    return object;
}
```

### slab_alloc_node

```c
void *__slab_alloc_node(struct kmem_cache *s,
    gfp_t gfpflags, int node, unsigned long addr, size_t orig_size) {
    struct kmem_cache_cpu *c;
    struct slab *slab;
    unsigned long tid;
    void *object;

redo:
    /* 1. get free obj from cpu cache */
    c = raw_cpu_ptr(s->cpu_slab);
    tid = READ_ONCE(c->tid);
    object = c->freelist;
    slab = c->slab;

/* slow path */
    if (unlikely(!object || !slab || !node_match(slab, node))) {
        object = __slab_alloc(s, gfpflags, node, addr, c, orig_size) {
            c = slub_get_cpu_ptr(s->cpu_slab);
            return ___slab_alloc(s, gfpflags, node, addr, c, orig_size) {
            reread_slab:
                slab = READ_ONCE(c->slab);
                if (!slab) {
                    goto new_slab;
                }

                if (unlikely(slab != c->slab)) {
                    local_unlock_irqrestore(&s->cpu_slab->lock, flags);
                    goto reread_slab;
                }

                freelist = c->freelist;
                if (freelist)
                    goto load_freelist;

                /* Check the slab->freelist and either transfer the freelist to the
                 * per cpu freelist or deactivate the slab. */
                freelist = get_freelist(s, slab) {
                    struct slab new;
                    unsigned long counters;
                    void *freelist;

                    do {
                        freelist = slab->freelist;
                        counters = slab->counters;

                        new.counters = counters;
                        new.inuse = slab->objects;
                        new.frozen = freelist != NULL;
                    } while (!__slab_update_freelist(s, slab,
                        freelist, counters,
                        NULL, new.counters,
                        "get_freelist")
                    );

                    return freelist;
                }
                if (!freelist) {
                    c->slab = NULL;
                    c->tid = next_tid(c->tid);
                    goto new_slab;
                }

            load_freelist: /* load a freelist to cpu freelist */
                c->freelist = get_freepointer(s, freelist/*object*/) {
                    object = kasan_reset_tag(object);
                    ptr_addr = (unsigned long)object + s->offset;
                    p = *(freeptr_t *)(ptr_addr);
                    return freelist_ptr_decode(s, p, ptr_addr);
                }
                c->tid = next_tid(c->tid);
                return freelist;

            deactivate_slab: /* Finishes removing the cpu slab, merges cpu's freelist with slab's freelist */
                struct slab new;
                struct slab old;
                if (slab != c->slab) {
                    local_unlock_irqrestore(&s->cpu_slab->lock, flags);
                    goto reread_slab;
                }
                freelist = c->freelist;
                c->slab = NULL;
                c->freelist = NULL;
                c->tid = next_tid(c->tid);
                deactivate_slab(s, slab, freelist) {
                    if (slab->freelist) {
                        stat(s, DEACTIVATE_REMOTE_FREES);
                        tail = DEACTIVATE_TO_TAIL;
                    }

                    /* Stage one: Count the objects on cpu's freelist as free_delta and
                     * remember the last object in freelist_tail for later splicing. */
                    freelist_tail = NULL;
                    freelist_iter = freelist;
                    while (freelist_iter) {
                        nextfree = get_freepointer(s, freelist_iter);

                        if (freelist_corrupted(s, slab, &freelist_iter, nextfree))
                            break;

                        freelist_tail = freelist_iter;
                        free_delta++;

                        freelist_iter = nextfree;
                    }

                    /* Stage two: Unfreeze the slab while splicing the per-cpu
                     * freelist to the head of slab's freelist. */
                    do {
                        old.freelist = READ_ONCE(slab->freelist);
                        old.counters = READ_ONCE(slab->counters);
                        VM_BUG_ON(!old.frozen);

                        /* Determine target state of the slab */
                        new.counters = old.counters;
                        new.frozen = 0;
                        if (freelist_tail) {
                            new.inuse -= free_delta;
                            /* set freepointer of freelist_tail is old.freelist */
                            set_freepointer(s, freelist_tail, old.freelist);
                            new.freelist = freelist;
                        } else {
                            new.freelist = old.freelist;
                        }
                    } while (!slab_update_freelist(s, slab,
                        old.freelist, old.counters,
                        new.freelist, new.counters,
                        "unfreezing slab")
                    );

                    /* Stage three: Manipulate the slab list based on the updated state. */
                    if (!new.inuse && n->nr_partial >= s->min_partial) {
                        discard_slab(s, slab) {
                            free_slab(s, slab) {
                                __free_page();
                            }
                        }
                    } else if (new.freelist) {
                        add_partial(n, slab, tail) {
                            __add_partial(n, slab, tail) {
                                n->nr_partial++;
                                if (tail == DEACTIVATE_TO_TAIL) {
                                    list_add_tail(&slab->slab_list, &n->partial);
                                } else {
                                    list_add(&slab->slab_list, &n->partial);
                                }
                                slab_set_node_partial(slab) {
                                    set_bit(PG_workingset, folio_flags(slab_folio(slab), 0));
                                }
                            }
                        }
                    }
                }

            new_slab:
            #ifdef CONFIG_SLUB_CPU_PARTIAL
                while (slub_percpu_partial(c)) {
                    if (unlikely(c->slab)) {
                        goto reread_slab;
                    }
                    if (unlikely(!slub_percpu_partial(c))) {
                        /* we were preempted and partial list got empty */
                        goto new_objects;
                    }

                    slab = slub_percpu_partial(c);
                    slub_set_percpu_partial(c, slab) {
                        c->partial = slab->next;
                    }

                    if (unlikely(!node_match(slab, node) || !pfmemalloc_match(slab, gfpflags))) {
                        slab->next = NULL;
                        __put_partials(s, slab);
                        continue;
                    }

                    freelist = freeze_slab(s, slab);
                    goto retry_load_slab;
                }
            #endif

            new_objects:
                /* get partial from node */
                pc.flags = gfpflags;
                pc.slab = &slab;
                pc.orig_size = orig_size;
                slab = get_partial(s, node, &pc) {
                    obj = get_partial_node(s, n) {
                        list_for_each_entry_safe(slab, slab2, &n->partial, slab_list) {
                            if (IS_ENABLED(CONFIG_SLUB_TINY) || kmem_cache_debug(s)) {
                                obj = alloc_single_from_partial(s, n, slab, pc->orig_size) {
                                    object = slab->freelist;
                                    slab->freelist = get_freepointer(s, object) {
                                        /* `object + s->offset` stores next free obj addr */
                                        return freelist_dereference(s, object + s->offset);
                                    }
                                    slab->inuse++;

                                    /* slab is runing out, move it from parital list to full list */
                                    if (slab->inuse == slab->objects) {
                                        remove_partial(n, slab) {
                                            list_del(&slab->slab_list);
                                            n->nr_partial--;
                                        }
                                        add_full(s, n, slab) {
                                            list_add(&slab->slab_list, &n->full);
                                        }
                                    }
                                }

                                if (obj) {
                                    partial = slab;
                                    pc->object = object;
                                    break;
                                }
                                continue;
                            }

                            remove_partial(n, slab) {
                                list_del(&slab->slab_list);
                                slab_clear_node_partial(slab);
                                n->nr_partial--;
                            }

                            if (!partial) {
                                partial = slab;
                            } else {
                                /* Put a slab into a partial slab slot if available */
                                put_cpu_partial(s, slab, 0);
                                partial_slabs++;
                            }

                            if (!kmem_cache_has_cpu_partial(s) || partial_slabs > s->cpu_partial_slabs / 2) {
                                break;
                            }
                        }

                        return partial;
                    }

                    if (obj)
                        return obj;
                    obj = get_any_partial(s)
                        return NULL;
                }
                if (slab) {
                    freelist = freeze_slab(s, slab) {
                        struct slab new;
                        unsigned long counters;
                        void *freelist;

                        do {
                            freelist = slab->freelist;
                            counters = slab->counters;

                            new.counters = counters;
                            VM_BUG_ON(new.frozen);

                            new.inuse = slab->objects;
                            new.frozen = 1;

                            slab_update_freelist(s, slab,
                                freelist, counters,
                                NULL, new.counters, "freeze_slab") {

                                bool ret;

                                if (s->flags & __CMPXCHG_DOUBLE) {
                                    ret = __update_freelist_fast(slab, freelist_old, counters_old,
                                                        freelist_new, counters_new);
                                } else {
                                    unsigned long flags;

                                    local_irq_save(flags);
                                    ret = __update_freelist_slow(slab, freelist_old, counters_old,
                                                        freelist_new, counters_new) {
                                        slab_lock(slab);
                                        if (slab->freelist == freelist_old &&
                                            slab->counters == counters_old) {

                                            slab->freelist = freelist_new /* NULL */;
                                            slab->counters = counters_new;
                                            ret = true;
                                        }
                                        slab_unlock(slab);
                                    }
                                    local_irq_restore(flags);
                                }
                                if (likely(ret))
                                    return true;

                                return false;
                            }
                        } while (!slab_update_freelist());

                        return freelist;
                    }
                    goto retry_load_slab;
                }

                slub_put_cpu_ptr(s->cpu_slab);
                /* node has no partial, alloc_page */
                slab = new_slab(s, gfpflags, node) {
                    return allocate_slab(s, flags & (GFP_RECLAIM_MASK | GFP_CONSTRAINT_MASK), node) {
                        flags &= gfp_allowed_mask;
                        flags |= s->allocflags;

                        alloc_gfp = (flags | __GFP_NOWARN | __GFP_NORETRY) & ~__GFP_NOFAIL;
                        if ((alloc_gfp & __GFP_DIRECT_RECLAIM) && oo_order(oo) > oo_order(s->min))
                            alloc_gfp = (alloc_gfp | __GFP_NOMEMALLOC) & ~__GFP_RECLAIM;

                        slab = alloc_slab_page(alloc_gfp, node, oo);
                        if (unlikely(!slab)) {
                            oo = s->min;
                            alloc_gfp = flags;
                            slab = alloc_slab_page(alloc_gfp, node, oo);
                            if (unlikely(!slab))
                                return NULL;
                            stat(s, ORDER_FALLBACK);
                        }

                        slab->objects = oo_objects(oo);
                        slab->inuse = 0;
                        slab->frozen = 0;

                        account_slab(slab, oo_order(oo), s, flags);

                        slab->slab_cache = s;

                        kasan_poison_slab(slab);

                        start = slab_address(slab) {
                            return folio_address(slab_folio(slab));
                        }

                        setup_slab_debug(s, slab, start);

                        /* Shuffle the single linked freelist based on a random pre-computed sequence */
                        shuffle = shuffle_freelist(s, slab);

                        if (!shuffle) {
                            start = fixup_red_left(s, start) {
                                if (kmem_cache_debug_flags(s, SLAB_RED_ZONE))
                                    p += s->red_left_pad;
                                return p;
                            }
                            start = setup_object(s, start) {
                                object = kasan_init_slab_obj(s, object);
                                if (unlikely(s->ctor)) {
                                    s->ctor(object);
                                }
                                return object;
                            }
                            slab->freelist = start;
                            for (idx = 0, p = start; idx < slab->objects - 1; idx++) {
                                next = p + s->size;
                                next = setup_object(s, next);
                                set_freepointer(s, p/*object*/, next/*fp*/) {
                                    unsigned long freeptr_addr = (unsigned long)object + s->offset;
                                    freeptr_addr = (unsigned long)kasan_reset_tag((void *)freeptr_addr);
                                    *(freeptr_t *)freeptr_addr = freelist_ptr_encode(s, fp, freeptr_addr);
                                }
                                p = next;
                            }
                            set_freepointer(s, p, NULL);
                        }

                        return slab;
                    }
                }
                c = slub_get_cpu_ptr(s->cpu_slab);

                if (unlikely(!slab)) {
                    slab_out_of_memory(s, gfpflags, node);
                    return NULL;
                }

                freelist = slab->freelist;
                slab->freelist = NULL;
                slab->inuse = slab->objects;
                slab->frozen = 1;

            retry_load_slab:
                if (unlikely(c->slab)) {
                    void *flush_freelist = c->freelist;
                    struct slab *flush_slab = c->slab;

                    c->slab = NULL;
                    c->freelist = NULL;
                    c->tid = next_tid(c->tid);

                    deactivate_slab(s, flush_slab, flush_freelist);

                    goto retry_load_slab;
                }

                c->slab = slab;
                goto load_freelist;
            }
        }
    } else {
/* fast path */
        void *next_object = get_freepointer_safe(s, object) {
            unsigned long freepointer_addr;
            freeptr_t p;

            if (!debug_pagealloc_enabled_static())
                return get_freepointer(s, object);

            object = kasan_reset_tag(object);
            freepointer_addr = (unsigned long)object + s->offset;
            copy_from_kernel_nofault(&p, (freeptr_t *)freepointer_addr, sizeof(p));
            return freelist_ptr_decode(s, p, freepointer_addr);
        }

        if (unlikely(!this_cpu_cmpxchg_double(
            s->cpu_slab->freelist, s->cpu_slab->tid,
            object, tid,
            next_object, next_tid(tid))))
        {
            note_cmpxchg_failure("slab_alloc", s, tid);
            goto redo;
        }

        prefetch_freepointer(s, next_object);
    }

    return object;
}
```

### slab_post_alloc_hook

```c
slab_post_alloc_hook(s, lru, gfpflags, 1, &object, init, orig_size) {
    unsigned int zero_size = s->object_size;
    bool kasan_init = init;
    size_t i;
    gfp_t init_flags = flags & gfp_allowed_mask;

    /* For kmalloc object, the allocated memory size(object_size) is likely
    * larger than the requested size(orig_size). If redzone check is
    * enabled for the extra space, don't zero it, as it will be redzoned
    * soon. The redzone operation for this extra space could be seen as a
    * replacement of current poisoning under certain debug option, and
    * won't break other sanity checks. */
    if (kmem_cache_debug_flags(s, SLAB_STORE_USER | SLAB_RED_ZONE) &&
        (s->flags & SLAB_KMALLOC))
        zero_size = orig_size;

    /* When slab_debug is enabled, avoid memory initialization integrated
    * into KASAN and instead zero out the memory via the memset below with
    * the proper size. Otherwise, KASAN might overwrite SLUB redzones and
    * cause false-positive reports. This does not lead to a performance
    * penalty on production builds, as slab_debug is not intended to be
    * enabled there. */
    if (__slub_debug_enabled())
        kasan_init = false;

    /* As memory initialization might be integrated into KASAN,
    * kasan_slab_alloc and initialization memset must be
    * kept together to avoid discrepancies in behavior.
    *
    * As p[i] might get tagged, memset and kmemleak hook come after KASAN. */
    for (i = 0; i < size; i++) {
        p[i] = kasan_slab_alloc(s, p[i], init_flags, kasan_init);
        if (p[i] && init && (!kasan_init || !kasan_has_integrated_init()))
            memset(p[i], 0, zero_size);
        if (gfpflags_allow_spinning(flags))
            kmemleak_alloc_recursive(p[i], s->object_size, 1, s->flags, init_flags);
        kmsan_slab_alloc(s, p[i], init_flags);
        alloc_tagging_slab_alloc_hook(s, p[i], flags);
    }

    return memcg_slab_post_alloc_hook(s, lru, flags, size, p) {
        if (likely(!memcg_kmem_online()))
            return true;

        if (likely(!(flags & __GFP_ACCOUNT) && !(s->flags & SLAB_ACCOUNT)))
            return true;

        ret = __memcg_slab_post_alloc_hook(s, lru, flags, size, p) {
            struct obj_cgroup *objcg;
            struct slab *slab;
            unsigned long off;
            size_t i;

            /* The obtained objcg pointer is safe to use within the current scope,
            * defined by current task or set_active_memcg() pair.
            * obj_cgroup_get() is used to get a permanent reference. */
            objcg = current_obj_cgroup();
            if (!objcg)
                return true;

            /* slab_alloc_node() avoids the NULL check, so we might be called with a
            * single NULL object. kmem_cache_alloc_bulk() aborts if it can't fill
            * the whole requested size.
            * return success as there's nothing to free back */
            if (unlikely(*p == NULL))
                return true;

            flags &= gfp_allowed_mask;

            if (lru) {
                int ret;
                struct mem_cgroup *memcg;

                memcg = get_mem_cgroup_from_objcg(objcg);
                ret = memcg_list_lru_alloc(memcg, lru, flags);
                css_put(&memcg->css);

                if (ret)
                    return false;
            }

            for (i = 0; i < size; i++) {
                slab = virt_to_slab(p[i]);

                if (!slab_obj_exts(slab) && alloc_slab_obj_exts(slab, s, flags, false)) {
                    continue;
                }

                /* if we fail and size is 1, memcg_alloc_abort_single() will
                * just free the object, which is ok as we have not assigned
                * objcg to its obj_ext yet
                *
                * for larger sizes, kmem_cache_free_bulk() will uncharge
                * any objects that were already charged and obj_ext assigned
                *
                * TODO: we could batch this until slab_pgdat(slab) changes
                * between iterations, with a more complicated undo */
                if (obj_cgroup_charge_account(objcg, flags, obj_full_size(s), slab_pgdat(slab), cache_vmstat_idx(s)))
                    return false;

                off = obj_to_index(s, slab, p[i]);
                obj_cgroup_get(objcg);
                slab_obj_exts(slab)[off].objcg = objcg;
            }

            return true;
        }
        if (likely(ret))
            return true;

        if (likely(size == 1)) {
            memcg_alloc_abort_single(s, *p);
            *p = NULL;
        } else {
            kmem_cache_free_bulk(s, size, p);
        }

        return false;
    }
}

int obj_cgroup_charge_account(struct obj_cgroup *objcg, gfp_t gfp, size_t size,
                     struct pglist_data *pgdat, enum node_stat_item idx)
{
    unsigned int nr_pages, nr_bytes;
    int ret;

    if (likely(consume_obj_stock(objcg, size, pgdat, idx)))
        return 0;

    /* In theory, objcg->nr_charged_bytes can have enough
     * pre-charged bytes to satisfy the allocation. However,
     * flushing objcg->nr_charged_bytes requires two atomic
     * operations, and objcg->nr_charged_bytes can't be big.
     * The shared objcg->nr_charged_bytes can also become a
     * performance bottleneck if all tasks of the same memcg are
     * trying to update it. So it's better to ignore it and try
     * grab some new pages. The stock's nr_bytes will be flushed to
     * objcg->nr_charged_bytes later on when objcg changes.
     *
     * The stock's nr_bytes may contain enough pre-charged bytes
     * to allow one less page from being charged, but we can't rely
     * on the pre-charged bytes not being changed outside of
     * consume_obj_stock() or refill_obj_stock(). So ignore those
     * pre-charged bytes as well when charging pages. To avoid a
     * page uncharge right after a page charge, we set the
     * allow_uncharge flag to false when calling refill_obj_stock()
     * to temporarily allow the pre-charged bytes to exceed the page
     * size limit. The maximum reachable value of the pre-charged
     * bytes is (sizeof(object) + PAGE_SIZE - 2) if there is no data
     * race. */
    nr_pages = size >> PAGE_SHIFT;
    nr_bytes = size & (PAGE_SIZE - 1);

    if (nr_bytes)
        nr_pages += 1;

    ret = obj_cgroup_charge_pages(objcg, gfp, nr_pages) {
        struct mem_cgroup *memcg;
        int ret;

        memcg = get_mem_cgroup_from_objcg(objcg);

        ret = try_charge_memcg(memcg, gfp, nr_pages);
        if (ret)
            goto out;

        account_kmem_nmi_safe(memcg, nr_pages);
        memcg1_account_kmem(memcg, nr_pages);
    out:
        css_put(&memcg->css);

        return ret;
    }
    if (!ret && (nr_bytes || pgdat))
        refill_obj_stock(objcg, nr_bytes ? PAGE_SIZE - nr_bytes : 0, false, size, pgdat, idx);

    return ret;
}

static bool consume_obj_stock(struct obj_cgroup *objcg, unsigned int nr_bytes,
                  struct pglist_data *pgdat, enum node_stat_item idx)
{
    struct obj_stock_pcp *stock;
    bool ret = false;

    if (!local_trylock(&obj_stock.lock))
        return ret;

    stock = this_cpu_ptr(&obj_stock);
    if (objcg == READ_ONCE(stock->cached_objcg) && stock->nr_bytes >= nr_bytes) {
        stock->nr_bytes -= nr_bytes;
        ret = true;

        if (pgdat) {
            __account_obj_stock(objcg, stock, nr_bytes, pgdat, idx) {
                int *bytes;

                /* Save vmstat data in stock and skip vmstat array update unless
                * accumulating over a page of vmstat data or when pgdat changes. */
                if (stock->cached_pgdat != pgdat) {
                    /* Flush the existing cached vmstat data */
                    struct pglist_data *oldpg = stock->cached_pgdat;

                    if (stock->nr_slab_reclaimable_b) {
                        mod_objcg_mlstate(objcg, oldpg, NR_SLAB_RECLAIMABLE_B,
                                stock->nr_slab_reclaimable_b);
                        stock->nr_slab_reclaimable_b = 0;
                    }
                    if (stock->nr_slab_unreclaimable_b) {
                        mod_objcg_mlstate(objcg, oldpg, NR_SLAB_UNRECLAIMABLE_B,
                                stock->nr_slab_unreclaimable_b);
                        stock->nr_slab_unreclaimable_b = 0;
                    }
                    stock->cached_pgdat = pgdat;
                }

                bytes = (idx == NR_SLAB_RECLAIMABLE_B) ? &stock->nr_slab_reclaimable_b
                                    : &stock->nr_slab_unreclaimable_b;
                /* Even for large object >= PAGE_SIZE, the vmstat data will still be
                * cached locally at least once before pushing it out. */
                if (!*bytes) {
                    *bytes = nr;
                    nr = 0;
                } else {
                    *bytes += nr;
                    if (abs(*bytes) > PAGE_SIZE) {
                        nr = *bytes;
                        *bytes = 0;
                    } else {
                        nr = 0;
                    }
                }
                if (nr)
                    mod_objcg_mlstate(objcg, pgdat, idx, nr);
            }
        }
    }

    local_unlock(&obj_stock.lock);

    return ret;
}

void refill_obj_stock(struct obj_cgroup *objcg, unsigned int nr_bytes,
        bool allow_uncharge, int nr_acct, struct pglist_data *pgdat,
        enum node_stat_item idx)
{
    struct obj_stock_pcp *stock;
    unsigned int nr_pages = 0;

    if (!local_trylock(&obj_stock.lock)) {
        if (pgdat)
            mod_objcg_mlstate(objcg, pgdat, idx, nr_bytes);
        nr_pages = nr_bytes >> PAGE_SHIFT;
        nr_bytes = nr_bytes & (PAGE_SIZE - 1);
        atomic_add(nr_bytes, &objcg->nr_charged_bytes);
        goto out;
    }

    stock = this_cpu_ptr(&obj_stock);
    if (READ_ONCE(stock->cached_objcg) != objcg) { /* reset if necessary */
        drain_obj_stock(stock);
        obj_cgroup_get(objcg);
        stock->nr_bytes = atomic_read(&objcg->nr_charged_bytes)
                ? atomic_xchg(&objcg->nr_charged_bytes, 0) : 0;
        WRITE_ONCE(stock->cached_objcg, objcg);

        allow_uncharge = true;    /* Allow uncharge when objcg changes */
    }
    stock->nr_bytes += nr_bytes;

    if (pgdat)
        __account_obj_stock(objcg, stock, nr_acct, pgdat, idx);
            --->

    if (allow_uncharge && (stock->nr_bytes > PAGE_SIZE)) {
        nr_pages = stock->nr_bytes >> PAGE_SHIFT;
        stock->nr_bytes &= (PAGE_SIZE - 1);
    }

    local_unlock(&obj_stock.lock);
out:
    if (nr_pages) {
        obj_cgroup_uncharge_pages(objcg, nr_pages) {
            struct mem_cgroup *memcg;

            memcg = get_mem_cgroup_from_objcg(objcg);

            account_kmem_nmi_safe(memcg, -nr_pages) {
                if (likely(!in_nmi())) {
                    mod_memcg_state(memcg, MEMCG_KMEM, val) {
                        int i = memcg_stats_index(idx);
                        int cpu;

                        if (mem_cgroup_disabled())
                            return;

                        if (WARN_ONCE(BAD_STAT_IDX(i), "%s: missing stat item %d\n", __func__, idx))
                            return;

                        while (memcg_is_dying(memcg))
                            memcg = parent_mem_cgroup(memcg);

                        cpu = get_cpu();

                        this_cpu_add(memcg->vmstats_percpu->state[i], val);
                        val = memcg_state_val_in_pages(idx, val);
                        memcg_rstat_updated(memcg, val, cpu);
                        trace_mod_memcg_state(memcg, idx, val);

                        put_cpu();
                    }
                } else {
                    /* preemption is disabled in_nmi(). */
                    css_rstat_updated(&memcg->css, smp_processor_id());
                    atomic_add(val, &memcg->kmem_stat);
                }
            }
            memcg1_account_kmem(memcg, -nr_pages);
            if (!mem_cgroup_is_root(memcg)) {
                refill_stock(memcg, nr_pages);
            }

            css_put(&memcg->css);
        }
    }
}
```

## slab_free

![](../images/kernel/mme-slab_free.png)

```c
void kmem_cache_free(struct kmem_cache *s, void *x)
{
    s = cache_from_obj(s, x) {
        return virt_to_cache(x) {
            struct slab *slab = virt_to_slab(obj) {
                struct folio *folio = virt_to_folio(addr);
                if (!folio_test_slab(folio))
                    return NULL;
                return folio_slab(folio);
            }
            return slab->slab_cache;
        }
    }
    if (!s)
        return;
    slab_free(s, virt_to_slab(x), x, _RET_IP_);
}

void slab_free(struct kmem_cache *s, struct slab *slab, void *object,
           unsigned long addr)
{
    memcg_slab_free_hook(s, slab, &object, 1) {
        struct slabobj_ext *obj_exts;

        if (!memcg_kmem_online())
            return;

        obj_exts = slab_obj_exts(slab);
        if (likely(!obj_exts))
            return;

        __memcg_slab_free_hook(s, slab, p, objects, obj_exts) {
            size_t obj_size = obj_full_size(s);

            for (int i = 0; i < objects; i++) {
                struct obj_cgroup *objcg;
                unsigned int off;

                off = obj_to_index(s, slab, p[i]) {
                    if (is_kfence_address(obj))
                        return 0;
                    return __obj_to_index(cache, slab_address(slab), obj) {
                        return reciprocal_divide(kasan_reset_tag(obj) - addr, cache->reciprocal_size);
                    }
                }
                objcg = obj_exts[off].objcg;
                if (!objcg)
                    continue;

                obj_exts[off].objcg = NULL;
                refill_obj_stock(objcg, obj_size, true, -obj_size, slab_pgdat(slab), cache_vmstat_idx(s));
                obj_cgroup_put(objcg);
            }
        }

    }
    alloc_tagging_slab_free_hook(s, slab, &object, 1);

    if (unlikely(!slab_free_hook(s, object, slab_want_init_on_free(s), false)))
        return;

    if (s->cpu_sheaves
        && likely(!IS_ENABLED(CONFIG_NUMA) || slab_nid(slab) == numa_mem_id())
        && likely(!slab_test_pfmemalloc(slab))) {
        if (likely(free_to_pcs(s, object)))
            return;
    }

    do_slab_free(s, slab, object/*head*/, object/*tail*/, 1/*cnt*/, addr) {
        struct kmem_cache_cpu *c;
        unsigned long tid;
        void **freelist;

    redo:
        c = raw_cpu_ptr(s->cpu_slab);
        tid = READ_ONCE(c->tid);

        /* Same with comment on barrier() in slab_alloc_node() */
        barrier();

/* slow path: slab doesnt belong to cur cpu cache */
        if (unlikely(slab != c->slab)) {
            __slab_free(s, slab, head, tail, cnt, addr) {
                struct slab new;

                do {
                    if (unlikely(n)) {
                        spin_unlock_irqrestore(&n->list_lock, flags);
                        n = NULL;
                    }
                    prior = slab->freelist;
                    counters = slab->counters;
                    set_freepointer(s, tail, prior);
                    new.counters = counters;
                    was_frozen = new.frozen;
                    new.inuse -= cnt;
                    /* !was_frozen: not on cpu cache
                        * !new.inuse: slab is empty (0 objs used)
                        * !prior: slab was full (all objs used ) */
                    if ((!new.inuse || !prior) && !was_frozen) {
                        /* Needs to be taken off a list */
                        if (!kmem_cache_has_cpu_partial(s) || prior) {
                            n = get_node(s, slab_nid(slab));
                            spin_lock_irqsave(&n->list_lock, flags);
                            on_node_partial = slab_test_node_partial(slab);
                        }
                    }
                } while (!slab_update_freelist(s, slab,
                    prior, counters,
                    head, new.counters,
                    "__slab_free")
                );

                if (likely(!n)) {
                    if (likely(was_frozen)) {
                        stat(s, FREE_FROZEN);
                    } else if (kmem_cache_has_cpu_partial(s) && !prior) {
                        /* !prior means we started with a full slab */
                        put_cpu_partial(s, slab, 1/*drain*/) {
                            struct slab *oldslab;
                            struct slab *slab_to_put = NULL;
                            unsigned long flags;
                            int slabs = 0;

                            /* head partial slab records the totoal nr of slabs */
                            oldslab = this_cpu_read(s->cpu_slab->partial);

                            if (oldslab) {
                                /* put excessive cpu partial to node partial */
                                if (drain && oldslab->slabs >= s->cpu_partial_slabs) {
                                    slab_to_put = oldslab;
                                    oldslab = NULL;
                                } else {
                                    slabs = oldslab->slabs;
                                }
                            }

                            slabs++;

                            slab->slabs = slabs;
                            slab->next = oldslab;

                            this_cpu_write(s->cpu_slab->partial, slab);

                            if (slab_to_put) {
                                __put_partials(s, slab_to_put) {
                                    while (partial_slab) {
                                        slab = partial_slab;
                                        partial_slab = slab->next;

                                        n2 = get_node(s, slab_nid(slab));
                                        if (n != n2) {
                                            if (n) {
                                                spin_unlock_irqrestore(&n->list_lock, flags);
                                            }
                                            n = n2;
                                            spin_lock_irqsave(&n->list_lock, flags);
                                        }

                                        /* put excessive node parital to buddy system */
                                        if (unlikely(!slab->inuse && n->nr_partial >= s->min_partial)) {
                                            slab->next = slab_to_discard;
                                            slab_to_discard = slab;
                                        } else {
                                            add_partial(n, slab, DEACTIVATE_TO_TAIL);
                                        }
                                    }

                                    if (n)
                                        spin_unlock_irqrestore(&n->list_lock, flags);

                                    while (slab_to_discard) {
                                        slab = slab_to_discard;
                                        slab_to_discard = slab_to_discard->next;
                                        discard_slab(s, slab);
                                    }
                                }
                            }
                        }
                        stat(s, CPU_PARTIAL_FREE);
                    }

                    return;
                }

                /* This slab was partially empty but not on the per-node partial list,
                    * in which case we shouldn't manipulate its list, just return. */
                if (prior && !on_node_partial) {
                    spin_unlock_irqrestore(&n->list_lock, flags);
                    return;
                }

                if (unlikely(!new.inuse && n->nr_partial >= s->min_partial))
                    goto slab_empty;

                /* Objects left in the slab. If it was not on the partial list before
                    * then add it.
                    * !prior: on full list */
                if (!kmem_cache_has_cpu_partial(s) && unlikely(!prior)) {
                    remove_full(s, n, slab);
                    add_partial(n, slab, DEACTIVATE_TO_TAIL);
                }
                spin_unlock_irqrestore(&n->list_lock, flags);
                return;

            /* free excessive empty slab to buddy system */
            slab_empty:
                if (prior) { /* Slab on the partial list. */
                    remove_partial(n, slab) {
                        list_del(&slab->slab_list);
                        slab_clear_node_partial(slab) {
                            clear_bit(PG_workingset, folio_flags(slab_folio(slab), 0));
                        }
                        n->nr_partial--;
                    }
                } else { /* Slab must be on the full list */
                    remove_full(s, n, slab)  {
                        /* full list is only enabled as SLAB_STORE_USER enabled */
                        if (!(s->flags & SLAB_STORE_USER))
                            return;
                        list_del(&slab->slab_list);
                    }
                }

                spin_unlock_irqrestore(&n->list_lock, flags);
                stat(s, FREE_SLAB);
                discard_slab(s, slab) {
                    __free_pages();
                }
            }
            return;
        }

/* fast path: slab belongs to cur cpu slab */

        if (USE_LOCKLESS_FAST_PATH()) {
            freelist = READ_ONCE(c->freelist);

            set_freepointer(s, tail, freelist);

            if (unlikely(!__update_cpu_freelist_fast(s, freelist, head, tid))) {
                note_cmpxchg_failure("slab_free", s, tid);
                goto redo;
            }
        } else {
            /* Update the free list under the local lock */
            local_lock(&s->cpu_slab->lock);
            c = this_cpu_ptr(s->cpu_slab);
            if (unlikely(slab != c->slab)) {
                local_unlock(&s->cpu_slab->lock);
                goto redo;
            }
            tid = c->tid;
            freelist = c->freelist;

            set_freepointer(s, tail, freelist);
            c->freelist = head;
            c->tid = next_tid(tid);

            local_unlock(&s->cpu_slab->lock);
        }
    }
}
```

### slab_free_hook

```c
static __always_inline
bool slab_free_hook(struct kmem_cache *s, void *x, bool init,
            bool after_rcu_delay)
{
    /* Are the object contents still accessible? */
    bool still_accessible = (s->flags & SLAB_TYPESAFE_BY_RCU) && !after_rcu_delay;

    kmemleak_free_recursive(x, s->flags);
    kmsan_slab_free(s, x);

    debug_check_no_locks_freed(x, s->object_size);

    if (!(s->flags & SLAB_DEBUG_OBJECTS))
        debug_check_no_obj_freed(x, s->object_size);

    /* Use KCSAN to help debug racy use-after-free. */
    if (!still_accessible)
        __kcsan_check_access(x, s->object_size, KCSAN_ACCESS_WRITE | KCSAN_ACCESS_ASSERT);

    if (kfence_free(x))
        return false;

    /* Give KASAN a chance to notice an invalid free operation before we
     * modify the object. */
    if (kasan_slab_pre_free(s, x))
        return false;

#ifdef CONFIG_SLUB_RCU_DEBUG
    if (still_accessible) {
        struct rcu_delayed_free *delayed_free;

        delayed_free = kmalloc(sizeof(*delayed_free), GFP_NOWAIT);
        if (delayed_free) {
            /* Let KASAN track our call stack as a "related work
             * creation", just like if the object had been freed
             * normally via kfree_rcu().
             * We have to do this manually because the rcu_head is
             * not located inside the object. */
            kasan_record_aux_stack(x);

            delayed_free->object = x;
            call_rcu(&delayed_free->head, slab_free_after_rcu_debug);
            return false;
        }
    }
#endif /* CONFIG_SLUB_RCU_DEBUG */

    /* As memory initialization might be integrated into KASAN,
     * kasan_slab_free and initialization memset's must be
     * kept together to avoid discrepancies in behavior.
     *
     * The initialization memset's clear the object and the metadata,
     * but don't touch the SLAB redzone.
     *
     * The object's freepointer is also avoided if stored outside the
     * object. */
    if (unlikely(init)) {
        int rsize;
        unsigned int inuse, orig_size;

        inuse = get_info_end(s);
        orig_size = get_orig_size(s, x);
        if (!kasan_has_integrated_init())
            memset(kasan_reset_tag(x), 0, orig_size);
        rsize = (s->flags & SLAB_RED_ZONE) ? s->red_left_pad : 0;
        memset((char *)kasan_reset_tag(x) + inuse, 0, s->size - inuse - rsize);
        /* Restore orig_size, otherwise kmalloc redzone overwritten
         * would be reported */
        set_orig_size(s, x, orig_size);

    }
    /* KASAN might put x into memory quarantine, delaying its reuse. */
    return !kasan_slab_free(s, x, init, still_accessible, false);
}
```

### free_to_pcs

```c
bool free_to_pcs(struct kmem_cache *s, void *object)
{
    struct slub_percpu_sheaves *pcs;

    if (!local_trylock(&s->cpu_sheaves->lock))
        return false;

    pcs = this_cpu_ptr(s->cpu_sheaves);

    if (unlikely(pcs->main->size == s->sheaf_capacity)) {
        pcs = __pcs_replace_full_main(s, pcs);
        if (unlikely(!pcs))
            return false;
    }

    pcs->main->objects[pcs->main->size++] = object;

    local_unlock(&s->cpu_sheaves->lock);

    stat(s, FREE_PCS);

    return true;
}

struct slub_percpu_sheaves *
__pcs_replace_full_main(struct kmem_cache *s, struct slub_percpu_sheaves *pcs)
{
    struct slab_sheaf *empty;
    struct node_barn *barn;
    bool put_fail;

restart:
    lockdep_assert_held(this_cpu_ptr(&s->cpu_sheaves->lock));

    barn = get_barn(s);
    if (!barn) {
        local_unlock(&s->cpu_sheaves->lock);
        return NULL;
    }

    put_fail = false;

    if (!pcs->spare) {
        empty = barn_get_empty_sheaf(barn);
        if (empty) {
            pcs->spare = pcs->main;
            pcs->main = empty;
            return pcs;
        }
        goto alloc_empty;
    }

    if (pcs->spare->size < s->sheaf_capacity) {
        swap(pcs->main, pcs->spare);
        return pcs;
    }

    empty = barn_replace_full_sheaf(barn, pcs->main) {
        struct slab_sheaf *empty;
        unsigned long flags;

        /* we don't repeat this check under barn->lock as it's not critical */
        if (data_race(barn->nr_full) >= MAX_FULL_SHEAVES)
            return ERR_PTR(-E2BIG);
        if (!data_race(barn->nr_empty))
            return ERR_PTR(-ENOMEM);

        spin_lock_irqsave(&barn->lock, flags);

        if (likely(barn->nr_empty)) {
            empty = list_first_entry(&barn->sheaves_empty, struct slab_sheaf, barn_list);
            list_del(&empty->barn_list);
            list_add(&full->barn_list, &barn->sheaves_full);
            barn->nr_empty--;
            barn->nr_full++;
        } else {
            empty = ERR_PTR(-ENOMEM);
        }

        spin_unlock_irqrestore(&barn->lock, flags);

        return empty;
    }

    if (!IS_ERR(empty)) {
        stat(s, BARN_PUT);
        pcs->main = empty;
        return pcs;
    }

    if (PTR_ERR(empty) == -E2BIG) {
        /* Since we got here, spare exists and is full */
        struct slab_sheaf *to_flush = pcs->spare;

        stat(s, BARN_PUT_FAIL);

        pcs->spare = NULL;
        local_unlock(&s->cpu_sheaves->lock);

        sheaf_flush_unused(s, to_flush);
        empty = to_flush;
        goto got_empty;
    }

    /* We could not replace full sheaf because barn had no empty
     * sheaves. We can still allocate it and put the full sheaf in
     * __pcs_install_empty_sheaf(), but if we fail to allocate it,
     * make sure to count the fail. */
    put_fail = true;

alloc_empty:
    local_unlock(&s->cpu_sheaves->lock);

    empty = alloc_empty_sheaf(s, GFP_NOWAIT);
    if (empty)
        goto got_empty;

    if (put_fail)
         stat(s, BARN_PUT_FAIL);

    if (!sheaf_flush_main(s))
        return NULL;

    if (!local_trylock(&s->cpu_sheaves->lock))
        return NULL;

    pcs = this_cpu_ptr(s->cpu_sheaves);

    /* we flushed the main sheaf so it should be empty now,
     * but in case we got preempted or migrated, we need to
     * check again */
    if (pcs->main->size == s->sheaf_capacity)
        goto restart;

    return pcs;

got_empty:
    if (!local_trylock(&s->cpu_sheaves->lock)) {
        barn_put_empty_sheaf(barn, empty);
        return NULL;
    }

    pcs = this_cpu_ptr(s->cpu_sheaves);
    __pcs_install_empty_sheaf(s, pcs, empty, barn) {
        lockdep_assert_held(this_cpu_ptr(&s->cpu_sheaves->lock));

        /* This is what we expect to find if nobody interrupted us. */
        if (likely(!pcs->spare)) {
            pcs->spare = pcs->main;
            pcs->main = empty;
            return;
        }

        /* Unlikely because if the main sheaf had space, we would have just
        * freed to it. Get rid of our empty sheaf. */
        if (pcs->main->size < s->sheaf_capacity) {
            barn_put_empty_sheaf(barn, empty);
            return;
        }

        /* Also unlikely for the same reason */
        if (pcs->spare->size < s->sheaf_capacity) {
            swap(pcs->main, pcs->spare);
            barn_put_empty_sheaf(barn, empty);
            return;
        }

        /* We probably failed barn_replace_full_sheaf() due to no empty sheaf
        * available there, but we allocated one, so finish the job. */
        barn_put_full_sheaf(barn, pcs->main);
        stat(s, BARN_PUT);
        pcs->main = empty;
    }

    return pcs;
}
```

### sheaf_flush_unused

```c
static void sheaf_flush_unused(struct kmem_cache *s, struct slab_sheaf *sheaf)
{
    if (!sheaf->size)
        return;

    stat_add(s, SHEAF_FLUSH, sheaf->size);

    __kmem_cache_free_bulk(s, sheaf->size, &sheaf->objects[0]/*p*/) {
        if (!size)
            return;

        do {
            struct detached_freelist df;

            size = build_detached_freelist(s, size, p, &df) {
                int lookahead = 3;
                void *object;
                struct page *page;
                struct slab *slab;
                size_t same;

                object = p[--size];
                page = virt_to_page(object);
                slab = page_slab(page);
                if (!s) {
                    /* Handle kalloc'ed objects */
                    if (!slab) {
                        free_large_kmalloc(page, object);
                        df->slab = NULL;
                        return size;
                    }
                    /* Derive kmem_cache from object */
                    df->slab = slab;
                    df->s = slab->slab_cache;
                } else {
                    df->slab = slab;
                    df->s = cache_from_obj(s, object); /* Support for memcg */
                }

                /* Start new detached freelist */
                df->tail = object;
                df->freelist = object;
                df->cnt = 1;

                if (is_kfence_address(object))
                    return size;

                set_freepointer(df->s, object, NULL);

                same = size;
                while (size) {
                    object = p[--size];
                    /* df->slab is always set at this point */
                    if (df->slab == virt_to_slab(object)) {
                        /* Opportunity build freelist */
                        set_freepointer(df->s, object, df->freelist);
                        df->freelist = object;
                        df->cnt++;
                        same--;
                        if (size != same)
                            swap(p[size], p[same]);
                        continue;
                    }

                    /* Limit look ahead search */
                    if (!--lookahead)
                        break;
                }

                return same;
            }
            if (!df.slab)
                continue;

            if (kfence_free(df.freelist))
                continue;

            do_slab_free(df.s, df.slab, df.freelist, df.tail, df.cnt, _RET_IP_);
                --->
        } while (likely(size));
    }

    sheaf->size = 0;
}
```

## kmem_cache_shrink

```c
int kmem_cache_shrink(struct kmem_cache *cachep)
{
    kasan_cache_shrink(cachep);

    return __kmem_cache_shrink(cachep) {
        flush_all(s);
        return __kmem_cache_do_shrink(s);
    }
}

int __kmem_cache_do_shrink(struct kmem_cache *s)
{
    int node;
    int i;
    struct kmem_cache_node *n;
    struct slab *slab;
    struct slab *t;
    struct list_head discard;
    struct list_head promote[SHRINK_PROMOTE_MAX];
    unsigned long flags;
    int ret = 0;

    for_each_kmem_cache_node(s, node, n) {
        INIT_LIST_HEAD(&discard);
        for (i = 0; i < SHRINK_PROMOTE_MAX; i++)
            INIT_LIST_HEAD(promote + i);

        if (n->barn) {
            barn_shrink(s, n->barn) {
                LIST_HEAD(empty_list);
                LIST_HEAD(full_list);
                struct slab_sheaf *sheaf, *sheaf2;
                unsigned long flags;

                spin_lock_irqsave(&barn->lock, flags);

                list_splice_init(&barn->sheaves_full, &full_list);
                barn->nr_full = 0;
                list_splice_init(&barn->sheaves_empty, &empty_list);
                barn->nr_empty = 0;

                spin_unlock_irqrestore(&barn->lock, flags);

                list_for_each_entry_safe(sheaf, sheaf2, &full_list, barn_list) {
                    sheaf_flush_unused(s, sheaf);
                        --->
                    free_empty_sheaf(s, sheaf) {
                        kfree(sheaf);
                    }
                }

                list_for_each_entry_safe(sheaf, sheaf2, &empty_list, barn_list)
                    free_empty_sheaf(s, sheaf);
            }
        }

        spin_lock_irqsave(&n->list_lock, flags);

        /* Build lists of slabs to discard or promote.
         *
         * Note that concurrent frees may occur while we hold the
         * list_lock. slab->inuse here is the upper limit. */
        list_for_each_entry_safe(slab, t, &n->partial, slab_list) {
            int free = slab->objects - slab->inuse;

            /* Do not reread slab->inuse */
            barrier();

            /* We do not keep full slabs on the list */
            BUG_ON(free <= 0);

            if (free == slab->objects) {
                list_move(&slab->slab_list, &discard);
                slab_clear_node_partial(slab);
                n->nr_partial--;
                dec_slabs_node(s, node, slab->objects);
            } else if (free <= SHRINK_PROMOTE_MAX)
                list_move(&slab->slab_list, promote + free - 1);
        }

        /* Promote the slabs filled up most to the head of the
         * partial list. */
        for (i = SHRINK_PROMOTE_MAX - 1; i >= 0; i--)
            list_splice(promote + i, &n->partial);

        spin_unlock_irqrestore(&n->list_lock, flags);

        /* Release empty slabs */
        list_for_each_entry_safe(slab, t, &discard, slab_list)
            free_slab(s, slab);

        if (node_nr_slabs(n))
            ret = 1;
    }

    return ret;
}
```

### flush_all

```c
static void flush_all(struct kmem_cache *s)
{
    cpus_read_lock();
    flush_all_cpus_locked(s) {
        struct slub_flush_work *sfw;
        unsigned int cpu;

        lockdep_assert_cpus_held();
        mutex_lock(&flush_lock);

        for_each_online_cpu(cpu) {
            sfw = &per_cpu(slub_flush, cpu);
            if (!has_cpu_slab(cpu, s) && !has_pcs_used(cpu, s)) {
                sfw->skip = true;
                continue;
            }
            INIT_WORK(&sfw->work, flush_cpu_slab);
            sfw->skip = false;
            sfw->s = s;
            queue_work_on(cpu, flushwq, &sfw->work);
        }

        for_each_online_cpu(cpu) {
            sfw = &per_cpu(slub_flush, cpu);
            if (sfw->skip)
                continue;
            flush_work(&sfw->work);
        }

        mutex_unlock(&flush_lock);
    }
    cpus_read_unlock();
}

static void flush_cpu_slab(struct work_struct *w)
{
    struct kmem_cache *s;
    struct slub_flush_work *sfw;

    sfw = container_of(w, struct slub_flush_work, work);

    s = sfw->s;

    if (s->cpu_sheaves) {
        pcs_flush_all(s) {
            struct slub_percpu_sheaves *pcs;
            struct slab_sheaf *spare, *rcu_free;

            local_lock(&s->cpu_sheaves->lock);
            pcs = this_cpu_ptr(s->cpu_sheaves);

            spare = pcs->spare;
            pcs->spare = NULL;

            rcu_free = pcs->rcu_free;
            pcs->rcu_free = NULL;

            local_unlock(&s->cpu_sheaves->lock);

            if (spare) {
                sheaf_flush_unused(s, spare);
                free_empty_sheaf(s, spare);
            }

            if (rcu_free)
                call_rcu(&rcu_free->rcu_head, rcu_free_sheaf_nobarn);

            sheaf_flush_main(s) {
                struct slub_percpu_sheaves *pcs;
                unsigned int batch, remaining;
                void *objects[PCS_BATCH_MAX];
                struct slab_sheaf *sheaf;
                bool ret = false;

            next_batch:
                if (!local_trylock(&s->cpu_sheaves->lock))
                    return ret;

                pcs = this_cpu_ptr(s->cpu_sheaves);
                sheaf = pcs->main;

                batch = min(PCS_BATCH_MAX, sheaf->size);

                sheaf->size -= batch;
                memcpy(objects, sheaf->objects + sheaf->size, batch * sizeof(void *));

                remaining = sheaf->size;

                local_unlock(&s->cpu_sheaves->lock);

                __kmem_cache_free_bulk(s, batch, &objects[0]);
                    --->

                stat_add(s, SHEAF_FLUSH, batch);

                ret = true;

                if (remaining)
                    goto next_batch;

                return ret;
            }
        }
    }

    flush_this_cpu_slab(s) {
        struct kmem_cache_cpu *c = this_cpu_ptr(s->cpu_slab);

        if (c->slab) {
            flush_slab(s, c) {
                unsigned long flags;
                struct slab *slab;
                void *freelist;

                local_lock_irqsave(&s->cpu_slab->lock, flags);

                slab = c->slab;
                freelist = c->freelist;

                c->slab = NULL;
                c->freelist = NULL;
                c->tid = next_tid(c->tid);

                local_unlock_irqrestore(&s->cpu_slab->lock, flags);

                if (slab) {
                    deactivate_slab(s, slab, freelist);
                        --->
                    stat(s, CPUSLAB_FLUSH);
                }
            }
        }

        put_partials(s);
    }
}

static void rcu_free_sheaf_nobarn(struct rcu_head *head)
{
    struct slab_sheaf *sheaf;
    struct kmem_cache *s;

    sheaf = container_of(head, struct slab_sheaf, rcu_head);
    s = sheaf->cache;

    __rcu_free_sheaf_prepare(s, sheaf) {
        bool init = slab_want_init_on_free(s);
        void **p = &sheaf->objects[0];
        unsigned int i = 0;
        bool pfmemalloc = false;

        while (i < sheaf->size) {
            struct slab *slab = virt_to_slab(p[i]);

            memcg_slab_free_hook(s, slab, p + i, 1);
            alloc_tagging_slab_free_hook(s, slab, p + i, 1);

            if (unlikely(!slab_free_hook(s, p[i], init, true))) {
                p[i] = p[--sheaf->size];
                continue;
            }

            if (slab_test_pfmemalloc(slab))
                pfmemalloc = true;

            i++;
        }

        return pfmemalloc;
    }

    sheaf_flush_unused(s, sheaf);

    free_empty_sheaf(s, sheaf);
}
```

## slub-fs

* /proc/slabinfo            Stats & visibility
* /sys/kernel/slab/         Main SLUB control & tuning
* /sys/kernel/debug/slab/   debugfs

```sh
/sys/kernel/slab/mnt_cache
├── aliases                    # Number of other caches sharing this slab
├── align                      # Object alignment in bytes
├── alloc_calls                # Total allocation calls (debug)
├── cache_dma                  # Cache uses DMA-capable memory
├── cgroup                     # Slab accounted to memory cgroups
├── cpu_partial                # Max partial slabs kept per CPU (tunable)
├── cpu_slabs                  # Slabs currently on per-CPU freelists
├── ctor                       # Objects have constructor function
├── destroy_by_rcu             # Objects freed via RCU
├── free_calls                 # Total free calls (debug)
├── hwcache_align              # Hardware cacheline alignment enabled
├── min_partial                # Min partial slabs kept globally (tunable)
├── objects                    # Currently allocated objects
├── object_size                # Size of one object (bytes)
├── objects_partial            # Objects in partial slabs
├── objs_per_slab              # Objects per slab
├── order                      # Page order of slab allocation
├── partial                    # Number of partial slabs
├── poison                     # Poison freed objects (debug)
├── reclaim_account            # Slab reclaim is accounted
├── red_zone                   # Red-zone overflow checking (debug)
├── remote_node_defrag_ratio   # NUMA remote defrag aggressiveness
├── sanity_checks              # Enable slab consistency checks
├── shrink                     # Force slab cache shrink
├── slabs                      # Total slabs allocated
├── slabs_cpu_partial          # Partial slabs on per-CPU lists
├── slab_size                  # Size of one slab (bytes)
├── store_user                 # Store alloc/free stack traces
├── total_objects              # Total objects in all slabs
├── trace                      # Enable slab allocation tracing
├── usersize                   # Usable object size for users
└── validate                   # Validate all slabs now

```

# kmalloc

* [bin的技术小屋](https://mp.weixin.qq.com/s/atHXeXxx0L63w99RW7bMHg)

```c
/* kmalloc is the normal method of allocating memory
 * for objects smaller than page size in the kernel. */
static void *kmalloc(size_t size, gfp_t flags) {
    if (__builtin_constant_p(size)) {
        if (size > KMALLOC_MAX_CACHE_SIZE)
            return kmalloc_large(size, flags);

    #ifndef CONFIG_SLOB
        if (!(flags & GFP_DMA)) {
            unsigned int index = kmalloc_index(size);
            if (!index)
                return ZERO_SIZE_PTR;
            return kmem_cache_alloc_trace(kmalloc_caches[index], flags, size);
        }
    #endif
    }

    return __kmalloc(size, flags) {
        struct kmem_cache *s;
        void *ret;

        if (unlikely(size > KMALLOC_MAX_CACHE_SIZE))
            return kmalloc_large(size, flags) {
                unsigned int order = get_order(size);
                return kmalloc_order_trace(size, flags, order) {
                    kmalloc_order(size, flags, order) {
                        void *ret;
                        struct page *page;

                        flags |= __GFP_COMP;
                        page = alloc_pages(flags, order);
                        ret = page ? page_address(page) : NULL;
                        kmemleak_alloc(ret, size, 1, flags);
                        kasan_kmalloc_large(ret, size, flags);
                        return ret;
                    }
                }
            }

        s = kmalloc_slab(size, flags) {
            unsigned int index;

            if (size <= 192) {
                if (!size)
                return ZERO_SIZE_PTR;

                index = size_index[size_index_elem(size)] {
                    return (bytes - 1) / 8;
                }
            } else {
                if (unlikely(size > KMALLOC_MAX_CACHE_SIZE)) {
                    WARN_ON(1);
                    return NULL;
                }
                index = fls(size - 1);
            }

            #ifdef CONFIG_ZONE_DMA
                if (unlikely((flags & GFP_DMA)))
                    return kmalloc_dma_caches[index];
            #endif
            return kmalloc_caches[index];
        }
        if (unlikely(ZERO_OR_NULL_PTR(s)))
            return s;

        ret = slab_alloc(s, flags, _RET_IP_);

        return ret;
    }
}
```

## kmalloc_caches

```c
/* mm/slab_common.c */
struct kmem_cache *kmalloc_caches[KMALLOC_SHIFT_HIGH + 1];

void kmem_cache_init(void)
{
    setup_kmalloc_cache_index_table();
    create_kmalloc_caches(0) {
        for (i = KMALLOC_SHIFT_LOW; i <= KMALLOC_SHIFT_HIGH; i++) {
            if (!kmalloc_caches[i])
                new_kmalloc_cache(i, flags);

            if (KMALLOC_MIN_SIZE <= 32 && !kmalloc_caches[1] && i == 6) {
                new_kmalloc_cache(1, flags);
            }
            if (KMALLOC_MIN_SIZE <= 64 && !kmalloc_caches[2] && i == 7) {
                new_kmalloc_cache(2, flags) {
                    kmalloc_caches[idx] = create_kmalloc_cache(kmalloc_info[idx].name,
                        kmalloc_info[idx].size, flags, 0,
                        kmalloc_info[idx].size
                    ) {

                    }
                }
            }
        }

        /* Kmalloc array is now usable */
        slab_state = UP;

        for (i = 0; i <= KMALLOC_SHIFT_HIGH; i++) {
            struct kmem_cache *s = kmalloc_caches[i];

            if (s) {
                unsigned int size = kmalloc_size(i);
                kmalloc_dma_caches[i] = create_kmalloc_cache(n,
                    size, SLAB_CACHE_DMA | flags, 0, 0) {

                    struct kmem_cache *s = kmem_cache_zalloc(kmem_cache, GFP_NOWAIT);

                    create_boot_cache(s, name, size, flags, useroffset, usersize);
                    list_add(&s->list, &slab_caches);
                    memcg_link_cache(s);
                    s->refcount = 1;
                    return s;
                }
            }
        }
    }
}

const struct kmalloc_info_struct kmalloc_info[] __initconst = {
    {NULL,                      0},    {"kmalloc-96",             96},
    {"kmalloc-192",           192},    {"kmalloc-8",               8},
    {"kmalloc-16",             16},    {"kmalloc-32",             32},
    {"kmalloc-64",             64},    {"kmalloc-128",           128},
    {"kmalloc-256",           256},    {"kmalloc-512",           512},
    {"kmalloc-1024",         1024},    {"kmalloc-2048",         2048},
    {"kmalloc-4096",         4096},    {"kmalloc-8192",         8192},
    {"kmalloc-16384",       16384},    {"kmalloc-32768",       32768},
    {"kmalloc-65536",       65536},    {"kmalloc-131072",     131072},
    {"kmalloc-262144",     262144},    {"kmalloc-524288",     524288},
    {"kmalloc-1048576",   1048576},    {"kmalloc-2097152",   2097152},
    {"kmalloc-4194304",   4194304},    {"kmalloc-8388608",   8388608},
    {"kmalloc-16777216", 16777216},    {"kmalloc-33554432", 33554432},
    {"kmalloc-67108864", 67108864}
};

/* Conversion table for small slabs sizes / 8 to the index in the
 * kmalloc array. This is necessary for slabs < 192 since we have non power
 * of two cache sizes there. The size of larger slabs can be determined using
 * fls. */
u8 size_index[24] = {
  3,  /* 8 */
  4,  /* 16 */
  5,  /* 24 */
  5,  /* 32 */
  6,  /* 40 */
  6,  /* 48 */
  6,  /* 56 */
  6,  /* 64 */
  1,  /* 72 */
  1,  /* 80 */
  1,  /* 88 */
  1,  /* 96 */
  7,  /* 104 */
  7,  /* 112 */
  7,  /* 120 */
  7,  /* 128 */
  2,  /* 136 */
  2,  /* 144 */
  2,  /* 152 */
  2,  /* 160 */
  2,  /* 168 */
  2,  /* 176 */
  2,  /* 184 */
  2   /* 192 */
};
```

# vmalloc

```c
static struct kmem_cache *vmap_area_cachep;

/* This linked list is used in pair with free_vmap_area_root.
 * It gives O(1) access to prev/next to perform fast coalescing. */
static LIST_HEAD(free_vmap_area_list);

/* This augment red-black tree represents the free vmap space.
 * All vmap_area objects in this tree are sorted by va->va_start
 * address. */
static struct rb_root free_vmap_area_root = RB_ROOT;

struct vm_struct {
    struct vm_struct  *next;
    void              *addr;
    unsigned long     size;
    unsigned long     flags;
    struct page       **pages;
    unsigned int      nr_pages;
    phys_addr_t       phys_addr;
    const void        *caller;
};

struct vmap_area {
    unsigned long va_start;
    unsigned long va_end;

    struct rb_node rb_node; /* address sorted rbtree */
    struct list_head list; /* address sorted list */

    union {
        unsigned long subtree_max_size; /* in "free" tree free_vmap_area_root */
        struct vm_struct *vm;           /* in "busy" tree vmap_area_root */
    };
};
```

```c
/* allocate virtually contiguous memory */
vmalloc(size) {
    __vmalloc_node_range(size, align, VMALLOC_START, VMALLOC_END, gfp_mask, PAGE_KERNEL, 0, node, caller) {
        struct vm_struct *area = __get_vm_area_node() {
            struct vmap_area *va;
            struct vm_struct *area;
            area = kzalloc_node(sizeof(*area));
            /* Allocate a region of KVA */
            va = alloc_vmap_area(size) {
                va = kmem_cache_alloc_node(vmap_area_cachep);
                addr = __alloc_vmap_area(
                    &free_vmap_area_root/*root*/, &free_vmap_area_list/*head*/,
                    size, align, vstart, vend) {

                    va = find_vmap_lowest_match();
                    adjust_va_to_fit_type();
                        kmem_cache_alloc(vmap_area_cachep, GFP_NOWAIT);
                }
                va->va_start = addr;
                va->va_end = addr + size;
                va->vm = NULL;
                insert_vmap_area(va, &vmap_area_root, &vmap_area_list);
            }
            setup_vmalloc_vm(area, va, flags, caller);
        }

        ret = __vmalloc_area_node(area, gfp_mask, prot, shift, node);
        return area->addr;
    }
}

/* Allocate physical pages and map them into vmalloc space. */
__vmalloc_area_node(area, gfp_mask, prot, shift, node) {
    vm_area_alloc_pages() {
        alloc_pages();
    }

    vmap_pages_range(addr, addr + size, prot, area->pages, page_shift) {
        vmap_range_noflush() {
            pgd = pgd_offset_k(addr);
            vmap_p4d_range() {
                p4d = p4d_alloc_track(&init_mm, pgd, addr, mask);
                vmap_pud_range() {
                    pud = pud_alloc_track(&init_mm, p4d, addr, mask);
                    vmap_pmd_range() {
                        pmd = pmd_alloc_track(&init_mm, pud, addr, mask);
                        vmap_pte_range() {
                            pte = pte_alloc_kernel_track(pmd, addr, mask);
                            set_pte_at(&init_mm, addr, pte, pfn_pte(pfn, prot)) {
                                set_ptes(mm, addr, ptep, pte, 1) {
                                    for (;;) {
                                        __set_pte_at(mm, addr, ptep, pte) {
                                            if (pte_present(pte) && pte_user_exec(pte) && !pte_special(pte)) {
                                                __sync_icache_dcache(pte) {
                                                    struct folio *folio = page_folio(pte_page(pte));
                                                    if (!test_bit(PG_dcache_clean, &folio->flags)) {
                                                        sync_icache_aliases(
                                                            (unsigned long)folio_address(folio),
                                                            (unsigned long)folio_address(folio) + folio_size(folio)) {

                                                            if (icache_is_aliasing()) {
                                                                dcache_clean_pou(start, end);
                                                                icache_inval_all_pou();
                                                            } else {
                                                                caches_clean_inval_pou(start, end);
                                                            }
                                                        }
                                                        set_bit(PG_dcache_clean, &folio->flags);
                                                    }
                                                }
                                            }

                                            set_pte(ptep, pte) {
                                                WRITE_ONCE(*ptep, pte);
                                                if (pte_valid_not_user(pte)) {
                                                    dsb(ishst);
                                                    isb();
                                                }
                                            }
                                        }
                                        if (--nr == 0)
                                            break;
                                        ptep++;
                                        addr += PAGE_SIZE;
                                        pte_val(pte) += PAGE_SIZE;
                                    }
                                }
                            }
                        }
                    }
                }
            }
        }
    }
}
```

# page_reclaim

![](../images/kernel/mem-page_reclaim.svg)

---

![](../images/kernel/mem-page_reclaim-lruvec.png)

* [Overview of Memory Reclaim in the Current Upstream Kernel.pdf](https://lpc.events/event/11/contributions/896/attachments/793/1493/slides-r2.pdf)

Q: how workingset works?

1. directly free the unmodified file pages
2. writeback the modified file pages to storage device
3. writeback annonymous pages to swapping area

**Memory candidate for page reclaim:**

| Type | Location | Reclaim Method | Key Challenge |
| :-: |:-: |:-: |:-: |
| Anonymous | LRU anon lists | Swap to disk | I/O overhead, pinned pages |
| File-Backed | LRU file lists | Drop, writeback, unmap | Dirty page I/O |
| Slab Cache | Shrinker-managed | Subsystem-specific free | Partial page reclaim |
| Unevictable | Unevictable LRU | Skipped (special cases) | Requires unlocking |
| Huge Pages | Compound pages | Split or free to pool | Splitting overhead |
| Free Pages | Buddy free lists | Compaction | Fragmentation |
| Movable Non-LRU | Subsystem-managed | Custom migration | Subsystem cooperation |

1. **File Page Cache**: This is memory used to cache files and block device data. When the system is under memory pressure, the kernel can reclaim page cache pages, as they can be re-read from disk if needed.
2. **Anonymous Pages**: These are pages used by processes that do not map to any file (e.g., heap and stack memory). If these pages are not actively used, they can be swapped out to disk.
3. **Swap Cache**: These are pages that have been swapped out to disk but are still cached in memory. If memory is needed, these pages can be reclaimed, as they can be retrieved from the swap area on disk.
4. **Slab Cache**: Memory allocated for kernel objects and data structures (e.g., **inodes**, **dentries**, or **buffers**). While this memory is often actively used, some of it can be reclaimed if it becomes unused or if memory pressure is high.
5. **Movable Non-LRU Pages**: Pages not on LRU lists but marked as movable by subsystems (e.g., device drivers, balloon drivers). Managed by custom migration callbacks (e.g., isolate_movable_page).
* Reclaim Process: Isolated via `isolate_movable_page` if the subsystem allows it (e.g., in isolate_migratepages_block).

```sh
/proc/sys/vm/               # Virtual memory kernel parameters for page reclaim
├── dirty_background_bytes  # Bytes of dirty memory before background reclaim/writeout (0 = use ratio)
├── dirty_background_ratio  # % of memory for dirty pages before background reclaim/writeout
├── dirty_bytes             # Bytes of dirty memory before foreground reclaim/writeout (0 = use ratio)
├── dirty_ratio             # % of memory for dirty pages before foreground reclaim/writeout
├── dirty_expire_centisecs  # Time dirty pages can stay in memory before reclaim (centisecs)
├── dirty_writeback_centisecs # Interval for periodic dirty page writeback (centisecs; 0 = disable)
├── drop_caches         # Drop caches to force reclaim (write-only: 1 = pagecache, 2 = slab, 3 = both)
├── min_free_kbytes     # Minimum free memory to maintain, triggering reclaim when low (kB)
├── overcommit_kbytes   # Fixed memory overcommit limit for reclaim calculations (kB)
├── overcommit_memory   # Memory overcommit policy (0 = heuristic, 1 = always, 2 = never)
├── overcommit_ratio    # % of physical RAM for overcommit calculation when overcommit_memory=0
├── swappiness          # Tendency to reclaim via swap (0-100; higher = more swapping)
├── vfs_cache_pressure  # Tendency to reclaim inode/dentry cache (0-100; higher = more aggressive)
├── watermark_boost_factor  # Boost memory watermarks to trigger earlier reclaim (0-10000; 0 = disable)
├── watermark_scale_factor  # Scaling factor for memory watermarks (1-1000; affects reclaim frequency)
├── zone_reclaim_mode       # NUMA zone reclaim behavior (0 = off, 1 = reclaim, 2 = writeout, 3 = swap)
├── panic_on_oom            # Panic on out-of-memory (0 = OOM killer, 1 = panic, 2 = panic if not system-wide)
├── oom_kill_allocating_task    # Prefer killing allocating task on OOM (0 = heuristic, 1 = kill requester)
├── oom_dump_tasks              # Dump task info on OOM killer invocation (0 or 1)
└── memory_failure_early_kill   # Kill processes on memory failure during reclaim (0 or 1)

/proc/          # Virtual filesystem with reclaim-related info
├── meminfo     # Memory stats (free, available, swap usage, etc.; reflects reclaim effects)
├── vmstat      # VM statistics (page faults, reclaim counters like pgscan_kswapd, pgsteal)
├── swaps       # Swap space details (total, used, priority per swap device)
├── zoneinfo    # Per-memory-zone stats (free pages, watermarks, scanned pages for reclaim)
├── buddyinfo   # Buddy allocator stats (free blocks by order; indicates reclaim success)
└── <pid>/              # Process-specific directories
    ├── oom_score       # OOM killer score for process (badness heuristic for reclaim)
    ├── oom_score_adj   # Adjust OOM score (-1000 to 1000; tunes process reclaim priority)
    └── smaps           # Memory mappings with reclaim-relevant stats (e.g., RSS, Dirty pages)

/sys/devices/system/node/   # NUMA node-specific reclaim info
├── node[0-N]/              # Per-NUMA node directories
│   ├── vmstat              # Node-specific VM stats (e.g., numa_pages_migrated, pgscan_direct)
│   └── meminfo             # Node-specific memory stats (free, used, swap for reclaim monitoring)
```

## lru

* [linux内存回收之 File page的 lru list算法原理](https://zhuanlan.zhihu.com/p/421298579)
* [[PATCH 0/6 v4] sched/mm: LRU drain flush on nohz_full](https://lore.kernel.org/all/20250703140717.25703-1-frederic@kernel.org/)

```c
enum pgdat_flags {
    /* reclaim scanning has recently found many dirty file pages at the tail of the LRU. */
    PGDAT_DIRTY,

    /* reclaim scanning has recently found many pages under writeback */
    PGDAT_WRITEBACK,

    PGDAT_RECLAIM_LOCKED, /* prevents concurrent reclaim */
};

enum zone_flags {
    ZONE_BOOSTED_WATERMARK, /* zone recently boosted watermarks.
                     * Cleared when kswapd is woken. */
    ZONE_RECLAIM_ACTIVE, /* kswapd may be scanning the zone. */
    ZONE_BELOW_HIGH,    /* zone is below high watermark. */
};

typedef struct pglist_data {
    struct lruvec           lruvec;
    struct task_struct*     kswapd;
};

struct lruvec {
    struct list_head        lists[NR_LRU_LISTS];
    spinlock_t              lru_lock;

    unsigned long           anon_cost;
    unsigned long           file_cost;

    atomic_long_t           nonresident_age;

    unsigned long           refaults[ANON_AND_FILE];

    unsigned long           flags;

    struct pglist_data      *pgdat;
};

#define LRU_BASE        0
#define LRU_ACTIVE      1
#define LRU_FILE        2

enum lru_list {
    LRU_INACTIVE_ANON   = LRU_BASE,
    LRU_ACTIVE_ANON     = LRU_BASE + LRU_ACTIVE,
    LRU_INACTIVE_FILE   = LRU_BASE + LRU_FILE,
    LRU_ACTIVE_FILE     = LRU_BASE + LRU_FILE + LRU_ACTIVE,
    LRU_UNEVICTABLE,
    NR_LRU_LISTS
};

enum pageflags {
    PG_locked,      /* Page is locked. Don't touch. */
    PG_referenced,  /* accessed recently */
    PG_dirty,
    PG_lru,
    PG_active,      /* active page */
    PG_swapcache,   /* Swap page: swp_entry_t in private */
    PG_swapbacked,  /* Page is backed by RAM/swap */
    PG_unevictable, /* Page is "unevictable"  */
    PG_workingset,
}
```

```c
/* Mark a page as having seen activity.
 *
 * inactive,unreferenced     -> inactive,referenced
 * inactive,referenced       -> active,unreferenced
 * active,unreferenced       -> active,referenced
 *
 * When a newly allocated page is not yet visible, so safe for non-atomic ops,
 * __SetPageReferenced(page) may be substituted for mark_page_accessed(page). */

void folio_mark_accessed(struct folio *folio)
{
    if (lru_gen_enabled()) {
        folio_inc_refs(folio);
        return;
    }

    if (!folio_test_referenced(folio)) {
        folio_set_referenced(folio);
    } else if (folio_test_unevictable(folio)) {
        /* Unevictable pages are on the "LRU_UNEVICTABLE" list. But,
        * this list is never rotated or maintained, so marking an
        * unevictable page accessed has no effect. */
    } else if (!folio_test_active(folio)) {
        /* If the folio is on the LRU, queue it for activation via
        * cpu_fbatches.activate. Otherwise, assume the folio is in a
        * folio_batch, mark it active and it'll be moved to the active
        * LRU on the next drain. */
        if (folio_test_lru(folio)) {
            folio_activate(folio);
        } else {
            __lru_cache_activate_folio(folio) {
                fbatch = this_cpu_ptr(&cpu_fbatches.lru_add);
                for (i = folio_batch_count(fbatch) - 1; i >= 0; i--) {
                    struct folio *batch_folio = fbatch->folios[i];
                    if (batch_folio == folio) {
                        folio_set_active(folio);
                        break;
                    }
                }
            }
        }

        folio_clear_referenced(folio);
        workingset_activation(folio);
    }
    if (folio_test_idle(folio)) {
        folio_clear_idle(folio);
    }
}
```

## MGLRU

## folio_batch

```c
#define PAGEVEC_SIZE        31

struct folio_batch {
    unsigned char nr;
    bool percpu_pvec_drained;
    struct folio *folios[PAGEVEC_SIZE];
};

struct cpu_fbatches {
    local_lock_t lock;
    struct folio_batch lru_add;
    struct folio_batch lru_deactivate_file;
    struct folio_batch lru_deactivate;
    struct folio_batch lru_lazyfree;
    struct folio_batch activate;
};

static DEFINE_PER_CPU(struct cpu_fbatches, cpu_fbatches) = {
    .lock = INIT_LOCAL_LOCK(lock),
};

unsigned folio_batch_add(struct folio_batch *fbatch, struct folio *folio)
{
    fbatch->folios[fbatch->nr++] = folio;
}

unsigned int folio_batch_space(struct folio_batch *fbatch)
{
    return PAGEVEC_SIZE - fbatch->nr;
}
```

```c
void lru_add_drain(void)
{
    local_lock(&cpu_fbatches.lock);
    lru_add_drain_cpu(smp_processor_id()) {
        struct cpu_fbatches *fbatches = &per_cpu(cpu_fbatches, cpu);
        struct folio_batch *fbatch = &fbatches->lru_add;

        if (folio_batch_count(fbatch)) {
            folio_batch_move_lru(fbatch, lru_add_fn/*move_fn*/) {
                int i;
                struct lruvec *lruvec = NULL;
                unsigned long flags = 0;

                for (i = 0; i < folio_batch_count(fbatch); i++) {
                    struct folio *folio = fbatch->folios[i];

                    /* block memcg migration while the folio moves between lru */
                    if (move_fn != lru_add_fn && !folio_test_clear_lru(folio))
                        continue;

                    lruvec = folio_lruvec_relock_irqsave(folio, lruvec, &flags);
                    move_fn(lruvec, folio);

                    folio_set_lru(folio);
                }

                if (lruvec)
                    unlock_page_lruvec_irqrestore(lruvec, flags);
                folios_put(fbatch->folios, folio_batch_count(fbatch));
                folio_batch_reinit(fbatch);
            }
        }

        fbatch = &per_cpu(lru_rotate.fbatch, cpu);
        /* Disabling interrupts below acts as a compiler barrier. */
        if (data_race(folio_batch_count(fbatch))) {
            local_lock_irqsave(&lru_rotate.lock, flags);
            folio_batch_move_lru(fbatch, lru_move_tail_fn);
            local_unlock_irqrestore(&lru_rotate.lock, flags);
        }

        fbatch = &fbatches->lru_deactivate_file;
        if (folio_batch_count(fbatch))
            folio_batch_move_lru(fbatch, lru_deactivate_file_fn);

        fbatch = &fbatches->lru_deactivate;
        if (folio_batch_count(fbatch))
            folio_batch_move_lru(fbatch, lru_deactivate_fn);

        fbatch = &fbatches->lru_lazyfree;
        if (folio_batch_count(fbatch))
            folio_batch_move_lru(fbatch, lru_lazyfree_fn);

        folio_activate_drain(cpu) {
            struct folio_batch *fbatch = &per_cpu(cpu_fbatches.activate, cpu);

            if (folio_batch_count(fbatch))
                folio_batch_move_lru(fbatch, folio_activate_fn);
        }
    }
    local_unlock(&cpu_fbatches.lock);
    mlock_drain_local();
}
```

## workingset

* [Linux memory workingset 内存工作集](https://mp.weixin.qq.com/s/JB5OUvjFygHSj-SmEysDpg)

**Shadow Entries:**
* When a page is evicted (e.g., file page dropped or anon page swapped), a shadow entry is stored in the page cache’s radix tree (struct address_space->i_pages) or swap cache.
* This entry records the eviction timestamp (an lruvec counter) rather than the full struct page.

### workingset_refault

```c
void workingset_refault(struct folio *folio, void *shadow)
{
    bool file = folio_is_file_lru(folio);
    struct pglist_data *pgdat;
    struct mem_cgroup *memcg;
    struct lruvec *lruvec;
    bool workingset;
    long nr;

    nr = folio_nr_pages(folio);
    memcg = folio_memcg(folio);
    pgdat = folio_pgdat(folio);
    lruvec = mem_cgroup_lruvec(memcg, pgdat);

    mod_lruvec_state(lruvec, WORKINGSET_REFAULT_BASE + file, nr);

    if (!workingset_test_recent(shadow, file, &workingset))
        return;

    folio_set_active(folio);
    workingset_age_nonresident(lruvec, nr) {
        do {
            atomic_long_add(nr_pages, &lruvec->nonresident_age);
        } while ((lruvec = parent_lruvec(lruvec)));
    }
    mod_lruvec_state(lruvec, WORKINGSET_ACTIVATE_BASE + file, nr);

    /* Folio was active prior to eviction */
    if (workingset) {
        folio_set_workingset(folio);
        lru_note_cost_refault(folio) {
            lru_note_cost(
                folio_lruvec(folio),
                folio_is_file_lru(folio),
                folio_nr_pages(folio),
                0
            );
        }
        mod_lruvec_state(lruvec, WORKINGSET_RESTORE_BASE + file, nr) {
            /* Update node */
            mod_node_page_state(lruvec_pgdat(lruvec), idx, val) {
                unsigned long flags;

                local_irq_save(flags);
                __mod_node_page_state(pgdat, item, delta) {
                    struct per_cpu_nodestat __percpu *pcp = pgdat->per_cpu_nodestats;
                    s8 __percpu *p = pcp->vm_node_stat_diff + item;
                    long x;
                    long t;

                    if (vmstat_item_in_bytes(item)) {
                        /*
                        * Only cgroups use subpage accounting right now; at
                        * the global level, these items still change in
                        * multiples of whole pages. Store them as pages
                        * internally to keep the per-cpu counters compact.
                        */
                        VM_WARN_ON_ONCE(delta & (PAGE_SIZE - 1));
                        delta >>= PAGE_SHIFT;
                    }

                    /* See __mod_zone_page_state() */
                    preempt_disable_nested();

                    x = delta + __this_cpu_read(*p);

                    t = __this_cpu_read(pcp->stat_threshold);

                    if (unlikely(abs(x) > t)) {
                        node_page_state_add(x, pgdat, item) {
                            atomic_long_add(x, &pgdat->vm_stat[item]);
	                        atomic_long_add(x, &vm_node_stat[item]);
                        }
                        x = 0;
                    }
                    __this_cpu_write(*p, x);

                    preempt_enable_nested();
                }
                local_irq_restore(flags);
            }

            /* Update memcg and lruvec */
            if (!mem_cgroup_disabled()) {
                mod_memcg_lruvec_state(lruvec, idx, val) {
                    struct pglist_data *pgdat = lruvec_pgdat(lruvec);
                    struct mem_cgroup_per_node *pn;
                    struct mem_cgroup *memcg;
                    int i = memcg_stats_index(idx);
                    int cpu;

                    if (WARN_ONCE(BAD_STAT_IDX(i), "%s: missing stat item %d\n", __func__, idx))
                        return;

                    pn = container_of(lruvec, struct mem_cgroup_per_node, lruvec);
                    memcg = pn->memcg;

                    while (memcg_is_dying(memcg))
                        memcg = parent_mem_cgroup(memcg);

                    pn = memcg->nodeinfo[pgdat->node_id];

                    cpu = get_cpu();

                    /* Update memcg */
                    this_cpu_add(memcg->vmstats_percpu->state[i], val);

                    /* Update lruvec */
                    this_cpu_add(pn->lruvec_stats_percpu->state[i], val);

                    val = memcg_state_val_in_pages(idx, val);
                    memcg_rstat_updated(memcg, val, cpu);
                    trace_mod_memcg_lruvec_state(memcg, idx, val);

                    put_cpu();
                }
            }
        }
    }
}

bool workingset_test_recent(void *shadow, bool file, bool *workingset)
{
    struct mem_cgroup *eviction_memcg;
    struct lruvec *eviction_lruvec;
    unsigned long refault_distance;
    unsigned long workingset_size;
    unsigned long refault;
    int memcgid;
    struct pglist_data *pgdat;
    unsigned long eviction;

    rcu_read_lock();

    unpack_shadow(shadow, &memcgid, &pgdat, &eviction, workingset) {
        unsigned long entry = xa_to_value(shadow);
        int memcgid, nid;
        bool workingset;

        workingset = entry & ((1UL << WORKINGSET_SHIFT) - 1);
        entry >>= WORKINGSET_SHIFT;
        nid = entry & ((1UL << NODES_SHIFT) - 1);
        entry >>= NODES_SHIFT;
        memcgid = entry & ((1UL << MEM_CGROUP_ID_SHIFT) - 1);
        entry >>= MEM_CGROUP_ID_SHIFT;

        *memcgidp = memcgid;
        *pgdat = NODE_DATA(nid);
        *evictionp = entry;
        *workingsetp = workingset;
    }
    eviction <<= bucket_order;

    eviction_memcg = mem_cgroup_from_id(memcgid);
    if (!mem_cgroup_disabled() && (!eviction_memcg || !mem_cgroup_tryget(eviction_memcg))) {
        rcu_read_unlock();
        return false;
    }

    rcu_read_unlock();


    mem_cgroup_flush_stats_ratelimited(eviction_memcg);

    eviction_lruvec = mem_cgroup_lruvec(eviction_memcg, pgdat);
    refault = atomic_long_read(&eviction_lruvec->nonresident_age);
    refault_distance = (refault - eviction) & EVICTION_MASK;

    workingset_size = lruvec_page_state(eviction_lruvec, NR_ACTIVE_FILE);
    if (!file) {
        workingset_size += lruvec_page_state(eviction_lruvec, NR_INACTIVE_FILE);
    }
    if (mem_cgroup_get_nr_swap_pages(eviction_memcg) > 0) {
        workingset_size += lruvec_page_state(eviction_lruvec, NR_ACTIVE_ANON);
        if (file) {
            workingset_size += lruvec_page_state(eviction_lruvec, NR_INACTIVE_ANON);
        }
    }

    mem_cgroup_put(eviction_memcg);
    return refault_distance <= workingset_size;
}
```

### workingset_activation

```c
/* folio_mark_accessed -> workingset_activation */
void workingset_activation(struct folio *folio)
{
    struct mem_cgroup *memcg;

    rcu_read_lock();

    memcg = folio_memcg_rcu(folio);
    if (!mem_cgroup_disabled() && !memcg)
        goto out;

    workingset_age_nonresident(folio_lruvec(folio), folio_nr_pages(folio)) {
        do {
            atomic_long_add(nr_pages, &lruvec->nonresident_age);
        } while ((lruvec = parent_lruvec(lruvec)));
    }

out:
    rcu_read_unlock();
}
```

### workingset_eviction

```c
/* shrink_folio_list -> __remove_mapping -> workingset_eviction */
void *workingset_eviction(struct folio *folio, struct mem_cgroup *target_memcg)
{
    struct pglist_data *pgdat = folio_pgdat(folio);
    unsigned long eviction;
    struct lruvec *lruvec;
    int memcgid;

    lruvec = mem_cgroup_lruvec(target_memcg, pgdat);
    /* XXX: target_memcg can be NULL, go through lruvec */
    memcgid = mem_cgroup_id(lruvec_memcg(lruvec));
    eviction = atomic_long_read(&lruvec->nonresident_age);
    eviction >>= bucket_order;
    workingset_age_nonresident(lruvec, folio_nr_pages(folio)) {
        do {
            atomic_long_add(nr_pages, &lruvec->nonresident_age);
        } while ((lruvec = parent_lruvec(lruvec)));
    }

    return pack_shadow(memcgid, pgdat, eviction, folio_test_workingset(folio)) {
        eviction &= EVICTION_MASK;
        eviction = (eviction << MEM_CGROUP_ID_SHIFT) | memcgid;
        eviction = (eviction << NODES_SHIFT) | pgdat->node_id;
        eviction = (eviction << WORKINGSET_SHIFT) | workingset;

        return xa_mk_value(eviction);
    }
}
```

## perform_reclaim

```c
struct scan_control {
    /* How many pages shrink_list() should reclaim */
    unsigned long nr_to_reclaim;
    /* Allocation order */
    s8 order;
    /* Scan (total_size >> priority) pages at once */
    s8 priority;

    /* Nodemask of nodes allowed by the caller. If NULL, all nodes are scanned. */
    nodemask_t    *nodemask;
    /* The highest zone to isolate folios for reclaim from */
    s8 reclaim_idx;
    /* This context's GFP mask */
    gfp_t gfp_mask;
    /* The memory cgroup that hit its limit and as a result is the
     * primary target of this reclaim invocation. */
    struct mem_cgroup *target_mem_cgroup;

    /* Incremented by the number of inactive pages that were scanned */
    unsigned long nr_scanned;
    /* Number of pages freed so far during a call to shrink_zones() */
    unsigned long nr_reclaimed;

    /* Writepage batching in laptop mode; RECLAIM_WRITE */
    unsigned int may_writepage:1;
    /* Can mapped folios be reclaimed? */
    unsigned int may_unmap:1;
    /* Can folios be swapped as part of reclaim? */
    unsigned int may_swap:1;

    /* Not allow cache_trim_mode to be turned on as part of reclaim?
     * whether the kernel should focus on trimming (reclaiming) cached pages */
    unsigned int no_cache_trim_mode:1;
    /* Has cache_trim_mode failed at least once? */
    unsigned int cache_trim_mode_failed:1;

    /* Proactive reclaim invoked by userspace through memory.reclaim */
    unsigned int proactive:1;

    /* Scan pressure balancing between anon and file LRUs */
    unsigned long    anon_cost;
    unsigned long    file_cost;

#ifdef CONFIG_MEMCG
    /* Swappiness value for proactive reclaim. Always use sc_swappiness()! */
    int *proactive_swappiness;
#endif

    /* Can active folios be deactivated as part of reclaim? */
#define DEACTIVATE_ANON 1
#define DEACTIVATE_FILE 2
    unsigned int may_deactivate:2;
    unsigned int force_deactivate:1;
    unsigned int skipped_deactivate:1;

    /* Cgroup memory below memory.low is protected as long as we
    * don't threaten to OOM. If any cgroup is reclaimed at
    * reduced force or passed over entirely due to its memory.low
    * setting (memcg_low_skipped), and nothing is reclaimed as a
    * result, then go back for one more cycle that reclaims the protected
    * memory (memcg_low_reclaim) to avert OOM. */
    unsigned int memcg_low_reclaim:1;
    unsigned int memcg_low_skipped:1;
    /* Shared cgroup tree walk failed, rescan the whole tree */
    unsigned int memcg_full_walk:1;
    unsigned int hibernation_mode:1;
    /* One of the zones is ready for compaction */
    unsigned int compaction_ready:1;
    /* There is easily reclaimable cold cache in the current node */
    unsigned int cache_trim_mode:1;
    /* The file folios on the current node are dangerously low */
    unsigned int file_is_tiny:1;
    /* Always discard instead of demoting to lower tier memory */
    unsigned int no_demotion:1;

    struct {
        unsigned int dirty;
        unsigned int unqueued_dirty;
        unsigned int congested;
        unsigned int writeback;
        unsigned int immediate;
        unsigned int file_taken;
        unsigned int taken;
    } nr;

    /* for recording the reclaimed slab by now */
    struct reclaim_state reclaim_state;
};
```

```c
__perform_reclaim(gfp_mask, order, ac) {
    unsigned int noreclaim_flag;
    unsigned long progress;

    cond_resched();

    /* We now go into synchronous reclaim */
    cpuset_memory_pressure_bump();
    fs_reclaim_acquire(gfp_mask);
    noreclaim_flag = memalloc_noreclaim_save();

    progress = try_to_free_pages(ac->zonelist, order, gfp_mask, ac->nodemask) {
        unsigned long nr_reclaimed;
        struct scan_control sc = {
            .nr_to_reclaim = SWAP_CLUSTER_MAX,
            .gfp_mask = current_gfp_context(gfp_mask),
            .reclaim_idx = gfp_zone(gfp_mask),
            .order = order,
            .nodemask = nodemask,
            /* #define DEF_PRIORITY 12
             * implies that we will scan 1/4096th of the
             * queues ("queue_length >> 12") during an aging round. */
            .priority = DEF_PRIORITY,
            .may_writepage = !laptop_mode,
            .may_unmap = 1,
            .may_swap = 1,
        };

        /* Do not enter reclaim if fatal signal was delivered while throttled.
         * 1 is returned so that the page allocator does not OOM kill at this point. */
        if (throttle_direct_reclaim(sc.gfp_mask, zonelist, nodemask))
            return 1;

        set_task_reclaim_state(current, &sc.reclaim_state);

        nr_reclaimed = do_try_to_free_pages(zonelist, &sc) {
            int initial_priority = sc->priority;
            pg_data_t *last_pgdat;
            struct zoneref *z;
            struct zone *zone;

        retry:
            if (!cgroup_reclaim(sc))
                __count_zid_vm_events(ALLOCSTALL, sc->reclaim_idx, 1);

            do {
                if (!sc->proactive) {
                    vmpressure_prio(sc->gfp_mask, sc->target_mem_cgroup, sc->priority);
                }
                sc->nr_scanned = 0;

                shrink_zones(zonelist, sc) {
                    struct zoneref *z;
                    struct zone *zone;
                    unsigned long nr_soft_reclaimed;
                    unsigned long nr_soft_scanned;
                    gfp_t orig_mask;
                    pg_data_t *last_pgdat = NULL;
                    pg_data_t *first_pgdat = NULL;

                    /* If the number of buffer_heads in the machine exceeds the maximum
                     * allowed level, force direct reclaim to scan the highmem zone as
                     * highmem pages could be pinning lowmem pages storing buffer_heads */
                    orig_mask = sc->gfp_mask;
                    if (buffer_heads_over_limit) {
                        sc->gfp_mask |= __GFP_HIGHMEM;
                        sc->reclaim_idx = gfp_zone(sc->gfp_mask);
                    }

                    /* iterate zones within a node from high to low zone */
                    for_each_zone_zonelist_nodemask(zone, z, zonelist, sc->reclaim_idx, sc->nodemask) {
                        /* Take care memory controller reclaiming has small influence
                         * to global LRU. */
                        if (!cgroup_reclaim(sc)) {
                            if (!cpuset_zone_allowed(zone, GFP_KERNEL | __GFP_HARDWALL))
                                continue;

                            if (IS_ENABLED(CONFIG_COMPACTION) &&
                                sc->order > PAGE_ALLOC_COSTLY_ORDER &&
                                compaction_ready(zone, sc)) {
                                sc->compaction_ready = true;
                                continue;
                            }

                            if (zone->zone_pgdat == last_pgdat)
                                continue;

                            nr_soft_scanned = 0;
                            nr_soft_reclaimed = memcg1_soft_limit_reclaim(
                                zone->zone_pgdat,
                                sc->order, sc->gfp_mask,
                                &nr_soft_scanned
                            );
                            sc->nr_reclaimed += nr_soft_reclaimed;
                            sc->nr_scanned += nr_soft_scanned;
                        }

                        if (!first_pgdat)
                            first_pgdat = zone->zone_pgdat;

                        /* scan at node level, skip the zone from same node of last_pgdat */
                        if (zone->zone_pgdat == last_pgdat)
                            continue;
                        last_pgdat = zone->zone_pgdat;

                        shrink_node(zone->zone_pgdat, sc)
                            --->
                    }

                    if (first_pgdat)
                        consider_reclaim_throttle(first_pgdat, sc);

                    /* Restore to original mask to avoid the impact on the caller if we
                     * promoted it to __GFP_HIGHMEM. */
                    sc->gfp_mask = orig_mask;
                }

                if (sc->nr_reclaimed >= sc->nr_to_reclaim)
                    break;

                if (sc->compaction_ready)
                    break;

                /* If we're getting trouble reclaiming, start doing
                 * writepage even in laptop mode. */
                if (sc->priority < DEF_PRIORITY - 2)
                    sc->may_writepage = 1;
            } while (--sc->priority >= 0);

            last_pgdat = NULL;
            for_each_zone_zonelist_nodemask(zone, z, zonelist, sc->reclaim_idx, sc->nodemask) {
                if (zone->zone_pgdat == last_pgdat)
                    continue;
                last_pgdat = zone->zone_pgdat;

                snapshot_refaults(sc->target_mem_cgroup, zone->zone_pgdat);

                if (cgroup_reclaim(sc)) {
                    struct lruvec *lruvec;

                    lruvec = mem_cgroup_lruvec(sc->target_mem_cgroup, zone->zone_pgdat);
                    clear_bit(LRUVEC_CGROUP_CONGESTED, &lruvec->flags);
                }
            }

            if (sc->nr_reclaimed)
                return sc->nr_reclaimed;

            /* Aborted reclaim to try compaction? don't OOM, then */
            if (sc->compaction_ready)
                return 1;

            /* We make inactive:active ratio decisions based on the node's
            * composition of memory, but a restrictive reclaim_idx or a
            * memory.low cgroup setting can exempt large amounts of
            * memory from reclaim. Neither of which are very common, so
            * instead of doing costly eligibility calculations of the
            * entire cgroup subtree up front, we assume the estimates are
            * good, and retry with forcible deactivation if that fails. */
            if (sc->skipped_deactivate) {
                sc->priority = initial_priority;
                sc->force_deactivate = 1;
                sc->skipped_deactivate = 0;
                goto retry;
            }

            /* Untapped cgroup reserves?  Don't OOM, retry. */
            if (sc->memcg_low_skipped) {
                sc->priority = initial_priority;
                sc->force_deactivate = 0;
                sc->memcg_low_reclaim = 1;
                sc->memcg_low_skipped = 0;
                goto retry;
            }

            return 0;
        }

        set_task_reclaim_state(current, NULL);

        return nr_reclaimed;
    }

    memalloc_noreclaim_restore(noreclaim_flag);
    fs_reclaim_release(gfp_mask);

    cond_resched();

    return progress;
}
```

## throttle_direct_reclaim

![](../images/kernel/mem-page_reclaim-throttle_direct_reclaim.png)

---

![](../images/kernel/mem-page_reclaim-allow_direct_reclaim.png)

```c
bool throttle_direct_reclaim(gfp_t gfp_mask, struct zonelist *zonelist,
    nodemask_t *nodemask)
{
    struct zoneref *z;
    struct zone *zone;
    pg_data_t *pgdat = NULL;

    if (current->flags & PF_KTHREAD)
        goto out;

    if (fatal_signal_pending(current))
        goto out;

    for_each_zone_zonelist_nodemask(zone, z, zonelist, gfp_zone(gfp_mask), nodemask) {
        if (zone_idx(zone) > ZONE_NORMAL)
            continue;

        /* Throttle based on the first usable node */
        pgdat = zone->zone_pgdat;
        ret = allow_direct_reclaim(pgdat) {
            if (pgdat->kswapd_failures >= MAX_RECLAIM_RETRIES)
                return true;

            for (i = 0; i <= ZONE_NORMAL; i++) {
                zone = &pgdat->node_zones[i];
                if (!managed_zone(zone))
                    continue;

                if (!zone_reclaimable_pages(zone))
                    continue;

                pfmemalloc_reserve += min_wmark_pages(zone);
                free_pages += zone_page_state_snapshot(zone, NR_FREE_PAGES);
            }

            /* If there are no reserves (unexpected config) then do not throttle */
            if (!pfmemalloc_reserve)
                return true;

            wmark_ok = free_pages > pfmemalloc_reserve / 2;

            /* kswapd must be awake if processes are being throttled */
            if (!wmark_ok && waitqueue_active(&pgdat->kswapd_wait)) {
                if (READ_ONCE(pgdat->kswapd_highest_zoneidx) > ZONE_NORMAL)
                    WRITE_ONCE(pgdat->kswapd_highest_zoneidx, ZONE_NORMAL);

                wake_up_interruptible(&pgdat->kswapd_wait);
            }

            return wmark_ok;
        }
        if (ret) {
            goto out;
        }
        break;
    }

    /* If no zone was usable by the allocation flags then do not throttle */
    if (!pgdat)
        goto out;

    /* Account for the throttling */
    count_vm_event(PGSCAN_DIRECT_THROTTLE);

    if (!(gfp_mask & __GFP_FS))
        wait_event_interruptible_timeout(
            pgdat->pfmemalloc_wait, allow_direct_reclaim(pgdat), HZ
        );
    else
        /* Throttle until kswapd wakes the process */
        wait_event_killable(
            zone->zone_pgdat->pfmemalloc_wait, allow_direct_reclaim(pgdat)
        );

    if (fatal_signal_pending(current))
        return true;

out:
    return false;
}
```

## shrink_node

![](../images/kernel/mem-page_reclaim-shrink_node.png)

---

![](../images/kernel/mem-page_reclaim-scan_balance.png)

```c
shrink_node(zone->zone_pgdat, sc) {
    unsigned long nr_reclaimed, nr_scanned, nr_node_reclaimed;
    struct lruvec *target_lruvec;
    bool reclaimable = false;

    if (lru_gen_enabled() && root_reclaim(sc)) {
        lru_gen_shrink_node(pgdat, sc);
        return;
    }

    target_lruvec = mem_cgroup_lruvec(sc->target_mem_cgroup, pgdat) {
        struct mem_cgroup_per_node *mz;
        struct lruvec *lruvec;

        if (mem_cgroup_disabled()) {
            lruvec = &pgdat->__lruvec;
            goto out;
        }

        if (!memcg)
            memcg = root_mem_cgroup;

        mz = memcg->nodeinfo[pgdat->node_id];
        lruvec = &mz->lruvec;
    out:
        /* Since a node can be onlined after the mem_cgroup was created,
         * we have to be prepared to initialize lruvec->pgdat here;
         * and if offlined then reonlined, we need to reinitialize it. */
        if (unlikely(lruvec->pgdat != pgdat))
            lruvec->pgdat = pgdat;
        return lruvec;
    }

again:
    memset(&sc->nr, 0, sizeof(sc->nr));

    nr_reclaimed = sc->nr_reclaimed;
    nr_scanned = sc->nr_scanned;

    prepare_scan_control(pgdat, sc);

    shrink_node_memcgs(pgdat, sc);
          --->

    flush_reclaim_state(sc);

    nr_node_reclaimed = sc->nr_reclaimed - nr_reclaimed;

    /* Record the subtree's reclaim efficiency */
    if (!sc->proactive) {
        vmpressure(sc->gfp_mask, sc->target_mem_cgroup, true,
            sc->nr_scanned - nr_scanned, nr_node_reclaimed);
    }

    if (nr_node_reclaimed)
        reclaimable = true;

    if (current_is_kswapd()) {
        if (sc->nr.writeback && sc->nr.writeback == sc->nr.taken)
            set_bit(PGDAT_WRITEBACK, &pgdat->flags);

        /* Allow kswapd to start writing pages during reclaim.*/
        if (sc->nr.unqueued_dirty == sc->nr.file_taken)
            set_bit(PGDAT_DIRTY, &pgdat->flags);

        /* If kswapd scans pages marked for immediate
         * reclaim and under writeback (nr_immediate), it
         * implies that pages are cycling through the LRU
         * faster than they are written so forcibly stall
         * until some pages complete writeback. */
        if (sc->nr.immediate) {
            reclaim_throttle(pgdat, VMSCAN_THROTTLE_WRITEBACK);
        }
    }

    /* Tag a node/memcg as congested if all the dirty pages were marked
     * for writeback and immediate reclaim (counted in nr.congested).
     *
     * Legacy memcg will stall in page writeback so avoid forcibly
     * stalling in reclaim_throttle(). */
    if (sc->nr.dirty && sc->nr.dirty == sc->nr.congested) {
        if (cgroup_reclaim(sc) && writeback_throttling_sane(sc))
            set_bit(LRUVEC_CGROUP_CONGESTED, &target_lruvec->flags);

        if (current_is_kswapd())
            set_bit(LRUVEC_NODE_CONGESTED, &target_lruvec->flags);
    }

    /* Stall direct reclaim for IO completions if the lruvec is
     * node is congested. Allow kswapd to continue until it
     * starts encountering unqueued dirty pages or cycling through
     * the LRU too quickly. */
    is_congested = (test_bit(LRUVEC_CGROUP_CONGESTED, &target_lruvec->flags)
        || test_bit(LRUVEC_NODE_CONGESTED, &target_lruvec->flags));
    if (!current_is_kswapd() && current_may_throttle()
        && !sc->hibernation_mode && is_congested) {

        reclaim_throttle(pgdat, VMSCAN_THROTTLE_CONGESTED);
    }

    ret = should_continue_reclaim(pgdat, nr_node_reclaimed, sc) {
        unsigned long pages_for_compaction;
        unsigned long inactive_lru_pages;
        int z;

        /* If not in reclaim/compaction mode, stop */
        if (!in_reclaim_compaction(sc))
            return false;

        if (!nr_reclaimed)
            return false;

        /* If compaction would go ahead or the allocation would succeed, stop */
        for (z = 0; z <= sc->reclaim_idx; z++) {
            struct zone *zone = &pgdat->node_zones[z];
            if (!managed_zone(zone))
                continue;

            /* Allocation can already succeed, nothing to do */
            if (zone_watermark_ok(zone, sc->order, min_wmark_pages(zone),
                        sc->reclaim_idx, 0))
                return false;

            if (compaction_suitable(zone, sc->order, sc->reclaim_idx))
                return false;
        }

        /* If we have not reclaimed enough pages for compaction and the
         * inactive lists are large enough, continue reclaiming */
        pages_for_compaction = compact_gap(sc->order);
        inactive_lru_pages = node_page_state(pgdat, NR_INACTIVE_FILE);
        if (can_reclaim_anon_pages(NULL, pgdat->node_id, sc))
            inactive_lru_pages += node_page_state(pgdat, NR_INACTIVE_ANON);

        return inactive_lru_pages > pages_for_compaction;
    }
    if (ret)
        goto again;

    if (reclaimable)
        pgdat->kswapd_failures = 0;
}
```

### shrink_node_memcgs

```c
shrink_node_memcgs(pgdat, sc) {
    struct mem_cgroup *target_memcg = sc->target_mem_cgroup;
    struct mem_cgroup *memcg;

    memcg = mem_cgroup_iter(target_memcg, NULL, NULL);
    do {
        struct lruvec *lruvec = mem_cgroup_lruvec(memcg, pgdat);
        unsigned long reclaimed;
        unsigned long scanned;

        mem_cgroup_calculate_protection(target_memcg, memcg) {
            memcg->memory.emin = ;
            memcg->memory.elow = ;
        }

        if (mem_cgroup_below_min(target_memcg, memcg)) {
            /* Hard protection.
             * If there is no reclaimable memory, OOM. */
            continue;
        } else if (mem_cgroup_below_low(target_memcg, memcg)) {
            /* Soft protection.
             * Respect the protection only as long as
             * there is an unprotected supply
             * of reclaimable memory from other cgroups. */
            if (!sc->memcg_low_reclaim) {
                sc->memcg_low_skipped = 1;
                continue;
            }
            memcg_memory_event(memcg, MEMCG_LOW);
        }
        /* else: cgroup mem usage is above the low, do reclaim */

        reclaimed = sc->nr_reclaimed;
        scanned = sc->nr_scanned;

        shrink_lruvec(lruvec, sc) {
            unsigned long nr[NR_LRU_LISTS];
            unsigned long targets[NR_LRU_LISTS];
            unsigned long nr_to_scan;
            enum lru_list lru;
            unsigned long nr_reclaimed = 0;
            unsigned long nr_to_reclaim = sc->nr_to_reclaim;
            bool proportional_reclaim;
            struct blk_plug plug;

            if (lru_gen_enabled() && !root_reclaim(sc)) {
                lru_gen_shrink_lruvec(lruvec, sc);
                return;
            }

            get_scan_count(lruvec, sc, nr);
                --->

            /* Record the original scan target for proportional adjustments later */
            memcpy(targets, nr, sizeof(nr));

            proportional_reclaim = (!cgroup_reclaim(sc) && !current_is_kswapd() &&
                        sc->priority == DEF_PRIORITY);

            blk_start_plug(&plug);
            while (nr[LRU_INACTIVE_ANON] || nr[LRU_ACTIVE_FILE] || nr[LRU_INACTIVE_FILE]) {
                unsigned long nr_anon, nr_file, percentage;
                unsigned long nr_scanned;

                for_each_evictable_lru(lru) {
                    if (nr[lru]) {
                        nr_to_scan = min(nr[lru], SWAP_CLUSTER_MAX);
                        nr[lru] -= nr_to_scan;
                        nr_reclaimed += shrink_list(lru, nr_to_scan, lruvec, sc) {
                            if (is_active_lru(lru)) {
                                /* may_deactivate       is_file_lru
                                 * DEACTIVATE_ANON 01   1 << 0 = 01
                                 * DEACTIVATE_FILE 10   1 << 1 = 10 */
                                if (sc->may_deactivate & (1 << is_file_lru(lru)))
                                    shrink_active_list(nr_to_scan, lruvec, sc, lru);
                                else
                                    sc->skipped_deactivate = 1;
                                return 0;
                            }

                            return shrink_inactive_list(nr_to_scan, lruvec, sc, lru);
                        }
                    }
                }

                cond_resched();

                if (nr_reclaimed < nr_to_reclaim || proportional_reclaim)
                    continue;

                nr_file = nr[LRU_INACTIVE_FILE] + nr[LRU_ACTIVE_FILE];
                nr_anon = nr[LRU_INACTIVE_ANON] + nr[LRU_ACTIVE_ANON];

                if (!nr_file || !nr_anon)
                    break;

                if (nr_file > nr_anon) {
                    unsigned long scan_target = targets[LRU_INACTIVE_ANON]
                        + targets[LRU_ACTIVE_ANON] + 1;
                    lru = LRU_BASE;
                    percentage = nr_anon * 100 / scan_target;
                } else {
                    unsigned long scan_target = targets[LRU_INACTIVE_FILE]
                        + targets[LRU_ACTIVE_FILE] + 1;
                    lru = LRU_FILE;
                    percentage = nr_file * 100 / scan_target;
                }

                /* Stop scanning the smaller of the LRU */
                nr[lru] = 0;
                nr[lru + LRU_ACTIVE] = 0;

                /* Recalculate the other LRU scan count based on its original
                 * scan target and the percentage scanning already complete */
                lru = (lru == LRU_FILE) ? LRU_BASE : LRU_FILE;
                nr_scanned = targets[lru] - nr[lru];
                nr[lru] = targets[lru] * (100 - percentage) / 100;
                nr[lru] -= min(nr[lru], nr_scanned);

                lru += LRU_ACTIVE;
                nr_scanned = targets[lru] - nr[lru];
                nr[lru] = targets[lru] * (100 - percentage) / 100;
                nr[lru] -= min(nr[lru], nr_scanned);
            }

            blk_finish_plug(&plug);
            sc->nr_reclaimed += nr_reclaimed;

            /* Even if we did not try to evict anon pages at all, we want to
             * rebalance the anon lru active/inactive ratio. */
            if (can_age_anon_pages(lruvec_pgdat(lruvec), sc)
                && inactive_is_low(lruvec, LRU_INACTIVE_ANON)) {

                shrink_active_list(SWAP_CLUSTER_MAX, lruvec, sc, LRU_ACTIVE_ANON);
            }
        }

        shrink_slab(sc->gfp_mask, pgdat->node_id, memcg, sc->priority);

        /* Record the group's reclaim efficiency */
        if (!sc->proactive) {
            vmpressure(sc->gfp_mask, memcg, false,
                sc->nr_scanned - scanned,
                sc->nr_reclaimed - reclaimed
            );
        }
    } while ((memcg = mem_cgroup_iter(target_memcg, memcg, NULL)));
}
```

#### get_scan_count

```c
void get_scan_count(struct lruvec *lruvec, struct scan_control *sc, unsigned long *nr)
{
    struct pglist_data *pgdat = lruvec_pgdat(lruvec);
    struct mem_cgroup *memcg = lruvec_memcg(lruvec);
    unsigned long anon_cost, file_cost, total_cost;
    int swappiness = mem_cgroup_swappiness(memcg);
    u64 fraction[ANON_AND_FILE];
    u64 denominator = 0;    /* gcc */
    enum scan_balance scan_balance;
    unsigned long ap, fp;
    enum lru_list lru;

    /* If we have no swap space, do not bother scanning anon folios. */
    if (!sc->may_swap || !can_reclaim_anon_pages(memcg, pgdat->node_id, sc)) {
        scan_balance = SCAN_FILE;
        goto out;
    }

    if (cgroup_reclaim(sc) && !swappiness) {
        scan_balance = SCAN_FILE;
        goto out;
    }

    /* Near OOM, priority decrease from DEAFULT to 0 */
    if (!sc->priority && swappiness) {
        scan_balance = SCAN_EQUAL;
        goto out;
    }

    /* Few File Pages */
    if (sc->file_is_tiny) {
        scan_balance = SCAN_ANON;
        goto out;
    }

    if (sc->cache_trim_mode) {
        scan_balance = SCAN_FILE;
        goto out;
    }

    scan_balance = SCAN_FRACT;

      /* This "weighted" approach ensures neither anon nor file reclaim
       * completely dominates the decision, even if one of them has very low or zero cost. */
    total_cost = sc->anon_cost + sc->file_cost;
    anon_cost = total_cost + sc->anon_cost; /* anon_cost = 2*anon + file */
    file_cost = total_cost + sc->file_cost; /* file_cost = 2*file + anon */
    total_cost = anon_cost + file_cost;            /* total_cost = 3*(anon + file) */

    ap = swappiness * (total_cost + 1);
    ap /= anon_cost + 1;

    fp = (200 - swappiness) * (total_cost + 1);
    fp /= file_cost + 1;

    fraction[0] = ap;
    fraction[1] = fp;
    denominator = ap + fp;

out:
    for_each_evictable_lru(lru) {
        int file = is_file_lru(lru);
        unsigned long lruvec_size;
        unsigned long low, min;
        unsigned long scan;

        lruvec_size = lruvec_lru_size(lruvec, lru, sc->reclaim_idx) {
            for (zid = 0; zid <= zone_idx; zid++) {
                struct zone *zone = &lruvec_pgdat(lruvec)->node_zones[zid];
                if (!managed_zone(zone))
                    continue;
                if (!mem_cgroup_disabled())
                    size += mem_cgroup_get_zone_lru_size(lruvec, lru, zid);
                else
                    size += zone_page_state(zone, NR_ZONE_LRU_BASE + lru);
            }
            return size;
        }
        mem_cgroup_protection(sc->target_mem_cgroup, memcg, &min, &low) {
            *min = READ_ONCE(memcg->memory.emin);
            *low = READ_ONCE(memcg->memory.elow);
        }

        if (min || low) {
            unsigned long cgroup_size = mem_cgroup_size(memcg) {
                return atomic_long_read(&counter->usage);
            }
            unsigned long protection;

            /* memory.low scaling, make sure we retry before OOM */
            if (!sc->memcg_low_reclaim && low > min) {
                protection = low;
                sc->memcg_low_skipped = 1;
            } else {
                protection = min;
            }

            /* Avoid TOCTOU with earlier protection check */
            cgroup_size = max(cgroup_size, protection);

            scan = lruvec_size - lruvec_size * protection /
                (cgroup_size + 1);

            scan = max(scan, SWAP_CLUSTER_MAX);
        } else {
            scan = lruvec_size;
        }

        scan >>= sc->priority;

        if (!scan && !mem_cgroup_online(memcg))
            scan = min(lruvec_size, SWAP_CLUSTER_MAX);

        switch (scan_balance) {
        case SCAN_EQUAL:
            /* Scan lists relative to size */
            break;
        case SCAN_FRACT:
            scan = mem_cgroup_online(memcg)
                ? div64_u64(scan * fraction[file], denominator)
                : DIV64_U64_ROUND_UP(scan * fraction[file], denominator);
            break;
        case SCAN_FILE:
        case SCAN_ANON:
            if ((scan_balance == SCAN_FILE) != file)
                scan = 0;
            break;
        default:
            /* Look ma, no brain */
            BUG();
        }

        nr[lru] = scan;
    }
}

void lru_note_cost(struct lruvec *lruvec, bool file,
        unsigned int nr_io, unsigned int nr_rotated)
{
    unsigned long cost;

    /* Reflect the relative cost of incurring IO and spending CPU
    * time on rotations. This doesn't attempt to make a precise
    * comparison, it just says: if reloads are about comparable
    * between the LRU lists, or rotations are overwhelmingly
    * different between them, adjust scan balance for CPU work. */
    cost = nr_io * SWAP_CLUSTER_MAX + nr_rotated;

    do {
        unsigned long lrusize;

        /* Hold lruvec->lru_lock is safe here, since
        * 1) The pinned lruvec in reclaim, or
        * 2) From a pre-LRU page during refault (which also holds the
        *    rcu lock, so would be safe even if the page was on the LRU
        *    and could move simultaneously to a new lruvec). */
        spin_lock_irq(&lruvec->lru_lock);
        /* Record cost event */
        if (file)
            lruvec->file_cost += cost;
        else
            lruvec->anon_cost += cost;

        /* Decay previous events
        *
        * Because workloads change over time (and to avoid
        * overflow) we keep these statistics as a floating
        * average, which ends up weighing recent refaults
        * more than old ones. */
        lrusize = lruvec_page_state(lruvec, NR_INACTIVE_ANON) +
            lruvec_page_state(lruvec, NR_ACTIVE_ANON) +
            lruvec_page_state(lruvec, NR_INACTIVE_FILE) +
            lruvec_page_state(lruvec, NR_ACTIVE_FILE);

        if (lruvec->file_cost + lruvec->anon_cost > lrusize / 4) {
            lruvec->file_cost /= 2;
            lruvec->anon_cost /= 2;
        }
        spin_unlock_irq(&lruvec->lru_lock);
    } while ((lruvec = parent_lruvec(lruvec)));
}
```

### shrink_active_list

![](../images/kernel/mem-page_reclaim-shrink_active_list.png)

```c
shrink_active_list(nr_to_scan, lruvec, sc, lru) {
    unsigned long nr_taken;
    unsigned long nr_scanned;
    unsigned long vm_flags;
    LIST_HEAD(l_hold);    /* The folios which were snipped off */
    LIST_HEAD(l_active);
    LIST_HEAD(l_inactive);
    unsigned nr_deactivate, nr_activate;
    unsigned nr_rotated = 0;
    int file = is_file_lru(lru);
    struct pglist_data *pgdat = lruvec_pgdat(lruvec);

    lru_add_drain();

    spin_lock_irq(&lruvec->lru_lock);

    /* 1. isolate nr_to_scan size folios, which may still be active/inactive */
    nr_taken = isolate_lru_folios(nr_to_scan, lruvec,
        &l_hold, &nr_scanned, sc, lru
    );

    __mod_node_page_state(pgdat, NR_ISOLATED_ANON + file, nr_taken);

    if (!cgroup_reclaim(sc))
        __count_vm_events(PGREFILL, nr_scanned);
    __count_memcg_events(lruvec_memcg(lruvec), PGREFILL, nr_scanned);

    spin_unlock_irq(&lruvec->lru_lock);

    /* 2. filter the folios which should still be active/inactive */
    while (!list_empty(&l_hold)) {
        struct folio *folio;

        cond_resched();
        folio = lru_to_folio(&l_hold);
        list_del(&folio->lru);

        /* 2.1 move to unevictable list */
        if (unlikely(!folio_evictable(folio))) {
            folio_putback_lru(folio);
            continue;
        }

        if (unlikely(buffer_heads_over_limit)) {
            if (folio_needs_release(folio) && folio_trylock(folio)) {
                filemap_release_folio(folio, 0);
                folio_unlock(folio);
            }
        }

        /* 2.2 rotate to active list
         * Referenced or rmap lock contention: rotate */
        if (folio_referenced(folio, 0, sc->target_mem_cgroup, &vm_flags) != 0) {
            /* Identify referenced, file-backed active folios and
             * give them one more trip around the active list. */
            if ((vm_flags & VM_EXEC) && folio_is_file_lru(folio)) {
                nr_rotated += folio_nr_pages(folio);
                list_add(&folio->lru, &l_active);
                continue;
            }
        }

        /* 2.3 move to inactive list */
        folio_clear_active(folio);    /* we are de-activating */
        folio_set_workingset(folio);
        list_add(&folio->lru, &l_inactive);
    }

    spin_lock_irq(&lruvec->lru_lock);

    /* 3. moves folios from private @list to appropriate LRU list.
     * Returns the number of pages moved to the given lruvec */
    nr_activate = move_folios_to_lru(lruvec, &l_active);
    nr_deactivate = move_folios_to_lru(lruvec, &l_inactive);

    __count_vm_events(PGDEACTIVATE, nr_deactivate);
    __count_memcg_events(lruvec_memcg(lruvec), PGDEACTIVATE, nr_deactivate);

    __mod_node_page_state(pgdat, NR_ISOLATED_ANON + file, -nr_taken);
    spin_unlock_irq(&lruvec->lru_lock);

    if (nr_rotated) {
        lru_note_cost(lruvec, file, 0/*nr_io*/, nr_rotated);
    }
}
```

#### isolate_lru_folios

```c
unsigned long isolate_lru_folios(unsigned long nr_to_scan,
    struct lruvec *lruvec, struct list_head *dst,
    unsigned long *nr_scanned, struct scan_control *sc,
    enum lru_list lru)
{
    struct list_head *src = &lruvec->lists[lru];
    unsigned long nr_taken = 0;
    unsigned long nr_zone_taken[MAX_NR_ZONES] = { 0 };
    unsigned long nr_skipped[MAX_NR_ZONES] = { 0, };
    unsigned long skipped = 0;
    unsigned long scan, total_scan, nr_pages;
    LIST_HEAD(folios_skipped);

    total_scan = 0;
    scan = 0;

    /* iterate list from old data to new data */
    while (scan < nr_to_scan && !list_empty(src)) {
        struct list_head *move_to = src;
        struct folio *folio;

        folio = lru_to_folio(src);
        prefetchw_prev_lru_folio(folio, src, flags);

        nr_pages = folio_nr_pages(folio);
        total_scan += nr_pages;

        if (folio_zonenum(folio) > sc->reclaim_idx || skip_cma(folio, sc)) {
            nr_skipped[folio_zonenum(folio)] += nr_pages;
            move_to = &folios_skipped;
            goto move;
        }

        scan += nr_pages;

        if (!folio_test_lru(folio))
            goto move;
        if (!sc->may_unmap && folio_mapped(folio))
            goto move;

        if (unlikely(!folio_try_get(folio)))
            goto move;

        if (!folio_test_clear_lru(folio)) {
            /* Another thread is already isolating this folio */
            folio_put(folio);
            goto move;
        }

        nr_taken += nr_pages;
        nr_zone_taken[folio_zonenum(folio)] += nr_pages;
        move_to = dst;
move:
        list_move(&folio->lru, move_to);
    }

    if (!list_empty(&folios_skipped)) {
        list_splice(&folios_skipped, src);
        for (zid = 0; zid < MAX_NR_ZONES; zid++) {
            if (!nr_skipped[zid])
                continue;

            __count_zid_vm_events(PGSCAN_SKIP, zid, nr_skipped[zid]);
            skipped += nr_skipped[zid];
        }
    }

    *nr_scanned = total_scan;
    update_lru_sizes(lruvec, lru, nr_zone_taken) {
        for (zid = 0; zid < MAX_NR_ZONES; zid++) {
            if (!nr_zone_taken[zid])
                continue;

            update_lru_size(lruvec, lru, zid, -nr_zone_taken[zid]) {
                __update_lru_size(lruvec, lru, zid, nr_pages);
            #ifdef CONFIG_MEMCG
                mem_cgroup_update_lru_size(lruvec, lru, zid, nr_pages);
            #endif
            }
        }
    }

    return nr_taken;
}
```

#### move_folios_to_lru

```c
/* Moves folios from private @list to appropriate LRU list.
 * Returns the number of pages moved to the given lruvec. */
move_folios_to_lru(lruvec, list) {
    int nr_pages, nr_moved = 0;
    struct folio_batch free_folios;

    folio_batch_init(&free_folios);
    while (!list_empty(list)) {
        struct folio *folio = lru_to_folio(list);

        list_del(&folio->lru);

        /* 3.1 move to unevictable list */
        if (unlikely(!folio_evictable(folio))) {
            spin_unlock_irq(&lruvec->lru_lock);
            folio_putback_lru(folio);
            spin_lock_irq(&lruvec->lru_lock);
            continue;
        }

        folio_set_lru(folio);

        /* 3.2 move to buddy system */
        if (unlikely(folio_put_testzero(folio))) {
            __folio_clear_lru_flags(folio);

            if (folio_test_large(folio) && folio_test_large_rmappable(folio)) {
                folio_undo_large_rmappable(folio);
            }
            if (folio_batch_add(&free_folios, folio) == 0) {
                spin_unlock_irq(&lruvec->lru_lock);
                mem_cgroup_uncharge_folios(&free_folios);
                free_unref_folios(&free_folios);
                spin_lock_irq(&lruvec->lru_lock);
            }

            continue;
        }

        /* 3.3 move to active/inactive list */
        lruvec_add_folio(lruvec, folio);
        nr_pages = folio_nr_pages(folio);
        nr_moved += nr_pages;
        if (folio_test_active(folio))
            workingset_age_nonresident(lruvec, nr_pages);
    }

    if (free_folios.nr) {
        spin_unlock_irq(&lruvec->lru_lock);
        mem_cgroup_uncharge_folios(&free_folios);
        free_unref_folios(&free_folios);
        spin_lock_irq(&lruvec->lru_lock);
    }

    return nr_moved;
}
```

### shrink_inactive_list

![](../images/kernel/mem-page_reclaim-shrink_inactive_list.png)

```c
shrink_inactive_list(nr_to_scan, lruvec, sc, lru) {
    LIST_HEAD(folio_list);
    unsigned long nr_scanned;
    unsigned int nr_reclaimed = 0;
    unsigned long nr_taken;
    struct reclaim_stat stat;
    bool file = is_file_lru(lru);
    enum vm_event_item item;
    struct pglist_data *pgdat = lruvec_pgdat(lruvec);
    bool stalled = false;

    while (unlikely(too_many_isolated(pgdat, file, sc))) {
        if (stalled)
            return 0;

        /* wait a bit for the reclaimer. */
        stalled = true;
        reclaim_throttle(pgdat, VMSCAN_THROTTLE_ISOLATED);

        /* We are about to die and free our memory. Return now. */
        if (fatal_signal_pending(current))
            return SWAP_CLUSTER_MAX;
    }

    lru_add_drain();

    spin_lock_irq(&lruvec->lru_lock);

    nr_taken = isolate_lru_folios(nr_to_scan, lruvec, &folio_list, &nr_scanned, sc, lru);

    __mod_node_page_state(pgdat, NR_ISOLATED_ANON + file, nr_taken);
    item = PGSCAN_KSWAPD + reclaimer_offset();
    if (!cgroup_reclaim(sc))
        __count_vm_events(item, nr_scanned);
    __count_memcg_events(lruvec_memcg(lruvec), item, nr_scanned);
    __count_vm_events(PGSCAN_ANON + file, nr_scanned);

    spin_unlock_irq(&lruvec->lru_lock);

    if (nr_taken == 0)
        return 0;

    nr_reclaimed = shrink_folio_list(&folio_list, pgdat, sc, &stat, false);
        --->

    spin_lock_irq(&lruvec->lru_lock);
    move_folios_to_lru(lruvec, &folio_list);
        --->

    __mod_node_page_state(pgdat, NR_ISOLATED_ANON + file, -nr_taken);
    item = PGSTEAL_KSWAPD + reclaimer_offset();
    if (!cgroup_reclaim(sc))
        __count_vm_events(item, nr_reclaimed);
    __count_memcg_events(lruvec_memcg(lruvec), item, nr_reclaimed);
    __count_vm_events(PGSTEAL_ANON + file, nr_reclaimed);
    spin_unlock_irq(&lruvec->lru_lock);

    lru_note_cost(lruvec, file, stat.nr_pageout, nr_scanned - nr_reclaimed);

    if (stat.nr_unqueued_dirty == nr_taken) {
        /* wakeup dirty page flusher threads */
        wakeup_flusher_threads(WB_REASON_VMSCAN);
        if (!writeback_throttling_sane(sc))
            reclaim_throttle(pgdat, VMSCAN_THROTTLE_WRITEBACK);
    }

    sc->nr.dirty += stat.nr_dirty;
    sc->nr.congested += stat.nr_congested;
    sc->nr.unqueued_dirty += stat.nr_unqueued_dirty;
    sc->nr.writeback += stat.nr_writeback;
    sc->nr.immediate += stat.nr_immediate;
    sc->nr.taken += nr_taken;
    if (file)
        sc->nr.file_taken += nr_taken;

    return nr_reclaimed;
}
```

#### shrink_folio_list

![](../images/kernel/mem-page_reclaim-shrink_folio_list.png)

```c
unsigned int shrink_folio_list(struct list_head *folio_list,
        struct pglist_data *pgdat, struct scan_control *sc,
        struct reclaim_stat *stat, bool ignore_references)
{
    struct folio_batch free_folios;
    LIST_HEAD(ret_folios);
    LIST_HEAD(demote_folios);
    unsigned int nr_reclaimed = 0;
    unsigned int pgactivate = 0;
    bool do_demote_pass;
    struct swap_iocb *plug = NULL;

    memset(stat, 0, sizeof(*stat));
    cond_resched();
    do_demote_pass = can_demote(pgdat->node_id, sc);

retry:
    while (!list_empty(folio_list)) {
        struct address_space *mapping;
        struct folio *folio;
        enum folio_references references = FOLIOREF_RECLAIM;
        bool dirty, writeback;
        unsigned int nr_pages;

        cond_resched();

        folio = lru_to_folio(folio_list);
        list_del(&folio->lru);

/* 1. Initial Checks and Skipping : locked, unevictable, unmappable folio */
        if (!folio_trylock(folio))
            goto keep;

        nr_pages = folio_nr_pages(folio);

        /* Account the number of base pages */
        sc->nr_scanned += nr_pages;

        if (unlikely(!folio_evictable(folio)))
            goto activate_locked;

        if (!sc->may_unmap && folio_mapped(folio))
            goto keep_locked;

        /* folio_update_gen() tried to promote this page? */
        if (lru_gen_enabled() && !ignore_references
            && folio_mapped(folio) && folio_test_referenced(folio))
            goto keep_locked;

        folio_check_dirty_writeback(folio, &dirty, &writeback);
        if (drty || writeback)
            stat->nr_dirty += nr_pages;

        if (dirty && !writeback)
            stat->nr_unqueued_dirty += nr_pages;

        if (writeback && folio_test_reclaim(folio))
            stat->nr_congested += nr_pages;

/* 2. check writeback page */
        if (folio_test_writeback(folio)) { /* anon/file page is under writeback */
            /* Case 1 above: Excessive Writeback
             * folios are being queued for I/O but
             * are being recycled through the LRU before the I/O can complete.*/
            if (current_is_kswapd() &&
                folio_test_reclaim(folio) &&
                test_bit(PGDAT_WRITEBACK, &pgdat->flags)) {
                stat->nr_immediate += nr_pages; /* do immediate reclaim */
                goto activate_locked;

            /* Case 2 above: No FS/IO or Not Marked for Reclaim
             * Global or new memcg reclaim encounters a folio that is
             * not marked for immediate reclaim, or the caller does not
             * have __GFP_FS */
            } else if (writeback_throttling_sane(sc) ||
                !folio_test_reclaim(folio) ||
                !may_enter_fs(folio, sc->gfp_mask)) { /* __GFP_FS __GFP_IO */

                folio_set_reclaim(folio);
                stat->nr_writeback += nr_pages;
                goto activate_locked;

            /* Case 3 above */
            } else {
                folio_unlock(folio);
                folio_wait_writeback(folio);
                /* then go back and try same folio again */
                list_add_tail(&folio->lru, folio_list);
                continue;
            }
        }

/* 3. check pte refcnt and PG_referenced */
        if (!ignore_references)
            references = folio_check_references(folio, sc);

        switch (references) {
        case FOLIOREF_ACTIVATE: /* hot page: pte_refcnt > 1 && PG_referenced */
            goto activate_locked;
        case FOLIOREF_KEEP:     /* warm page: pte_refcnt > 0 */
            stat->nr_ref_keep += nr_pages;
            goto keep_locked;
        case FOLIOREF_RECLAIM:  /* cold page: pte_refcnt == 0 */
        case FOLIOREF_RECLAIM_CLEAN:
            ; /* try to reclaim the folio below */
        }

        /* Before reclaiming the folio, try to relocate
         * its contents to another node. */
        if (do_demote_pass &&
            (thp_migration_supported() || !folio_test_large(folio))) {
            list_add(&folio->lru, &demote_folios);
            folio_unlock(folio);
            continue;
        }

/* 4. add anno swapbacked folio to swap cache
 * and mark PG_swapcache PG_dirty */
        if (folio_test_anon(folio) && folio_test_swapbacked(folio)) {
            if (!folio_test_swapcache(folio)) {
                if (!(sc->gfp_mask & __GFP_IO))
                    goto keep_locked;
                if (folio_maybe_dma_pinned(folio))
                    goto keep_locked;
                if (folio_test_large(folio)) {
                    if (!can_split_folio(folio, NULL))
                        goto activate_locked;
                    if (!folio_entire_mapcount(folio) && split_folio_to_list(folio, folio_list))
                        goto activate_locked;
                }

                if (!add_to_swap(folio)) { /* add failed */
                    if (!folio_test_large(folio))
                        goto activate_locked_split;
                    /* Fallback to swap normal pages */
                    if (split_folio_to_list(folio, folio_list))
                        goto activate_locked;
                    if (!add_to_swap(folio))
                        goto activate_locked_split;
                }
            }
        } else if (folio_test_swapbacked(folio) && folio_test_large(folio)) {
            if (split_folio_to_list(folio, folio_list))
                goto keep_locked;
        }

        /* large page was splited */
        if ((nr_pages > 1) && !folio_test_large(folio)) {
            sc->nr_scanned -= (nr_pages - 1);
            nr_pages = 1;
        }

/* 5. unmap user space mapped folio */
        /* The folio is mapped into the page tables of one or more
         * processes. Try to unmap it here. */
        if (folio_mapped(folio)) {
            enum ttu_flags flags = TTU_BATCH_FLUSH;
            bool was_swapbacked = folio_test_swapbacked(folio);

            if (folio_test_pmd_mappable(folio))
                flags |= TTU_SPLIT_HUGE_PMD;

            /* try to unmap and write swp_pte */
            try_to_unmap(folio, flags);
            if (folio_mapped(folio)) {
                stat->nr_unmap_fail += nr_pages;
                if (!was_swapbacked && folio_test_swapbacked(folio))
                    stat->nr_lazyfree_fail += nr_pages;
                goto activate_locked;
            }
        }

        if (folio_maybe_dma_pinned(folio))
            goto activate_locked;

/* 6. writeback diry anon/file page cache */
        mapping = folio_mapping(folio);
        if (folio_test_dirty(folio)) {
            /* dont writeback:
             * 1. only kswap can writeback, otherwise its may be stack overflow
             * 2. folio is not under reaclaim
             * 3. pgdat is not doing batch writeback */
            if (folio_is_file_lru(folio) &&
                (!current_is_kswapd() || !folio_test_reclaim(folio)
                    || !test_bit(PGDAT_DIRTY, &pgdat->flags))) {

                node_stat_mod_folio(folio, NR_VMSCAN_IMMEDIATE, nr_pages);
                folio_set_reclaim(folio);
                goto activate_locked;
            }

            if (references == FOLIOREF_RECLAIM_CLEAN)
                goto keep_locked;
            if (!may_enter_fs(folio, sc->gfp_mask))
                goto keep_locked;
            if (!sc->may_writepage)
                goto keep_locked;

            try_to_unmap_flush_dirty();
            switch (pageout(folio, mapping, &plug)) {
            case PAGE_KEEP:
                goto keep_locked;
            case PAGE_ACTIVATE:
                goto activate_locked;
            case PAGE_SUCCESS:
                /* after successful wirteback,
                 * the folio may be writebackable, dirty again */
                stat->nr_pageout += nr_pages;

                if (folio_test_writeback(folio))
                    goto keep;
                if (folio_test_dirty(folio))
                    goto keep;
                if (!folio_trylock(folio))
                    goto keep;
                if (folio_test_dirty(folio) || folio_test_writeback(folio))
                    goto keep_locked;

                mapping = folio_mapping(folio);
                fallthrough;
            case PAGE_CLEAN:
                ; /* try to free the folio below */
            }
        }

/* 7. free folio buffer */
        /* If the folio has buffers, try to free the buffer
         * mappings associated with this folio. If we succeed
         * we try to free the folio as well. */
        if (folio_needs_release(folio)) {
            if (!filemap_release_folio(folio, sc->gfp_mask))
                goto activate_locked;
            if (!mapping && folio_ref_count(folio) == 1) {
                folio_unlock(folio);
                if (folio_put_testzero(folio))
                    goto free_it;
                else {
                    /* rare race with speculative reference.
                    * the speculative reference will free
                    * this folio shortly, so we may
                    * increment nr_reclaimed here (and
                    * leave it off the LRU). */
                    nr_reclaimed += nr_pages;
                    continue;
                }
            }
        }

/* 8. remove address space mapping from swapcache or filecache */
        if (folio_test_anon(folio) && !folio_test_swapbacked(folio)) {
            if (!folio_ref_freeze(folio, 1))
                goto keep_locked;
            count_vm_events(PGLAZYFREED, nr_pages);
            count_memcg_folio_events(folio, PGLAZYFREED, nr_pages);
        } else if (!mapping || !__remove_mapping(mapping, folio, true, sc->target_mem_cgroup))
            goto keep_locked;

        folio_unlock(folio);

free_it:
        nr_reclaimed += nr_pages;

        if (folio_test_large(folio) && folio_test_large_rmappable(folio))
            folio_undo_large_rmappable(folio);

        if (folio_batch_add(&free_folios, folio) == 0) {
            mem_cgroup_uncharge_folios(&free_folios);
            try_to_unmap_flush();
            free_unref_folios(&free_folios);
        }

        continue;

activate_locked_split:
        if (nr_pages > 1) {
            sc->nr_scanned -= (nr_pages - 1);
            nr_pages = 1;
        }
activate_locked:
        /* Not a candidate for swapping, so reclaim swap space. */
        if (folio_test_swapcache(folio)
            && (mem_cgroup_swap_full(folio) || folio_test_mlocked(folio))) {

            /* Free the swap cache used for this folio. */
            folio_free_swap(folio);
        }
        if (!folio_test_mlocked(folio)) {
            int type = folio_is_file_lru(folio);
            folio_set_active(folio);
            stat->nr_activate[type] += nr_pages;
            count_memcg_folio_events(folio, PGACTIVATE, nr_pages);
        }
keep_locked:
        folio_unlock(folio);
keep:
        list_add(&folio->lru, &ret_folios);
    }
    /* 'folio_list' is always empty here */

    /* Migrate folios selected for demotion */
    nr_reclaimed += demote_folio_list(&demote_folios, pgdat);
    /* Folios that could not be demoted are still in @demote_folios */
    if (!list_empty(&demote_folios)) {
        /* Folios which weren't demoted go back on @folio_list */
        list_splice_init(&demote_folios, folio_list);

        if (!sc->proactive) {
            do_demote_pass = false;
            goto retry;
        }
    }

    pgactivate = stat->nr_activate[0] + stat->nr_activate[1];

    mem_cgroup_uncharge_folios(&free_folios);
    try_to_unmap_flush();
    free_unref_folios(&free_folios);

    list_splice(&ret_folios, folio_list);
    count_vm_events(PGACTIVATE, pgactivate);

    if (plug)
        swap_write_unplug(plug);
    return nr_reclaimed;
}
```

##### remove_mapping

```c
/* remove mapping from swapcache or filecache */
int __remove_mapping(struct address_space *mapping, struct folio *folio,
                bool reclaimed, struct mem_cgroup *target_memcg)
{
    int refcount;
    void *shadow = NULL;

    BUG_ON(!folio_test_locked(folio));
    BUG_ON(mapping != folio_mapping(folio));

    if (!folio_test_swapcache(folio))
        spin_lock(&mapping->host->i_lock);
    xa_lock_irq(&mapping->i_pages);

    refcount = 1 + folio_nr_pages(folio);
    if (!folio_ref_freeze(folio, refcount))
        goto cannot_free;
    /* note: atomic_cmpxchg in folio_ref_freeze provides the smp_rmb */
    if (unlikely(folio_test_dirty(folio))) {
        folio_ref_unfreeze(folio, refcount);
        goto cannot_free;
    }
/* 1. swap cache */
    if (folio_test_swapcache(folio)) {
        swp_entry_t swap = folio->swap;

        if (reclaimed && !mapping_exiting(mapping)) {
            shadow = workingset_eviction(folio, target_memcg);
        }
        __delete_from_swap_cache(folio, swap, shadow);
            --->
        mem_cgroup_swapout(folio, swap);
        xa_unlock_irq(&mapping->i_pages);
        put_swap_folio(folio, swap);
    } else {
/* 2. file cache */
        void (*free_folio)(struct folio *);

        free_folio = mapping->a_ops->free_folio;

        if (reclaimed && folio_is_file_lru(folio)
            && !mapping_exiting(mapping) && !dax_mapping(mapping)) {

            shadow = workingset_eviction(folio, target_memcg);
        }
        __filemap_remove_folio(folio, shadow) {
            struct address_space *mapping = folio->mapping;

            filemap_unaccount_folio(mapping, folio) {
                nr = folio_nr_pages(folio);

                __lruvec_stat_mod_folio(folio, NR_FILE_PAGES, -nr);
                if (folio_test_swapbacked(folio)) {
                    __lruvec_stat_mod_folio(folio, NR_SHMEM, -nr);
                    if (folio_test_pmd_mappable(folio)) {
                        __lruvec_stat_mod_folio(folio, NR_SHMEM_THPS, -nr);
                    }
                } else if (folio_test_pmd_mappable(folio)) {
                    __lruvec_stat_mod_folio(folio, NR_FILE_THPS, -nr);
                    filemap_nr_thps_dec(mapping);
                }

                if (WARN_ON_ONCE(folio_test_dirty(folio) && mapping_can_writeback(mapping)))
                    folio_account_cleaned(folio, inode_to_wb(mapping->host));
            }

            page_cache_delete(mapping, folio, shadow) {
                XA_STATE(xas, &mapping->i_pages, folio->index);
                long nr = 1;

                mapping_set_update(&xas, mapping);

                xas_set_order(&xas, folio->index, folio_order(folio));
                nr = folio_nr_pages(folio);

                VM_BUG_ON_FOLIO(!folio_test_locked(folio), folio);

                xas_store(&xas, shadow);
                xas_init_marks(&xas);

                folio->mapping = NULL;
                /* Leave page->index set: truncation lookup relies upon it */
                mapping->nrpages -= nr;
            }
        }

        xa_unlock_irq(&mapping->i_pages);
        if (mapping_shrinkable(mapping))
            inode_add_lru(mapping->host);
        spin_unlock(&mapping->host->i_lock);

        if (free_folio)
            free_folio(folio);
    }

    return 1;

cannot_free:
    xa_unlock_irq(&mapping->i_pages);
    if (!folio_test_swapcache(folio))
        spin_unlock(&mapping->host->i_lock);
    return 0;
}
```

## shrinker

```c
struct shrinker {
    unsigned long (*count_objects)(
        struct shrinker *, struct shrink_control *sc);
    unsigned long (*scan_objects)(
        struct shrinker *, struct shrink_control *sc);

    long batch; /* reclaim batch size, 0 = default */
    int seeks;  /* seeks to recreate an obj */
    unsigned flags;

    refcount_t refcount;
    struct completion done; /* use to wait for refcount to reach 0 */
    struct rcu_head rcu;

    void *private_data;

    /* These are for internal use */
    struct list_head list;
#ifdef CONFIG_MEMCG
    /* ID in shrinker_idr */
    int id;
#endif

    /* objs pending delete, per node */
    atomic_long_t *nr_deferred;
};
```

```c
shrinker_register(sbi->s_es_shrinker); /* shrinker to reclaim extents from extent status tree */
shrinker_register(s->s_shrink); /* per-sb shrinker handle */
shrinker_register(huge_zero_page_shrinker);
shrinker_register(deferred_split_shrinker);
shrinker_register(vmap_node_shrinker); /* for vmalloc vmap node */
shrinker_register(workingset_shadow_shrinker);
shrinker_register(pool->shrinker); /* mm-zspool */
shrinker_register(zswap_shrinker);
shrinker_register(kfree_rcu_shrinker);
```

```c
static struct ctl_table kern_table[] = {
    {
        .procname       = "drop_caches",
        .data           = &sysctl_drop_caches,
        .maxlen         = sizeof(int),
        .mode           = 0200,
        .proc_handler   = drop_caches_sysctl_handler,
        .extra1         = SYSCTL_ONE,
        .extra2         = SYSCTL_FOUR,
    }
};

int drop_caches_sysctl_handler(struct ctl_table *table, int write,
        void *buffer, size_t *length, loff_t *ppos)
{
    int ret;

    ret = proc_dointvec_minmax(table, write, buffer, length, ppos);
    if (ret)
        return ret;
    if (write) {
        static int stfu;

        if (sysctl_drop_caches & 1) {
            lru_add_drain_all();
            iterate_supers(drop_pagecache_sb, NULL);
            count_vm_event(DROP_PAGECACHE);
        }
        if (sysctl_drop_caches & 2) {
            drop_slab() {
                int nid;
                int shift = 0;
                unsigned long freed;

                do {
                    freed = 0;
                    for_each_online_node(nid) {
                        if (fatal_signal_pending(current))
                            return;

                        freed += drop_slab_node(nid) {
                            unsigned long freed = 0;
                            struct mem_cgroup *memcg = NULL;

                            memcg = mem_cgroup_iter(NULL, NULL, NULL);
                            do {
                                freed += shrink_slab(GFP_KERNEL, nid, memcg, 0);
                            } while ((memcg = mem_cgroup_iter(NULL, memcg, NULL)) != NULL);

                            return freed;
                        }
                    }
                } while ((freed >> shift++) > 1);
            }
            count_vm_event(DROP_SLAB);
        }
        stfu |= sysctl_drop_caches & 4;
    }
    return 0;
}
```

### shrink_slab

```c
unsigned long shrink_slab(gfp_t gfp_mask, int nid, struct mem_cgroup *memcg, int priority)
{
    unsigned long ret, freed = 0;
    struct shrinker *shrinker;

    if (!mem_cgroup_disabled() && !mem_cgroup_is_root(memcg))
        return shrink_slab_memcg(gfp_mask, nid, memcg, priority);

    rcu_read_lock();
    list_for_each_entry_rcu(shrinker, &shrinker_list, list) {
        struct shrink_control sc = {
            .gfp_mask   = gfp_mask,
            .nid        = nid,
            .memcg      = memcg,
        };

        if (!shrinker_try_get(shrinker))
            continue;

        rcu_read_unlock();

        ret = do_shrink_slab(&sc, shrinker, priority) {
            unsigned long freed = 0;
            unsigned long long delta;
            long total_scan;
            long freeable;
            long nr;
            long new_nr;
            long batch_size = shrinker->batch ? shrinker->batch : SHRINK_BATCH;
            long scanned = 0, next_deferred;

            freeable = shrinker->count_objects(shrinker, shrinkctl);
            if (freeable == 0 || freeable == SHRINK_EMPTY)
                return freeable;

            /* copy the current shrinker scan count into a local variable
            * and zero it so that other concurrent shrinker invocations
            * don't also do this scanning work. */
            nr = xchg_nr_deferred(shrinker, shrinkctl);

            if (shrinker->seeks) {
                delta = freeable >> priority;
                delta *= 4;
                do_div(delta, shrinker->seeks);
            } else {
                /* These objects don't require any IO to create. Trim
                * them aggressively under memory pressure to keep
                * them from causing refetches in the IO caches. */
                delta = freeable / 2;
            }

            total_scan = nr >> priority;
            total_scan += delta;
            total_scan = min(total_scan, (2 * freeable));

            while (total_scan >= batch_size || total_scan >= freeable) {
                unsigned long ret;
                unsigned long nr_to_scan = min(batch_size, total_scan);

                shrinkctl->nr_to_scan = nr_to_scan;
                shrinkctl->nr_scanned = nr_to_scan;
                ret = shrinker->scan_objects(shrinker, shrinkctl);
                if (ret == SHRINK_STOP)
                    break;
                freed += ret;

                total_scan -= shrinkctl->nr_scanned;
                scanned += shrinkctl->nr_scanned;

                cond_resched();
            }

            next_deferred = max_t(long, (nr + delta - scanned), 0);
            next_deferred = min(next_deferred, (2 * freeable));

            /* move the unused scan count back into the shrinker in a
            * manner that handles concurrent updates. */
            new_nr = add_nr_deferred(next_deferred, shrinker, shrinkctl) {
                int nid = sc->nid;

                if (!(shrinker->flags & SHRINKER_NUMA_AWARE))
                    nid = 0;

                if (sc->memcg && (shrinker->flags & SHRINKER_MEMCG_AWARE))
                    return add_nr_deferred_memcg(nr, nid, shrinker, sc->memcg);

                return atomic_long_add_return(nr, &shrinker->nr_deferred[nid]);
            }

            return freed;
        }
        if (ret == SHRINK_EMPTY)
            ret = 0;
        freed += ret;

        rcu_read_lock();
        shrinker_put(shrinker);
    }

    rcu_read_unlock();
    cond_resched();
    return freed;
}
```

### shrink_slab_memcg

```c
unsigned long shrink_slab_memcg(gfp_t gfp_mask, int nid,
    struct mem_cgroup *memcg, int priority)
{
    struct shrinker_info *info;
    unsigned long ret, freed = 0;
    int offset, index = 0;

    if (!mem_cgroup_online(memcg))
        return 0;

again:
    rcu_read_lock();
    info = rcu_dereference(memcg->nodeinfo[nid]->shrinker_info);
    if (unlikely(!info))
        goto unlock;

    if (index < shrinker_id_to_index(info->map_nr_max)) {
        struct shrinker_info_unit *unit;

        unit = info->unit[index];

        rcu_read_unlock();

        for_each_set_bit(offset, unit->map, SHRINKER_UNIT_BITS) {
            struct shrink_control sc = {
                .gfp_mask = gfp_mask,
                .nid = nid,
                .memcg = memcg,
            };
            struct shrinker *shrinker;
            int shrinker_id = calc_shrinker_id(index, offset);

            rcu_read_lock();
            shrinker = idr_find(&shrinker_idr, shrinker_id);
            if (unlikely(!shrinker || !shrinker_try_get(shrinker))) {
                clear_bit(offset, unit->map);
                rcu_read_unlock();
                continue;
            }
            rcu_read_unlock();

            /* Call non-slab shrinkers even though kmem is disabled */
            if (!memcg_kmem_online() && !(shrinker->flags & SHRINKER_NONSLAB))
                continue;

            ret = do_shrink_slab(&sc, shrinker, priority);
            if (ret == SHRINK_EMPTY) {
                clear_bit(offset, unit->map);
                smp_mb__after_atomic();
                ret = do_shrink_slab(&sc, shrinker, priority);
                if (ret == SHRINK_EMPTY)
                    ret = 0;
                else
                    set_shrinker_bit(memcg, nid, shrinker_id);
            }
            freed += ret;
            shrinker_put(shrinker);
        }

        index++;
        goto again;
    }
unlock:
    rcu_read_unlock();
    return freed;
}
```

### super_shrinker

```c
struct super_block {
    struct list_lru     s_dentry_lru;
    struct list_lru     s_inode_lru;
    struct rcu_head     rcu;
    struct work_struct  destroy_work;
};
```

```c
struct super_block *sget_fc(struct fs_context *fc,
    int (*test)(struct super_block *, struct fs_context *),
    int (*set)(struct super_block *, struct fs_context *))
{
    struct super_block *s = NULL;

    s = alloc_super(fc->fs_type, fc->sb_flags, user_ns) {
        struct super_block *s = kzalloc(sizeof(struct super_block), GFP_KERNEL);
        s->s_shrink = shrinker_alloc(SHRINKER_NUMA_AWARE | SHRINKER_MEMCG_AWARE,
                    "sb-%s", type->name);

        s->s_shrink->scan_objects = super_cache_scan;
        s->s_shrink->count_objects = super_cache_count;
        s->s_shrink->batch = 1024;
        s->s_shrink->private_data = s;
    }

    shrinker_register(s->s_shrink);
    return s;
}

unsigned long super_cache_scan(struct shrinker *shrink,
    struct shrink_control *sc)
{
    struct super_block *sb;
    long    fs_objects = 0;
    long    total_objects;
    long    freed = 0;
    long    dentries;
    long    inodes;

    sb = shrink->private_data;

    /* Deadlock avoidance.  We may hold various FS locks, and we don't want
    * to recurse into the FS that called us in clear_inode() and friends.. */
    if (!(sc->gfp_mask & __GFP_FS))
        return SHRINK_STOP;

    if (!super_trylock_shared(sb))
        return SHRINK_STOP;

    if (sb->s_op->nr_cached_objects)
        fs_objects = sb->s_op->nr_cached_objects(sb, sc);

    inodes = list_lru_shrink_count(&sb->s_inode_lru, sc);
    dentries = list_lru_shrink_count(&sb->s_dentry_lru, sc);
    total_objects = dentries + inodes + fs_objects + 1;
    if (!total_objects)
        total_objects = 1;

    /* proportion the scan between the caches */
    dentries = mult_frac(sc->nr_to_scan, dentries, total_objects);
    inodes = mult_frac(sc->nr_to_scan, inodes, total_objects);
    fs_objects = mult_frac(sc->nr_to_scan, fs_objects, total_objects);

    /* prune the dcache first as the icache is pinned by it, then
    * prune the icache, followed by the filesystem specific caches
    *
    * Ensure that we always scan at least one object - memcg kmem
    * accounting uses this to fully empty the caches. */
    sc->nr_to_scan = dentries + 1;
    freed = prune_dcache_sb(sb, sc) {
        LIST_HEAD(dispose);
        long freed;

        freed = list_lru_shrink_walk(&sb->s_dentry_lru, sc, dentry_lru_isolate, &dispose);
        shrink_dentry_list(&dispose) {
            while (!list_empty(list)) {
                struct dentry *dentry;

                dentry = list_entry(list->prev, struct dentry, d_lru);
                spin_lock(&dentry->d_lock);
                rcu_read_lock();
                if (!lock_for_kill(dentry)) {
                    bool can_free;
                    rcu_read_unlock();
                    d_shrink_del(dentry);
                    can_free = dentry->d_flags & DCACHE_DENTRY_KILLED;
                    spin_unlock(&dentry->d_lock);
                    if (can_free)
                        dentry_free(dentry);
                    continue;
                }
                d_shrink_del(dentry) {
                    D_FLAG_VERIFY(dentry, DCACHE_SHRINK_LIST | DCACHE_LRU_LIST);
                    list_del_init(&dentry->d_lru);
                    dentry->d_flags &= ~(DCACHE_SHRINK_LIST | DCACHE_LRU_LIST);
                    this_cpu_dec(nr_dentry_unused);
                }

                shrink_kill(dentry) {
                    do {
                        rcu_read_unlock();
                        victim = __dentry_kill(victim);
                            --->
                        rcu_read_lock();
                    } while (victim && lock_for_kill(victim));
                    rcu_read_unlock();
                    if (victim)
                        spin_unlock(&victim->d_lock);
                }
            }
        }
        return freed;
    }

    sc->nr_to_scan = inodes + 1;
    freed += prune_icache_sb(sb, sc) {
        LIST_HEAD(freeable);
        long freed;

        freed = list_lru_shrink_walk(&sb->s_inode_lru, sc, inode_lru_isolate, &freeable);
        dispose_list(&freeable) {
            while (!list_empty(head)) {
                struct inode *inode;

                inode = list_first_entry(head, struct inode, i_lru);
                list_del_init(&inode->i_lru);

                evict(inode);
                    --->
                cond_resched();
            }
        }
        return freed;
    }

    if (fs_objects) {
        sc->nr_to_scan = fs_objects + 1;
        freed += sb->s_op->free_cached_objects(sb, sc);
    }

    super_unlock_shared(sb);
    return freed;
}
```

## vmpressure

```c
void vmpressure(gfp_t gfp, struct mem_cgroup *memcg, bool tree,
        unsigned long scanned, unsigned long reclaimed)
{
    struct vmpressure *vmpr;

    if (mem_cgroup_disabled())
        return;

    /* The in-kernel users only care about the reclaim efficiency
    * for this @memcg rather than the whole subtree, and there
    * isn't and won't be any in-kernel user in a legacy cgroup. */
    if (!cgroup_subsys_on_dfl(memory_cgrp_subsys) && !tree)
        return;

    vmpr = memcg_to_vmpressure(memcg);

    /* Here we only want to account pressure that userland is able to
    * help us with. For example, suppose that DMA zone is under
    * pressure; if we notify userland about that kind of pressure,
    * then it will be mostly a waste as it will trigger unnecessary
    * freeing of memory by userland (since userland is more likely to
    * have HIGHMEM/MOVABLE pages instead of the DMA fallback). That
    * is why we include only movable, highmem and FS/IO pages.
    * Indirect reclaim (kswapd) sets sc->gfp_mask to GFP_KERNEL, so
    * we account it too. */
    if (!(gfp & (__GFP_HIGHMEM | __GFP_MOVABLE | __GFP_IO | __GFP_FS)))
        return;

    /* If we got here with no pages scanned, then that is an indicator
    * that reclaimer was unable to find any shrinkable LRUs at the
    * current scanning depth. But it does not mean that we should
    * report the critical pressure, yet. If the scanning priority
    * (scanning depth) goes too high (deep), we will be notified
    * through vmpressure_prio(). But so far, keep calm. */
    if (!scanned)
        return;

    if (tree) {
        spin_lock(&vmpr->sr_lock);
        scanned = vmpr->tree_scanned += scanned;
        vmpr->tree_reclaimed += reclaimed;
        spin_unlock(&vmpr->sr_lock);

        if (scanned < vmpressure_win)
            return;
        schedule_work(&vmpr->work);
    } else {
        enum vmpressure_levels level;

        /* For now, no users for root-level efficiency */
        if (!memcg || mem_cgroup_is_root(memcg))
            return;

        spin_lock(&vmpr->sr_lock);
        scanned = vmpr->scanned += scanned;
        reclaimed = vmpr->reclaimed += reclaimed;
        if (scanned < vmpressure_win) {
            spin_unlock(&vmpr->sr_lock);
            return;
        }
        vmpr->scanned = vmpr->reclaimed = 0;
        spin_unlock(&vmpr->sr_lock);

        level = vmpressure_calc_level(scanned, reclaimed) {
            unsigned long scale = scanned + reclaimed;
            unsigned long pressure = 0;

            if (reclaimed >= scanned)
                goto out;

            pressure = scale - (reclaimed * scale / scanned);
            pressure = pressure * 100 / scale;

        out:
            return vmpressure_level(pressure) {
                if (pressure >= vmpressure_level_critical)
                    return VMPRESSURE_CRITICAL;
                else if (pressure >= vmpressure_level_med)
                    return VMPRESSURE_MEDIUM;
                return VMPRESSURE_LOW;
            }
        }

        if (level > VMPRESSURE_LOW) {
            WRITE_ONCE(memcg->socket_pressure, jiffies + HZ);
        }
    }
}
```

# page_compact

```sh
/proc/sys/vm/           # Virtual memory kernel parameters
├── compact_memory      # Trigger manual memory compaction (write-only: echo 1 to compact all zones)
├── extfrag_threshold   # Fragmentation threshold for compaction (1-1000; lower values increase compaction aggressiveness)
├── min_unmapped_ratio  # Minimum % of unmapped pages to trigger compaction (0-100; affects compaction trigger)
├── compact_unevictable_allowed # Allow compaction of unevictable pages (0 or 1; typically 1)
└── zone_reclaim_mode   # NUMA zone reclaim behavior (0, 1, 2, or 3; affects compaction on NUMA systems)

/proc/              # Virtual filesystem with compaction-related info
├── buddyinfo       # Buddy allocator stats (free memory blocks by order per zone; indicates fragmentation)
├── pagetypeinfo    # Page type and migration stats (free/used pages by order; useful for compaction analysis)
├── zoneinfo        # Per-memory-zone stats (free pages, min/free/high watermarks, affects compaction)

/sys/devices/system/node/   # NUMA node-specific compaction info
├── node[0-N]/      # Per-NUMA node directories
│   ├── compact     # Trigger compaction for this node (write-only: echo 1)
│   └── vmstat      # Node-specific VM stats (includes compaction-related counters like compact_migrate_scanned)
```

![](../images/kernel/mem-page_compact.svg)

* [OPPO内核工匠 - Linux内核内存规整详解](https://mp.weixin.qq.com/s/Ts7yGSuTrh3JLMnP4E3ajA)

---

**Compaction Eligible Page Types**
1. Anonymous Pages: Process-private memory (e.g., heap, stack).
2. File-Backed Pages (Page Cache): Pages caching file data (mapping to address_space).
3. Swap-Cached Pages: Pages in swap cache (PG_swapcache), backed by swap.
4. Compound Pages (THP, Hugetlbfs): Large pages (e.g., 2MB THP).

**Compaction Ineligible Page Types**
* Kernel Pages (Slab): PG_slab pages (e.g., kmalloc() buffers) are non-movable.
* Locked Pages: PG_mlocked (via mlock()) or PG_locked (in I/O).
* Unevictable Pages: PG_unevictable (e.g., mlocked, kernel-reserved).
* Pinned Pages: DMA or device-mapped pages (PG_uncached).
* Free Pages: Already in buddy allocator, not migrated.
* Hardware Poisoned: PG_hwpoison pages with errors.

---

| A | B |
| - | - |
| ![](../images/kernel/mem-page_compact-points.png) | ![](../images/kernel/mem-page_compact-cases.png) |

* **MIGRATE_ASYNC** means never block
* **MIGRATE_SYNC_LIGHT** allow blocking on most operations but not ->writepage as the potential stall time is too significant
* **MIGRATE_SYNC** will block when migrating pages

---

| Aspect | Dirty Pages | Writable Pages |
| :-: | :-: | :-: |
| Definition | Modified in memory, not synced to disk. | Pages with write permission (dirty or clean). |
| State | Always writable and modified. | May be clean or dirty. |
| Migration Handling | May require writeback or special care. | Requires PTE updates, locking; simpler if clean. |
| Compaction Impact | Skipped in MIGRATE_ASYNC if complex.  | Movable if locked, regardless of dirtiness. |
| Examples | Modified file pages, swapped anon pages. | Process heap, stack, or file mappings. |

```c
struct zone {
    /* pfn where compaction free scanner should start */
    unsigned long       compact_cached_free_pfn;
    /* pfn where compaction migration scanner should start */
    unsigned long       compact_cached_migrate_pfn[ASYNC_AND_SYNC];
    unsigned long       compact_init_migrate_pfn;
    unsigned long       compact_init_free_pfn;

    unsigned int        compact_considered;
    unsigned int        compact_defer_shift;
    int                 compact_order_failed;

    /* Set to true when the PG_migrate_skip bits should be cleared */
    bool                compact_blockskip_flush;
};

struct compact_control {
    struct list_head freepages[NR_PAGE_ORDERS]; /* List of free pages to migrate to */
    struct list_head migratepages;  /* List of pages being migrated */
    unsigned int nr_freepages;      /* Number of isolated free pages */
    unsigned int nr_migratepages;   /* Number of pages to migrate */
    unsigned long free_pfn;         /* isolate_freepages search base */

    /* Acts as an in/out parameter to page isolation for migration.
     * isolate_migratepages uses it as a search base.
     * isolate_migratepages_block will update the value to the next pfn
     * after the last isolated one. */
    unsigned long migrate_pfn;
    unsigned long fast_start_pfn;   /* a pfn to start linear scan from */
    struct zone *zone;
    unsigned long total_migrate_scanned;
    unsigned long total_free_scanned;
    unsigned short fast_search_fail;/* failures to use free list searches */
    short search_order;             /* order to start a fast search at */
    const gfp_t gfp_mask;           /* gfp mask of a direct compactor */
    int order;                      /* order only set by direct/async compactor */
    int migratetype;                /* migratetype of direct compactor */
    const unsigned int alloc_flags; /* alloc flags of a direct compactor */
    const int highest_zoneidx;      /* zone index of a direct compactor */
    enum migrate_mode mode;         /* Async or sync migration mode */
    bool ignore_skip_hint;          /* Scan blocks even if marked skip */
    bool no_set_skip_hint;          /* Don't mark blocks for skipping */
    bool ignore_block_suitable;     /* Scan blocks considered unsuitable */
    bool direct_compaction;         /* False from kcompactd or /proc/... */
    bool proactive_compaction;      /* kcompactd proactive compaction */
    bool whole_zone;                /* Whole zone should/has been scanned */
    bool contended;                 /* Signal lock contention */
    bool finish_pageblock;          /* Scan the remainder of a pageblock. Used
                    * when there are potentially transient
                    * isolation or migration failures to
                    * ensure forward progress. */
    bool alloc_contig;              /* alloc_contig_range allocation */
};
```

```c
status = compact_zone_order(zone, order, gfp_mask, prio, alloc_flags, ac->highest_zoneidx, capture) {
    enum compact_result ret;
    struct compact_control cc = {
        .order = order, /* order required for compaction */
        .search_order = order, /* free pages search order from high to low */
        .gfp_mask = gfp_mask, /* compact file pages for __GFP_FS */
        .zone = zone,
        /* not compact dirty and writing page for MIGRATE_ASYNC & MIGRATE_SYNC_LIGHT */
        .mode = (prio == COMPACT_PRIO_ASYNC) ? MIGRATE_ASYNC : MIGRATE_SYNC_LIGHT,
        .alloc_flags = alloc_flags,
        .highest_zoneidx = highest_zoneidx,
        .direct_compaction = true,
        .whole_zone = (prio == MIN_COMPACT_PRIORITY),
        .ignore_skip_hint = (prio == MIN_COMPACT_PRIORITY),
        .ignore_block_suitable = (prio == MIN_COMPACT_PRIORITY)
    };
    struct capture_control capc = {
        .cc = &cc,
        .page = NULL, /* call compaction_capture to capture the page when __free_one_page */
    };

    WRITE_ONCE(current->capture_control, &capc);

    ret = compact_zone(&cc, &capc) {
        enum compact_result ret;
        unsigned long start_pfn = cc->zone->zone_start_pfn;
        unsigned long end_pfn = zone_end_pfn(cc->zone);
        unsigned long last_migrated_pfn;
        const bool sync = cc->mode != MIGRATE_ASYNC;
        bool update_cached;
        unsigned int nr_succeeded = 0;

        cc->total_migrate_scanned = 0;
        cc->total_free_scanned = 0;
        cc->nr_migratepages = 0;
        cc->nr_freepages = 0;
        INIT_LIST_HEAD(&cc->freepages);
        INIT_LIST_HEAD(&cc->migratepages);

        cc->migratetype = gfp_migratetype(cc->gfp_mask);

        if (!is_via_compact_memory(cc->order)) { /* return order == -1; */
            ret = compaction_suit_allocation_order(
                cc->zone, cc->order,
                cc->highest_zoneidx,
                cc->alloc_flags
            );
            if (ret != COMPACT_CONTINUE)
                return ret;
        }

        if (compaction_restarting(cc->zone, cc->order))
            __reset_isolation_suitable(cc->zone);

        cc->fast_start_pfn = 0;
        if (cc->whole_zone) {
            cc->migrate_pfn = start_pfn;
            cc->free_pfn = pageblock_start_pfn(end_pfn - 1);
        } else {
            cc->migrate_pfn = cc->zone->compact_cached_migrate_pfn[sync];
            cc->free_pfn = cc->zone->compact_cached_free_pfn;
            if (cc->free_pfn < start_pfn || cc->free_pfn >= end_pfn) {
                cc->free_pfn = pageblock_start_pfn(end_pfn - 1);
                cc->zone->compact_cached_free_pfn = cc->free_pfn;
            }
            if (cc->migrate_pfn < start_pfn || cc->migrate_pfn >= end_pfn) {
                cc->migrate_pfn = start_pfn;
                cc->zone->compact_cached_migrate_pfn[0] = cc->migrate_pfn;
                cc->zone->compact_cached_migrate_pfn[1] = cc->migrate_pfn;
            }

            if (cc->migrate_pfn <= cc->zone->compact_init_migrate_pfn)
                cc->whole_zone = true;
        }

        last_migrated_pfn = 0;

        /* Migrate has separate cached PFNs for ASYNC and SYNC* migration on
        * the basis that some migrations will fail in ASYNC mode. However,
        * if the cached PFNs match and pageblocks are skipped due to having
        * no isolation candidates, then the sync state does not matter.
        * Until a pageblock with isolation candidates is found, keep the
        * cached PFNs in sync to avoid revisiting the same blocks. */
        update_cached = !sync &&
            cc->zone->compact_cached_migrate_pfn[0] == cc->zone->compact_cached_migrate_pfn[1];

        /* lru_add_drain_all could be expensive with involving other CPUs */
        lru_add_drain();

        while ((ret = compact_finished(cc)) == COMPACT_CONTINUE) {
            int err;
            unsigned long iteration_start_pfn = cc->migrate_pfn;

            /* Avoid multiple rescans of the same pageblock which can
            * happen if a page cannot be isolated (dirty/writeback in
            * async mode) or if the migrated pages are being allocated
            * before the pageblock is cleared.  The first rescan will
            * capture the entire pageblock for migration. If it fails,
            * it'll be marked skip and scanning will proceed as normal. */
            cc->finish_pageblock = false;
            if (pageblock_start_pfn(last_migrated_pfn) == pageblock_start_pfn(iteration_start_pfn)) {
                cc->finish_pageblock = true;
            }

    rescan:
            switch (isolate_migratepages(cc)) {
            case ISOLATE_ABORT:
                ret = COMPACT_CONTENDED;
                putback_movable_pages(&cc->migratepages);
                cc->nr_migratepages = 0;
                goto out;
            case ISOLATE_NONE:
                if (update_cached) {
                    cc->zone->compact_cached_migrate_pfn[1] =
                        cc->zone->compact_cached_migrate_pfn[0];
                }

                /* We haven't isolated and migrated anything, but
                * there might still be unflushed migrations from
                * previous cc->order aligned block. */
                goto check_drain;
            case ISOLATE_SUCCESS:
                update_cached = false;
                last_migrated_pfn = max(cc->zone->zone_start_pfn,
                    pageblock_start_pfn(cc->migrate_pfn - 1));
            }

            err = migrate_pages(&cc->migratepages, compaction_alloc,
                    compaction_free, (unsigned long)cc, cc->mode,
                    MR_COMPACTION, &nr_succeeded);

            /* All pages were either migrated or will be released */
            cc->nr_migratepages = 0;
            if (err) {
                putback_movable_pages(&cc->migratepages);
                /* migrate_pages() may return -ENOMEM when scanners meet
                * and we want compact_finished() to detect it */
                if (err == -ENOMEM && !compact_scanners_met(cc)) {
                    ret = COMPACT_CONTENDED;
                    goto out;
                }
                /* If an ASYNC or SYNC_LIGHT fails to migrate a page
                * within the pageblock_order-aligned block and
                * fast_find_migrateblock may be used then scan the
                * remainder of the pageblock. This will mark the
                * pageblock "skip" to avoid rescanning in the near
                * future. This will isolate more pages than necessary
                * for the request but avoid loops due to
                * fast_find_migrateblock revisiting blocks that were
                * recently partially scanned. */
                if (!pageblock_aligned(cc->migrate_pfn) &&
                    !cc->ignore_skip_hint && !cc->finish_pageblock &&
                    (cc->mode < MIGRATE_SYNC)) {
                    cc->finish_pageblock = true;

                    /* Draining pcplists does not help THP if
                    * any page failed to migrate. Even after
                    * drain, the pageblock will not be free. */
                    if (cc->order == COMPACTION_HPAGE_ORDER)
                        last_migrated_pfn = 0;

                    goto rescan;
                }
            }

            /* Stop if a page has been captured */
            if (capc && capc->page) {
                ret = COMPACT_SUCCESS;
                break;
            }

    check_drain:
            /* Has the migration scanner moved away from the previous
            * cc->order aligned block where we migrated from? If yes,
            * flush the pages that were freed, so that they can merge and
            * compact_finished() can detect immediately if allocation
            * would succeed. */
            if (cc->order > 0 && last_migrated_pfn) {
                unsigned long current_block_start =
                    block_start_pfn(cc->migrate_pfn, cc->order);

                if (last_migrated_pfn < current_block_start) {
                    lru_add_drain_cpu_zone(cc->zone);
                    /* No more flushing until we migrate again */
                    last_migrated_pfn = 0;
                }
            }
        }

    out:
        /* Release free pages and update where the free scanner should restart,
         * so we don't leave any returned pages behind in the next attempt. */
        if (cc->nr_freepages > 0) {
            unsigned long free_pfn = release_freepages(&cc->freepages);

            cc->nr_freepages = 0;
            VM_BUG_ON(free_pfn == 0);
            /* The cached pfn is always the first in a pageblock */
            free_pfn = pageblock_start_pfn(free_pfn);
            /* Only go back, not forward. The cached pfn might have been
            * already reset to zone end in compact_finished() */
            if (free_pfn > cc->zone->compact_cached_free_pfn)
                cc->zone->compact_cached_free_pfn = free_pfn;
        }

        return ret;
    }

    WRITE_ONCE(current->capture_control, NULL);
    *capture = READ_ONCE(capc.page);
    if (*capture)
        ret = COMPACT_SUCCESS;

    return ret;
}
```

## compact_finished

![](../images/kernel/mem-page_compact-finished.png)

```c
enum compact_result compact_finished(struct compact_control *cc)
{
    int ret;

    ret = __compact_finished(cc) {
        unsigned int order;
        const int migratetype = cc->migratetype;
        int ret;

/* 1. Compaction run completes if the migrate and free scanner meet */
        if (compact_scanners_met(cc)) {
            reset_cached_positions(cc->zone) {
                zone->compact_cached_migrate_pfn[0] = zone->zone_start_pfn;
                zone->compact_cached_migrate_pfn[1] = zone->zone_start_pfn;
                zone->compact_cached_free_pfn = pageblock_start_pfn(zone_end_pfn(zone) - 1);
            }

            /* Mark that the PG_migrate_skip information should be cleared
             * by kswapd when it goes to sleep. kcompactd does not set the
             * flag itself as the decision to be clear should be directly
             * based on an allocation request. */
            if (cc->direct_compaction)
                cc->zone->compact_blockskip_flush = true;

            return (cc->whole_zone) ? COMPACT_COMPLETE : COMPACT_PARTIAL_SKIPPED;
        }

/* 2. proactive mode: fragementation score under wmark */
        if (cc->proactive_compaction) {
            int score, wmark_low;
            pg_data_t *pgdat;

            pgdat = cc->zone->zone_pgdat;
            if (kswapd_is_running(pgdat))
                return COMPACT_PARTIAL_SKIPPED;

              score = fragmentation_score_zone(cc->zone);
                --->
            wmark_low = fragmentation_score_wmark(true);
                --->
            if (score > wmark_low)
                ret = COMPACT_CONTINUE;
            else
                ret = COMPACT_SUCCESS;

            goto out;
        }

        /* order == -1 is expected when compacting proactively via
         * 1. /proc/sys/vm/compact_memory
         * 2. /sys/devices/system/node/nodex/compact
         * 3. /proc/sys/vm/compaction_proactiveness */
        if (is_via_compact_memory(cc->order)) /* return cc-order == -1; */
            return COMPACT_CONTINUE;

        /* Always finish scanning a pageblock to reduce the possibility of
         * fallbacks in the future. This is particularly important when
         * migration source is unmovable/reclaimable but it's not worth
         * special casing. */
        if (!pageblock_aligned(cc->migrate_pfn))
            return COMPACT_CONTINUE;

/* 3. Direct compactor */
        ret = COMPACT_NO_SUITABLE_PAGE;
        for (order = cc->order; order < NR_PAGE_ORDERS; order++) {
            struct free_area *area = &cc->zone->free_area[order];
            bool can_steal;

            /* 3.1 Job done if page is free of the right migratetype */
            if (!free_area_empty(area, migratetype))
                return COMPACT_SUCCESS;

            /* 3.2 MIGRATE_MOVABLE can fallback on MIGRATE_CMA */
            if (migratetype == MIGRATE_MOVABLE && !free_area_empty(area, MIGRATE_CMA)) {
                return COMPACT_SUCCESS;
            }

            /* 3.3 Job done if allocation would steal freepages from
             * other migratetype buddy lists. */
            if (find_suitable_fallback(area, order, migratetype, true, &can_steal) != -1) {
                return COMPACT_SUCCESS;
            }
        }

    out:
        if (cc->contended || fatal_signal_pending(current))
            ret = COMPACT_CONTENDED;

        return ret;
    }

    if (ret == COMPACT_NO_SUITABLE_PAGE)
        ret = COMPACT_CONTINUE;

    return ret;
}
```

## isolate_migratepages

```c
isolate_migrate_t isolate_migratepages(struct compact_control *cc)
{
    unsigned long block_start_pfn;
    unsigned long block_end_pfn;
    unsigned long low_pfn;
    struct page *page;
    const isolate_mode_t isolate_mode =
        (sysctl_compact_unevictable_allowed ? ISOLATE_UNEVICTABLE : 0)
        | (cc->mode != MIGRATE_SYNC ? ISOLATE_ASYNC_MIGRATE : 0);
    bool fast_find_block;

    low_pfn = fast_find_migrateblock(cc);
    block_start_pfn = max(pageblock_start_pfn(low_pfn), cc->zone->zone_start_pfn);
    fast_find_block = low_pfn != cc->migrate_pfn && !cc->fast_search_fail;
    block_end_pfn = pageblock_end_pfn(low_pfn);

    for (; block_end_pfn <= cc->free_pfn;
        fast_find_block = false,
        cc->migrate_pfn = low_pfn = block_end_pfn,
        block_start_pfn = block_end_pfn,
        block_end_pfn += pageblock_nr_pages) {

        if (!(low_pfn % (COMPACT_CLUSTER_MAX * pageblock_nr_pages)))
            cond_resched();

        page = pageblock_pfn_to_page(block_start_pfn, block_end_pfn, cc->zone);
        if (!page) {
            unsigned long next_pfn;
            next_pfn = skip_offline_sections(block_start_pfn);
            if (next_pfn)
                block_end_pfn = min(next_pfn, cc->free_pfn);
            continue;
        }

        /* If isolation recently failed, do not retry. Only check the
         * pageblock once. COMPACT_CLUSTER_MAX causes a pageblock
         * to be visited multiple times. Assume skip was checked
         * before making it "skip" so other compaction instances do
         * not scan the same block. */
        bool isolation_suitable(struct compact_control *cc, struct page *page) {
            if (cc->ignore_skip_hint)
                return true;
            return !get_pageblock_skip(page);
        }
        if ((pageblock_aligned(low_pfn) || low_pfn == cc->zone->zone_start_pfn)
            && !fast_find_block && !isolation_suitable(cc, page)) {

            continue;
        }

        ret = suitable_migration_source(cc, page) {
            int block_mt;

            if (pageblock_skip_persistent(page))
                return false;
            if ((cc->mode != MIGRATE_ASYNC) || !cc->direct_compaction)
                return true;

            block_mt = get_pageblock_migratetype(page);

            if (cc->migratetype == MIGRATE_MOVABLE) {
                return is_migrate_movable(block_mt) {
                    return is_migrate_cma(mt) || mt == MIGRATE_MOVABLE;
                }
            } else {
                return block_mt == cc->migratetype;
            }
        }
        if (!ret) {
            update_cached_migrate(cc, block_end_pfn) {
                pfn = pageblock_end_pfn(pfn);
                /* Update where async and sync compaction should restart */
                if (pfn > zone->compact_cached_migrate_pfn[0])
                    zone->compact_cached_migrate_pfn[0] = pfn;
                if (cc->mode != MIGRATE_ASYNC && pfn > zone->compact_cached_migrate_pfn[1])
                    zone->compact_cached_migrate_pfn[1] = pfn;
            }
            continue;
        }

        ret = isolate_migratepages_block(cc, low_pfn, block_end_pfn, isolate_mode);
        if (ret) {
            return ISOLATE_ABORT;
        }

        /* Either we isolated something and proceed with migration. Or
         * we failed and compact_zone should decide if we should
         * continue or not. */
        break;
    }

    return cc->nr_migratepages ? ISOLATE_SUCCESS : ISOLATE_NONE;
}
```

### fast_find_migrateblock

1. If the target order is too small, then there's absolutely no need to search. Still, use cc->migrate_pfn as the starting pfn.
2. Since there's a requirement for cc->order, it's only applicable to `direct memory` and `async memory` compaction (only these two scenarios specify cc->order; for other scenarios, cc->order is -1).
3. The pageblock where the free page is located must be within the 1/2 or 1/8 of the memory search range. This part is the most likely area to be scanned by migrate scanner, to avoid affecting the scanning of free pages.
4. Scanning for free pages will alter the layout of the freelist, so try to ensure that the free list isn't scanned for free memory blocks repeatedly in the next scan.
5. If cc->ignore_skip_hint is set, the migrate scanner won't use the fast mechanism for migrating pages.
6. Only search for movable free pages, and the search range starts from order - 1.

```c
unsigned long fast_find_migrateblock(struct compact_control *cc)
{
    unsigned int limit = freelist_scan_limit(cc) {
        unsigned short shift = BITS_PER_LONG - 1;
        return (COMPACT_CLUSTER_MAX >> min(shift, cc->fast_search_fail)) + 1;
    }
    unsigned int nr_scanned = 0;
    unsigned long distance;
    unsigned long pfn = cc->migrate_pfn;
    unsigned long high_pfn;
    int order;
    bool found_block = false;

/* 1. Initialization and Early Checks: */

    /* Skip hints are relied on to avoid repeats on the fast search */
    if (cc->ignore_skip_hint)
        return pfn;

    /* If the pageblock should be finished then do not select a different pageblock. */
    if (cc->finish_pageblock)
        return pfn;

    /* continus scan:
    * If the migrate_pfn is not at the start of a zone or the start
    * of a pageblock then assume this is a continuation of a previous
    * scan restarted due to COMPACT_CLUSTER_MAX. */
    if (pfn != cc->zone->zone_start_pfn && pfn != pageblock_start_pfn(pfn))
        return pfn;

/* 2. Small Order Optimization:
    * For smaller orders, just linearly scan as the number of pages
    * to migrate should be relatively small and does not necessarily
    * justify freeing up a large block for a small allocation. */
    if (cc->order <= PAGE_ALLOC_COSTLY_ORDER)
        return pfn;

/* 3. Direct Compaction Restriction
    * Only allow kcompactd and direct requests for movable pages to
    * quickly clear out a MOVABLE pageblock for allocation. This
    * reduces the risk that a large movable pageblock is freed for
    * an unmovable/reclaimable small allocation. */
    if (cc->direct_compaction && cc->migratetype != MIGRATE_MOVABLE)
        return pfn;

/* 4. Search Space Determination:
    * When starting the migration scanner, pick any pageblock within the
    * first half of the search space. Otherwise try and pick a pageblock
    * within the first eighth to reduce the chances that a migration
    * target later becomes a source. */
    distance = (cc->free_pfn - cc->migrate_pfn) >> 1;
    if (cc->migrate_pfn != cc->zone->zone_start_pfn)
        distance >>= 2;
    high_pfn = pageblock_start_pfn(cc->migrate_pfn + distance);

/* 5. Fast Search Loop: */
    for (order = cc->order - 1;
        order >= PAGE_ALLOC_COSTLY_ORDER && !found_block && nr_scanned < limit;
        order--) {

        struct free_area *area = &cc->zone->free_area[order];
        struct list_head *freelist;
        unsigned long flags;
        struct page *freepage;

        if (!area->nr_free)
            continue;

        spin_lock_irqsave(&cc->zone->lock, flags);
        freelist = &area->free_list[MIGRATE_MOVABLE];

        list_for_each_entry(freepage, freelist, buddy_list) {
            unsigned long free_pfn;

            if (nr_scanned++ >= limit) {
                move_freelist_tail(freelist, freepage);
                break;
            }

            free_pfn = page_to_pfn(freepage);
            if (free_pfn < high_pfn) {
                if (get_pageblock_skip(freepage))
                    continue;

                /* Reorder to so a future search skips recent pages */
                move_freelist_tail(freelist, freepage);

                update_fast_start_pfn(cc, free_pfn) {
                    if (cc->fast_start_pfn == ULONG_MAX)
                        return;
                    if (!cc->fast_start_pfn)
                        cc->fast_start_pfn = pfn;
                    cc->fast_start_pfn = min(cc->fast_start_pfn, pfn);
                }
                pfn = pageblock_start_pfn(free_pfn);
                if (pfn < cc->zone->zone_start_pfn)
                    pfn = cc->zone->zone_start_pfn;
                cc->fast_search_fail = 0;
                found_block = true;
                break;
            }
        }
        spin_unlock_irqrestore(&cc->zone->lock, flags);
    }

    cc->total_migrate_scanned += nr_scanned;

    if (!found_block) {
        cc->fast_search_fail++;
        pfn = reinit_migrate_pfn(cc) {
            if (!cc->fast_start_pfn || cc->fast_start_pfn == ULONG_MAX)
                return cc->migrate_pfn;
            cc->migrate_pfn = cc->fast_start_pfn;
            cc->fast_start_pfn = ULONG_MAX;

            return cc->migrate_pfn;
        }
    }
    return pfn;
}
```

### isolate_migratepages_block

1. Not isolate **huge pages**, but **alloc_contig_range** may dissolve hugetlbfs
2. Not isolate **free buddy pages**
3. **non-LRU** page allocated for kernel usage, can be isolated if the developer impelements isolate_page, migratepage and putback_page operations
4. Not isolate **pinned pages**
5. Not isolate **file-mapped page** on GFP_NOFS
6. Isolation affected by policy: ISOLATE_ASYNC_MIGRATE wont isolate dirty pages and writing pages
7. **Direct && Async** compact will skip the whole order-aligned page block on any failure within the block and put back any isolated pages. This behavior is contorled by `skip_on_failur` local varible.

```c
int isolate_migratepages_block(struct compact_control *cc, unsigned long low_pfn,
            unsigned long end_pfn, isolate_mode_t mode)
{
    pg_data_t *pgdat = cc->zone->zone_pgdat;
    unsigned long nr_scanned = 0, nr_isolated = 0;
    struct lruvec *lruvec;
    unsigned long flags = 0;
    struct lruvec *locked = NULL;
    struct folio *folio = NULL;
    struct page *page = NULL, *valid_page = NULL;
    struct address_space *mapping;
    unsigned long start_pfn = low_pfn;
    /* skip to the end of the current order-aligned block
     * if isolating a page fails, rather than continuing PFN-by-PFN within that block. */
    bool skip_on_failure = false;
    unsigned long next_skip_pfn = 0;
    bool skip_updated = false;
    int ret = 0;

    cc->migrate_pfn = low_pfn;

    while (unlikely(too_many_isolated(cc))) {
        if (cc->nr_migratepages)
            return -EAGAIN;
        if (cc->mode == MIGRATE_ASYNC)
            return -EAGAIN;
        reclaim_throttle(pgdat, VMSCAN_THROTTLE_ISOLATED);

        if (fatal_signal_pending(current))
            return -EINTR;
    }

    cond_resched();

    if (cc->direct_compaction && (cc->mode == MIGRATE_ASYNC)) {
        skip_on_failure = true;
        next_skip_pfn = block_end_pfn(low_pfn, cc->order);
    }

    /* Time to isolate some pages for migration */
    for (; low_pfn < end_pfn; low_pfn++) {
        bool is_dirty, is_unevictable;

        if (skip_on_failure && low_pfn >= next_skip_pfn) {
            /* We have isolated all migration candidates in the
             * previous order-aligned block, and did not skip it due
             * to failure. We should migrate the pages now and
             * hopefully succeed compaction. */
            if (nr_isolated)
                break;

            /* We failed to isolate in the previous order-aligned
             * block. Set the new boundary to the end of the
             * current block. Note we can't simply increase
             * next_skip_pfn by 1 << order, as low_pfn might have
             * been incremented by a higher number due to skipping
             * a compound or a high-order buddy page in the
             * previous loop iteration. */
            next_skip_pfn = block_end_pfn(low_pfn, cc->order);
        }

        /* Periodically drop the lock (if held) regardless of its
         * contention, to give chance to IRQs. Abort completely if
         * a fatal signal is pending. */
        if (!(low_pfn % COMPACT_CLUSTER_MAX)) {
            if (locked) {
                unlock_page_lruvec_irqrestore(locked, flags);
                locked = NULL;
            }

            if (fatal_signal_pending(current)) {
                cc->contended = true;
                ret = -EINTR;

                goto fatal_pending;
            }

            cond_resched();
        }

        nr_scanned++;

        page = pfn_to_page(low_pfn);

        /* Check if the pageblock has already been marked skipped.
         * Only the first PFN is checked as the caller isolates
         * COMPACT_CLUSTER_MAX at a time so the second call must
         * not falsely conclude that the block should be skipped. */
        if (!valid_page && (pageblock_aligned(low_pfn) || low_pfn == cc->zone->zone_start_pfn)) {
            if (!isolation_suitable(cc, page)) {
                low_pfn = end_pfn;
                folio = NULL;
                goto isolate_abort;
            }
            valid_page = page;
        }

        if (PageHuge(page)) {
            const unsigned int order = compound_order(page);
            /* skip hugetlbfs if we are not compacting for pages
            * bigger than its order. THPs and other compound pages
            * are handled below. */
            if (!cc->alloc_contig) {

                if (order <= MAX_PAGE_ORDER) {
                    low_pfn += (1UL << order) - 1;
                    nr_scanned += (1UL << order) - 1;
                }
                goto isolate_fail;
            }
            /* for alloc_contig case */
            if (locked) {
                unlock_page_lruvec_irqrestore(locked, flags);
                locked = NULL;
            }

            folio = page_folio(page);
            ret = isolate_or_dissolve_huge_folio(folio, &cc->migratepages) {
                int ret = -EBUSY;

                /* Not to disrupt normal path by vainly holding hugetlb_lock */
                if (!folio_test_hugetlb(folio))
                    return 0;

                /* Fence off gigantic pages as there is a cyclic dependency between
                * alloc_contig_range and them. Return -ENOMEM as this has the effect
                * of bailing out right away without further retrying. */
                if (folio_order(folio) > MAX_PAGE_ORDER)
                    return -ENOMEM;

                if (folio_ref_count(folio) && folio_isolate_hugetlb(folio, list))
                    ret = 0;
                else if (!folio_ref_count(folio))
                    ret = alloc_and_dissolve_hugetlb_folio(folio, list);
                        --->

                return ret;
            }

            /* Fail isolation in case isolate_or_dissolve_huge_folio()
            * reports an error. In case of -ENOMEM, abort right away. */
            if (ret < 0) {
                /* Do not report -EBUSY down the chain */
                if (ret == -EBUSY)
                    ret = 0;
                low_pfn += (1UL << order) - 1;
                nr_scanned += (1UL << order) - 1;
                goto isolate_fail;
            }

            if (folio_test_hugetlb(folio)) {
                /* Hugepage was successfully isolated and placed
                * on the cc->migratepages list. */
                low_pfn += folio_nr_pages(folio) - 1;
                goto isolate_success_no_list;
            }

            /* Ok, the hugepage was dissolved. Now these pages are
            * Buddy and cannot be re-allocated because they are
            * isolated. Fall-through as the check below handles
            * Buddy pages. */
        }

        if (PageBuddy(page)) {
            unsigned long freepage_order = buddy_order_unsafe(page);

            if (freepage_order > 0 && freepage_order <= MAX_PAGE_ORDER) {
                low_pfn += (1UL << freepage_order) - 1;
                nr_scanned += (1UL << freepage_order) - 1;
            }
            continue;
        }

        if (PageCompound(page) && !cc->alloc_contig) {
            const unsigned int order = compound_order(page);
            if (likely(order <= MAX_PAGE_ORDER)) {
                low_pfn += (1UL << order) - 1;
                nr_scanned += (1UL << order) - 1;
            }
            goto isolate_fail;
        }

        if (!PageLRU(page)) {
            /* non-LRU but page->mapping & PAGE_MAPPING_MOVABLE page */
            if (unlikely(__PageMovable(page)) && !PageIsolated(page)) {
                if (locked) {
                    unlock_page_lruvec_irqrestore(locked, flags);
                    locked = NULL;
                }

                if (isolate_movable_page(page, mode)) {
                    folio = page_folio(page);
                    goto isolate_success;
                }
            }

            goto isolate_fail;
        }

        folio = folio_get_nontail_page(page);
        if (unlikely(!folio))
            goto isolate_fail;

        mapping = folio_mapping(folio) {
            struct address_space *mapping;

            /* This happens if someone calls flush_dcache_page on slab page */
            if (unlikely(folio_test_slab(folio)))
                return NULL;

            if (unlikely(folio_test_swapcache(folio)))
                return swap_address_space(folio->swap);

            mapping = folio->mapping;
            if ((unsigned long)mapping & PAGE_MAPPING_FLAGS)
                return NULL;

            return mapping;
        }
        /* an anonymous page is pinned in memory */
        if (!mapping && (folio_ref_count(folio) - 1) > folio_mapcount(folio))
            goto isolate_fail_put;

        /* Only allow to migrate anonymous pages in GFP_NOFS context
         * because those do not depend on fs locks. */
        if (!(cc->gfp_mask & __GFP_FS) && mapping)
            goto isolate_fail_put;

        /* Only take pages on LRU: a check now makes later tests safe */
        if (!folio_test_lru(folio))
            goto isolate_fail_put;

        is_unevictable = folio_test_unevictable(folio);

        /* Compaction might skip unevictable pages but CMA takes them */
        if (!(mode & ISOLATE_UNEVICTABLE) && is_unevictable)
            goto isolate_fail_put;

        if ((mode & ISOLATE_ASYNC_MIGRATE) && folio_test_writeback(folio))
            goto isolate_fail_put;

        is_dirty = folio_test_dirty(folio);

        if (((mode & ISOLATE_ASYNC_MIGRATE) && is_dirty) || (mapping && is_unevictable)) {
            bool migrate_dirty = true;
            bool is_unmovable;

            if (!folio_trylock(folio))
                goto isolate_fail_put;

            mapping = folio_mapping(folio);
            if ((mode & ISOLATE_ASYNC_MIGRATE) && is_dirty) {
                migrate_dirty = !mapping || mapping->a_ops->migrate_folio;
            }
            is_unmovable = mapping && mapping_unmovable(mapping);
            folio_unlock(folio);
            if (!migrate_dirty || is_unmovable)
                goto isolate_fail_put;
        }

        if (!folio_test_clear_lru(folio))
            goto isolate_fail_put;

        lruvec = folio_lruvec(folio);

        /* If we already hold the lock, we can skip some rechecking */
        if (lruvec != locked) {
            if (locked) {
                unlock_page_lruvec_irqrestore(locked, flags);
            }

            compact_lock_irqsave(&lruvec->lru_lock, &flags, cc);
            locked = lruvec;

            /* Try get exclusive access under lock. If marked for
             * skip, the scan is aborted unless the current context
             * is a rescan to reach the end of the pageblock. */
            if (!skip_updated && valid_page) {
                skip_updated = true;
                if (test_and_set_skip(cc, valid_page) && !cc->finish_pageblock) {
                    low_pfn = end_pfn;
                    goto isolate_abort;
                }
            }

            /* folio become large since the non-locked check, and it's on LRU. */
            if (unlikely(folio_test_large(folio) && !cc->alloc_contig)) {
                low_pfn += folio_nr_pages(folio) - 1;
                nr_scanned += folio_nr_pages(folio) - 1;
                folio_set_lru(folio);
                goto isolate_fail_put;
            }
        }

        /* The folio is taken off the LRU */
        if (folio_test_large(folio))
            low_pfn += folio_nr_pages(folio) - 1;

        /* Successfully isolated */
        lruvec_del_folio(lruvec, folio);

isolate_success:
        list_add(&folio->lru, &cc->migratepages);

isolate_success_no_list:
        cc->nr_migratepages += folio_nr_pages(folio);
        nr_isolated += folio_nr_pages(folio);
        nr_scanned += folio_nr_pages(folio) - 1;

        /* Avoid isolating too much unless this block is being
        * fully scanned (e.g. dirty/writeback pages, parallel allocation)
        * or a lock is contended. For contention, isolate quickly to
        * potentially remove one source of contention. */
        if (cc->nr_migratepages >= COMPACT_CLUSTER_MAX
            && !cc->finish_pageblock && !cc->contended) {

            ++low_pfn;
            break;
        }

        continue;

isolate_fail_put:
        /* Avoid potential deadlock in freeing page under lru_lock */
        if (locked) {
            unlock_page_lruvec_irqrestore(locked, flags);
            locked = NULL;
        }
        folio_put(folio);

isolate_fail:
        /* skip the remainder of the page block if !skip_on_failure
         * which is !(direct_compact && MIGRATE_ASYNC) */
        if (!skip_on_failure && ret != -ENOMEM)
            continue;

        /* We have isolated some pages, but then failed. Release them
         * instead of migrating, as we cannot form the cc->order buddy
         * page anyway. */
        if (nr_isolated) {
            if (locked) {
                unlock_page_lruvec_irqrestore(locked, flags);
                locked = NULL;
            }
            putback_movable_pages(&cc->migratepages);
            cc->nr_migratepages = 0;
            nr_isolated = 0;
        }

        if (low_pfn < next_skip_pfn) {
            low_pfn = next_skip_pfn - 1;
            /* The check near the loop beginning would have updated
             * next_skip_pfn too, but this is a bit simpler. */
            next_skip_pfn += 1UL << cc->order;
        }

        if (ret == -ENOMEM)
            break;
    }

    /* The PageBuddy() check could have potentially brought us outside
     * the range to be scanned. */
    if (unlikely(low_pfn > end_pfn))
        low_pfn = end_pfn;

    folio = NULL;

isolate_abort:
    if (locked)
        unlock_page_lruvec_irqrestore(locked, flags);
    if (folio) {
        folio_set_lru(folio);
        folio_put(folio);
    }

    /* Update the cached scanner pfn once the pageblock has been scanned.
     * Pages will either be migrated in which case there is no point
     * scanning in the near future or migration failed in which case the
     * failure reason may persist. The block is marked for skipping if
     * there were no pages isolated in the block or if the block is
     * rescanned twice in a row. */
    if (low_pfn == end_pfn && (!nr_isolated || cc->finish_pageblock)) {
        if (!cc->no_set_skip_hint && valid_page && !skip_updated) {
            set_pageblock_skip(valid_page);
        }
        update_cached_migrate(cc, low_pfn);
    }

fatal_pending:
    cc->total_migrate_scanned += nr_scanned;
    if (nr_isolated)
        count_compact_events(COMPACTISOLATED, nr_isolated);

    cc->migrate_pfn = low_pfn;

    return ret;
}
```

## isolate_freepages

```c
void isolate_freepages(struct compact_control *cc)
{
    struct zone *zone = cc->zone;
    struct page *page;
    unsigned long block_start_pfn;    /* start of current pageblock */
    unsigned long isolate_start_pfn; /* exact pfn we start at */
    unsigned long block_end_pfn;    /* end of current pageblock */
    unsigned long low_pfn;         /* lowest pfn scanner is able to scan */
    struct list_head *freelist = &cc->freepages;
    unsigned int stride;

    /* Try a small search of the free lists for a candidate */
    fast_isolate_freepages(cc);
    if (cc->nr_freepages)
        goto splitmap;

    isolate_start_pfn = cc->free_pfn;
    block_start_pfn = pageblock_start_pfn(isolate_start_pfn);
    block_end_pfn = min(block_start_pfn + pageblock_nr_pages, zone_end_pfn(zone));
    low_pfn = pageblock_end_pfn(cc->migrate_pfn);
    stride = cc->mode == MIGRATE_ASYNC ? COMPACT_CLUSTER_MAX : 1;

    for (; block_start_pfn >= low_pfn;
        block_end_pfn = block_start_pfn,
        block_start_pfn -= pageblock_nr_pages,
        isolate_start_pfn = block_start_pfn) {

        unsigned long nr_isolated;

        if (!(block_start_pfn % (COMPACT_CLUSTER_MAX * pageblock_nr_pages)))
            cond_resched();

        page = pageblock_pfn_to_page(block_start_pfn, block_end_pfn, zone);
        if (!page) {
            unsigned long next_pfn;
            next_pfn = skip_offline_sections_reverse(block_start_pfn);
            if (next_pfn)
                block_start_pfn = max(next_pfn, low_pfn);
            continue;
        }

        ret = suitable_migration_target(cc, page) {
            if (PageBuddy(page)) {
                int order = cc->order > 0 ? cc->order : pageblock_order;
                if (buddy_order_unsafe(page) >= order)
                    return false;
            }

            if (cc->ignore_block_suitable)
                return true;
            if (is_migrate_movable(get_pageblock_migratetype(page)))
                return true;

            return false;
        }
        if (!ret)
            continue;

        if (!isolation_suitable(cc, page)) /* PB_migrate_skip */
            continue;

        /* Found a block suitable for isolating free pages from. */
        nr_isolated = isolate_freepages_block(cc, &isolate_start_pfn/*start_pfn*/,
            block_end_pfn, freelist, stride, false/*strict*/);

        /* Update the skip hint if the full pageblock was scanned */
        if (isolate_start_pfn == block_end_pfn)
            update_pageblock_skip(cc, page, block_start_pfn - pageblock_nr_pages);

        /* Are enough freepages isolated? */
        if (cc->nr_freepages >= cc->nr_migratepages) {
            if (isolate_start_pfn >= block_end_pfn) {
                /* Restart at previous pageblock if more
                 * freepages can be isolated next time. */
                isolate_start_pfn = block_start_pfn - pageblock_nr_pages;
            }
            break;
        } else if (isolate_start_pfn < block_end_pfn) {
            /* If isolation failed early, do not continue needlessly. */
            break;
        }

        /* Adjust stride depending on isolation */
        if (nr_isolated) {
            stride = 1;
            continue;
        }
        stride = min_t(unsigned int, COMPACT_CLUSTER_MAX, stride << 1);
    }

    cc->free_pfn = isolate_start_pfn;

splitmap:
    split_map_pages(freelist) {
        unsigned int i, order, nr_pages;
        struct page *page, *next;
        LIST_HEAD(tmp_list);

        list_for_each_entry_safe(page, next, list, lru) {
            list_del(&page->lru);

            order = page_private(page);
            nr_pages = 1 << order;

            post_alloc_hook(page, order, __GFP_MOVABLE);
            if (order) {
                split_page(page, order) {
                    for (i = 1; i < (1 << order); i++) {
                        set_page_refcounted(page + i);
                    }
                    split_page_owner(page, 1 << order);
                    split_page_memcg(page, 1 << order);
                }
            }

            for (i = 0; i < nr_pages; i++) {
                list_add(&page->lru, &tmp_list);
                page++;
            }
        }

        list_splice(&tmp_list, list);
    }
}
```

### fast_isolate_freepages

![](../images/kernel/mem-isolate_freepages-low-min-pfn.png)

```c
void fast_isolate_freepages(struct compact_control *cc)
{
    unsigned int limit = max(1U, freelist_scan_limit(cc) >> 1);
    unsigned int nr_scanned = 0, total_isolated = 0;
    unsigned long low_pfn, min_pfn, highest = 0;
    unsigned long nr_isolated = 0;
    unsigned long distance;
    struct page *page = NULL;
    bool scan_start = false;
    int order;

    /* Full compaction passes in a negative order */
    if (cc->order <= 0)
        return;

    /* If starting the scan, use a deeper search and use the highest
     * PFN found if a suitable one is not found. */
    if (cc->free_pfn >= cc->zone->compact_init_free_pfn) {
        limit = pageblock_nr_pages >> 1;
        scan_start = true;
    }

    /* Preferred point is in the top quarter (1/4) of the scan space but take
     * a pfn from the top half(1/2) if the search is problematic. */
    distance = (cc->free_pfn - cc->migrate_pfn);
    low_pfn = pageblock_start_pfn(cc->free_pfn - (distance >> 2));
    min_pfn = pageblock_start_pfn(cc->free_pfn - (distance >> 1));

    if (WARN_ON_ONCE(min_pfn > low_pfn))
        low_pfn = min_pfn;

    /* Search starts from the last successful isolation order or the next
     * order to search after a previous failure */
    cc->search_order = min_t(unsigned int, cc->order - 1, cc->search_order);

    for (order = cc->search_order;
        !page && order >= 0;
        order = next_search_order(cc, order)) {

        struct free_area *area = &cc->zone->free_area[order];
        struct list_head *freelist;
        struct page *freepage;
        unsigned long flags;
        unsigned int order_scanned = 0;
        unsigned long high_pfn = 0;

        if (!area->nr_free)
            continue;

        spin_lock_irqsave(&cc->zone->lock, flags);
        freelist = &area->free_list[MIGRATE_MOVABLE];

        list_for_each_entry_reverse(freepage, freelist, buddy_list) {
            unsigned long pfn;

            order_scanned++;
            nr_scanned++;
            pfn = page_to_pfn(freepage);

            if (pfn >= highest)
                highest = max(pageblock_start_pfn(pfn), cc->zone->zone_start_pfn);

            if (pfn >= low_pfn) {
                cc->fast_search_fail = 0;
                cc->search_order = order;
                page = freepage;
                break;
            }

            if (pfn >= min_pfn && pfn > high_pfn) {
                high_pfn = pfn;
                /* Shorten the scan if a candidate is found */
                limit >>= 1;
            }

            if (order_scanned >= limit)
                break;
        }

        /* Use a maximum candidate pfn if a preferred one was not found */
        if (!page && high_pfn) {
            page = pfn_to_page(high_pfn);
            /* Update freepage for the list reorder below */
            freepage = page;
        }

        /* Reorder to so a future search skips recent pages */
        move_freelist_head(freelist, freepage);

        if (page) {
            if (__isolate_free_page(page, order)) {
                set_page_private(page, order);
                nr_isolated = 1 << order;
                nr_scanned += nr_isolated - 1;
                total_isolated += nr_isolated;
                cc->nr_freepages += nr_isolated;
                list_add_tail(&page->lru, &cc->freepages);
                count_compact_events(COMPACTISOLATED, nr_isolated);
            } else {
                /* If isolation fails, abort the search */
                order = cc->search_order + 1;
                page = NULL;
            }
        }

        spin_unlock_irqrestore(&cc->zone->lock, flags);

        /* Skip fast search if enough freepages isolated */
        if (cc->nr_freepages >= cc->nr_migratepages)
            break;

        /* Smaller scan on next order so the total scan is related
        * to freelist_scan_limit. */
        if (order_scanned >= limit)
            limit = max(1U, limit >> 1);
    }

    if (!page) {
        cc->fast_search_fail++;
        if (scan_start) {
            /* Use the highest PFN found above min. If one was
            * not found, be pessimistic for direct compaction
            * and use the min mark. */
            if (highest >= min_pfn) {
                page = pfn_to_page(highest);
                cc->free_pfn = highest;
            } else {
                if (cc->direct_compaction && pfn_valid(min_pfn)) {
                    page = pageblock_pfn_to_page(min_pfn,
                        min(pageblock_end_pfn(min_pfn), zone_end_pfn(cc->zone)),
                        cc->zone
                    );
                    if (page && !suitable_migration_target(cc, page))
                        page = NULL;

                    cc->free_pfn = min_pfn;
                }
            }
        }
    }

    if (highest && highest >= cc->zone->compact_cached_free_pfn) {
        highest -= pageblock_nr_pages;
        cc->zone->compact_cached_free_pfn = highest;
    }

    cc->total_free_scanned += nr_scanned;
    if (!page)
        return;

    low_pfn = page_to_pfn(page);
    fast_isolate_around(cc, low_pfn) {
        unsigned long start_pfn, end_pfn;
        struct page *page;

        /* Do not search around if there are enough pages already */
        if (cc->nr_freepages >= cc->nr_migratepages)
            return;

        /* Minimise scanning during async compaction */
        if (cc->direct_compaction && cc->mode == MIGRATE_ASYNC)
            return;

        /* Pageblock boundaries */
        start_pfn = max(pageblock_start_pfn(pfn), cc->zone->zone_start_pfn);
        end_pfn = min(pageblock_end_pfn(pfn), zone_end_pfn(cc->zone));

        page = pageblock_pfn_to_page(start_pfn, end_pfn, cc->zone);
        if (!page)
            return;

        isolate_freepages_block(cc, &start_pfn, end_pfn, &cc->freepages, 1, false);

        /* Skip this pageblock in the future as it's full or nearly full */
        if (start_pfn == end_pfn && !cc->no_set_skip_hint)
            set_pageblock_skip(page);
    }
}
```

### isolate_freepages_block

```c
isolate_freepages_block(cc, &isolate_start_pfn/*start_pfn*/,
    block_end_pfn, freelist, stride, false/*strict*/) {

    if (strict)
        stride = 1;

    page = pfn_to_page(blockpfn);

    for (; blockpfn < end_pfn; blockpfn += stride, page += stride) {
        int isolated;

        if (!(blockpfn % COMPACT_CLUSTER_MAX)
            && compact_unlock_should_abort(&cc->zone->lock, flags, &locked, cc))
            break;

        nr_scanned++;

        if (PageCompound(page)) {
            const unsigned int order = compound_order(page);
            if (blockpfn + (1UL << order) <= end_pfn) {
                blockpfn += (1UL << order) - 1;
                page += (1UL << order) - 1;
                nr_scanned += (1UL << order) - 1;
            }

            goto isolate_fail;
        }

        /* only return true for the head page of a free buddy block */
        if (!PageBuddy(page))
            goto isolate_fail;

        /* If we already hold the lock, we can skip some rechecking. */
        if (!locked) {
            locked = compact_lock_irqsave(&cc->zone->lock, &flags, cc);

            /* Recheck this is a buddy page under lock */
            if (!PageBuddy(page))
                goto isolate_fail;
        }

        /* If (!PageBuddy(page)) ensures this is the head pages of a free buddy block
         * Found a free page, will break it into order-0 pages */
        order = buddy_order(page);
        isolated = __isolate_free_page(page, order) {
            struct zone *zone = page_zone(page);
            int mt = get_pageblock_migratetype(page);

            if (!is_migrate_isolate(mt)) { /* migratetype == MIGRATE_ISOLATE */
                unsigned long watermark;
                watermark = zone->_watermark[WMARK_MIN] + (1UL << order);
                if (!zone_watermark_ok(zone, 0, watermark, 0, ALLOC_CMA))
                    return 0;

                __mod_zone_freepage_state(zone, -(1UL << order), mt);
            }

            del_page_from_free_list(page, zone, order);

            if (order >= pageblock_order - 1) {
                struct page *endpage = page + (1 << order) - 1;
                for (; page < endpage; page += pageblock_nr_pages) {
                    int mt = get_pageblock_migratetype(page);
                    if (migratetype_is_mergeable(mt)) {
                        set_pageblock_migratetype(page, MIGRATE_MOVABLE);
                    }
                }
            }

            return 1UL << order;
        }
        if (!isolated)
            break;
        set_page_private(page, order);

        nr_scanned += isolated - 1;
        total_isolated += isolated;
        cc->nr_freepages += isolated;

        list_add_tail(&page->lru, freelist);

        if (!strict && cc->nr_migratepages <= cc->nr_freepages) {
            blockpfn += isolated;
            break;
        }
        /* Advance to the end of split page */
        blockpfn += isolated - 1;
        page += isolated - 1;

        continue;

isolate_fail:
        if (strict)
            break;
    }

    if (locked)
        spin_unlock_irqrestore(&cc->zone->lock, flags);

    /* Be careful to not go outside of the pageblock. */
    if (unlikely(blockpfn > end_pfn))
        blockpfn = end_pfn;

    /* Record how far we have got within the block */
    *start_pfn = blockpfn;

    if (strict && blockpfn < end_pfn)
        total_isolated = 0;

    cc->total_free_scanned += nr_scanned;
    if (total_isolated)
        count_compact_events(COMPACTISOLATED, total_isolated);
    return total_isolated;
}
```

# page_migrate

![](../images/kernel/mem-page_migrate.svg)

---

![](../images/kernel/mem-page_migrate.png)

Type | Note
:-: | :-:
MIGRATE_ASYNC | means never block
MIGRATE_SYNC_LIGHT | allow blocking on most operations but not `->writepage` as the potential stall time is too significant
MIGRATE_SYNC | will block when migrating pages
MIGRATE_SYNC_NO_COPY | will block when migrating pages but will not copy pages with the CPU

```c
enum migrate_reason {
    MR_COMPACTION,
    MR_MEMORY_FAILURE,
    MR_MEMORY_HOTPLUG,
    MR_SYSCALL,
    MR_MEMPOLICY_MBIND,
    MR_NUMA_MISPLACED,
    MR_CONTIG_RANGE,
    MR_LONGTERM_PIN,
    MR_DEMOTION,
    MR_TYPES
};
```

```c
int migrate_pages(struct list_head *from, new_folio_t get_new_folio,
        free_folio_t put_new_folio, unsigned long private,
        enum migrate_mode mode, int reason, unsigned int *ret_succeeded)
{
    int rc, rc_gather;
    int nr_pages;
    struct folio *folio, *folio2;
    LIST_HEAD(folios);
    LIST_HEAD(ret_folios);
    LIST_HEAD(split_folios);
    struct migrate_pages_stats stats;

    rc_gather = migrate_hugetlbs(from, get_new_folio, put_new_folio, private,
                    mode, reason, &stats, &ret_folios);
    if (rc_gather < 0)
        goto out;

again:
    nr_pages = 0;
    list_for_each_entry_safe(folio, folio2, from, lru) {
        /* Retried hugetlb folios will be kept in list  */
        if (folio_test_hugetlb(folio)) {
            list_move_tail(&folio->lru, &ret_folios);
            continue;
        }

        nr_pages += folio_nr_pages(folio);
        if (nr_pages >= NR_MAX_BATCHED_MIGRATION)
            break;
    }
    if (nr_pages >= NR_MAX_BATCHED_MIGRATION)
        list_cut_before(&folios, from, &folio2->lru);
    else
        list_splice_init(from, &folios);

    if (mode == MIGRATE_ASYNC)
        rc = migrate_pages_batch(&folios, get_new_folio, put_new_folio,
                private, mode, reason, &ret_folios,
                &split_folios, &stats,
                NR_MAX_MIGRATE_PAGES_RETRY);
    else
        rc = migrate_pages_sync(&folios, get_new_folio, put_new_folio,
                private, mode, reason, &ret_folios,
                &split_folios, &stats);

    list_splice_tail_init(&folios, &ret_folios);
    if (rc < 0) {
        rc_gather = rc;
        list_splice_tail(&split_folios, &ret_folios);
        goto out;
    }
    if (!list_empty(&split_folios)) {
        migrate_pages_batch(&split_folios, get_new_folio,
                put_new_folio, private, MIGRATE_ASYNC, reason,
                &ret_folios, NULL, &stats, 1);
        list_splice_tail_init(&split_folios, &ret_folios);
    }
    rc_gather += rc;
    if (!list_empty(from))
        goto again;
out:
    list_splice(&ret_folios, from);

    if (list_empty(from))
        rc_gather = 0;

    if (ret_succeeded)
        *ret_succeeded = stats.nr_succeeded;

    return rc_gather;
}
```

## migreate_pages_batch

```c
int migrate_pages_batch(struct list_head *from,
        new_folio_t get_new_folio, free_folio_t put_new_folio,
        unsigned long private, enum migrate_mode mode, int reason,
        struct list_head *ret_folios, struct list_head *split_folios,
        struct migrate_pages_stats *stats, int nr_pass)
{
    LIST_HEAD(unmap_folios);
    LIST_HEAD(dst_folios);
    bool nosplit = (reason == MR_NUMA_MISPLACED);

/* 1. unmap src folios */
    for (pass = 0; pass < nr_pass && retry; pass++) {
        retry = 0;
        thp_retry = 0;
        nr_retry_pages = 0;

        list_for_each_entry_safe(folio, folio2, from, lru) {
            is_large = folio_test_large(folio);
            is_thp = is_large && folio_test_pmd_mappable(folio);
            nr_pages = folio_nr_pages(folio);

            cond_resched();

            if (!thp_migration_supported() && is_thp) {
                nr_failed++;
                stats->nr_thp_failed++;
                if (!try_split_folio(folio, split_folios)) {
                    continue;
                }
                list_move_tail(&folio->lru, ret_folios);
                continue;
            }

            rc = migrate_folio_unmap(get_new_folio, put_new_folio,
                private, folio, &dst, mode, reason,
                ret_folios);
            switch(rc) {
            case -ENOMEM:
                nr_failed++;
                stats->nr_thp_failed += is_thp;
                /* Large folio NUMA faulting doesn't split to retry. */
                if (is_large && !nosplit) {
                    int ret = try_split_folio(folio, split_folios);

                    if (!ret) {
                        stats->nr_thp_split += is_thp;
                        stats->nr_split++;
                        break;
                    } else if (reason == MR_LONGTERM_PIN && ret == -EAGAIN) {
                        retry++;
                        thp_retry += is_thp;
                        nr_retry_pages += nr_pages;
                        /* Undo duplicated failure counting. */
                        nr_failed--;
                        stats->nr_thp_failed -= is_thp;
                        break;
                    }
                }

                stats->nr_failed_pages += nr_pages + nr_retry_pages;
                /* nr_failed isn't updated for not used */
                stats->nr_thp_failed += thp_retry;
                rc_saved = rc;
                if (list_empty(&unmap_folios))
                    goto out;
                else
                    goto move;
            case -EAGAIN:
                retry++;
                thp_retry += is_thp;
                nr_retry_pages += nr_pages;
                break;
            case MIGRATEPAGE_SUCCESS:
                stats->nr_succeeded += nr_pages;
                stats->nr_thp_succeeded += is_thp;
                break;
            case MIGRATEPAGE_UNMAP:
                list_move_tail(&folio->lru, &unmap_folios);
                list_add_tail(&dst->lru, &dst_folios);
                break;
            default:
                nr_failed++;
                stats->nr_thp_failed += is_thp;
                stats->nr_failed_pages += nr_pages;
                break;
            }
        }
    }
    nr_failed += retry;
    stats->nr_thp_failed += thp_retry;
    stats->nr_failed_pages += nr_retry_pages;

move:
    /* Flush TLBs for all unmapped folios */
    try_to_unmap_flush();

/* 2. move src to dst */
    retry = 1;
    for (pass = 0; pass < nr_pass && retry; pass++) {
        retry = 0;
        thp_retry = 0;
        nr_retry_pages = 0;

        dst = list_first_entry(&dst_folios, struct folio, lru);
        dst2 = list_next_entry(dst, lru);
        list_for_each_entry_safe(folio, folio2, &unmap_folios, lru) {
            is_thp = folio_test_large(folio) && folio_test_pmd_mappable(folio);
            nr_pages = folio_nr_pages(folio);

            cond_resched();

            rc = migrate_folio_move(put_new_folio, private, folio, dst, mode, reason, ret_folios);
            switch(rc) {
            case -EAGAIN:
                retry++;
                thp_retry += is_thp;
                nr_retry_pages += nr_pages;
                break;
            case MIGRATEPAGE_SUCCESS:
                stats->nr_succeeded += nr_pages;
                stats->nr_thp_succeeded += is_thp;
                break;
            default:
                nr_failed++;
                stats->nr_thp_failed += is_thp;
                stats->nr_failed_pages += nr_pages;
                break;
            }
            dst = dst2;
            dst2 = list_next_entry(dst, lru);
        }
    }
    nr_failed += retry;
    stats->nr_thp_failed += thp_retry;
    stats->nr_failed_pages += nr_retry_pages;

    rc = rc_saved ? : nr_failed;
out:

/*3.  Cleanup remaining folios */
    dst = list_first_entry(&dst_folios, struct folio, lru);
    dst2 = list_next_entry(dst, lru);
    list_for_each_entry_safe(folio, folio2, &unmap_folios, lru) {
        int old_page_state = 0;
        struct anon_vma *anon_vma = NULL;

        __migrate_folio_extract(dst, &old_page_state, &anon_vma);
        migrate_folio_undo_src(folio, old_page_state & PAGE_WAS_MAPPED,
                    anon_vma, true, ret_folios);
        list_del(&dst->lru);
        migrate_folio_undo_dst(dst, true, put_new_folio, private);
        dst = dst2;
        dst2 = list_next_entry(dst, lru);
    }

    return rc;
}
```

## migrate_folio_unmap

```c
rc = migrate_folio_unmap(get_new_folio, put_new_folio,
    private, folio/*src*/, &dst, mode, reason,
    ret_folios) {

    if (folio_ref_count(src) == 1) {
        /* Folio was freed from under us. So we are done. */
        folio_clear_active(src);
        folio_clear_unevictable(src);
        /* free_pages_prepare() will clear PG_isolated. */
        list_del(&src->lru);
        migrate_folio_done(src, reason);
        return MIGRATEPAGE_SUCCESS;
    }

    dst = get_new_folio(src, private); /* compaction_alloc -> isolate_freepages */
    if (!dst)
        return -ENOMEM;
    *dstp = dst;

    dst->private = NULL;

    if (!folio_trylock(src)) {
        if (mode == MIGRATE_ASYNC)
            goto out;

        if (current->flags & PF_MEMALLOC)
            goto out;

        if (mode == MIGRATE_SYNC_LIGHT && !folio_test_uptodate(src))
            goto out;

        folio_lock(src);
    }
    locked = true;
    if (folio_test_mlocked(src))
        old_page_state |= PAGE_WAS_MLOCKED;

    if (folio_test_writeback(src)) {
        switch (mode) {
        case MIGRATE_SYNC:
        case MIGRATE_SYNC_NO_COPY:
            break;
        default:
            rc = -EBUSY;
            goto out;
        }
        folio_wait_writeback(src);
    }

    if (folio_test_anon(src) && !folio_test_ksm(src))
        anon_vma = folio_get_anon_vma(src);

    if (unlikely(!folio_trylock(dst)))
        goto out;
    dst_locked = true;

    if (unlikely(!is_lru)) {
        __migrate_folio_record(dst, old_page_state, anon_vma);
        return MIGRATEPAGE_UNMAP;
    }

    if (!src->mapping) {
        if (folio_test_private(src)) {
            try_to_free_buffers(src);
            goto out;
        }
    } else if (folio_mapped(src)) {
        try_to_migrate(src, mode == MIGRATE_ASYNC ? TTU_BATCH_FLUSH : 0);
            --->
        old_page_state |= PAGE_WAS_MAPPED;
    }

    /* By try_to_migrate(), src->mapcount goes down to 0 */
    if (!folio_mapped(src)) {
        __migrate_folio_record(dst, old_page_state, anon_vma);
        return MIGRATEPAGE_UNMAP;
    }

out:
    if (rc == -EAGAIN)
        ret = NULL;

    migrate_folio_undo_src(src, old_page_state & PAGE_WAS_MAPPED,
                anon_vma, locked, ret);
    migrate_folio_undo_dst(dst, dst_locked, put_new_folio, private);

    return rc;
}
```

### try_to_migrate

```c
void try_to_migrate(struct folio *folio, enum ttu_flags flags)
{
    struct rmap_walk_control rwc = {
        .rmap_one = try_to_migrate_one,
        .arg = (void *)flags,
        .done = folio_not_mapped,
        .anon_lock = folio_lock_anon_vma_read,
    };

    if (WARN_ON_ONCE(flags & ~(TTU_RMAP_LOCKED | TTU_SPLIT_HUGE_PMD | TTU_SYNC | TTU_BATCH_FLUSH)))
        return;

    if (folio_is_zone_device(folio) &&
        (!folio_is_device_private(folio) && !folio_is_device_coherent(folio)))
        return;

    if (!folio_test_ksm(folio) && folio_test_anon(folio))
        rwc.invalid_vma = invalid_migration_vma;

    if (flags & TTU_RMAP_LOCKED)
        rmap_walk_locked(folio, &rwc);
    else
        rmap_walk(folio, &rwc);
}
```

```c
bool try_to_migrate_one(struct folio *folio, struct vm_area_struct *vma,
            unsigned long address, void *arg)
{
    struct mm_struct *mm = vma->vm_mm;
    DEFINE_FOLIO_VMA_WALK(pvmw, folio, vma, address, 0);
    pte_t pteval;
    struct page *subpage;
    bool anon_exclusive, ret = true;
    struct mmu_notifier_range range;
    enum ttu_flags flags = (enum ttu_flags)(long)arg;
    unsigned long pfn;
    unsigned long hsz = 0;

    if (flags & TTU_SYNC)
        pvmw.flags = PVMW_SYNC;

    /* For THP, we have to assume the worse case ie pmd for invalidation.
    * For hugetlb, it could be much worse if we need to do pud
    * invalidation in the case of pmd sharing.
    *
    * Note that the page can not be free in this function as call of
    * try_to_unmap() must hold a reference on the page. */
    range.end = vma_address_end(&pvmw);
    mmu_notifier_range_init(&range, MMU_NOTIFY_CLEAR, 0, vma->vm_mm, address, range.end);

    if (folio_test_hugetlb(folio)) {
        adjust_range_if_pmd_sharing_possible(vma, &range.start, &range.end);
        hsz = huge_page_size(hstate_vma(vma));
    }
    mmu_notifier_invalidate_range_start(&range);

    while (page_vma_mapped_walk(&pvmw)) {
        /* PMD-mapped THP migration entry */
        if (!pvmw.pte) {
            if (flags & TTU_SPLIT_HUGE_PMD) {
                split_huge_pmd_locked(vma, pvmw.address, pvmw.pmd, true);
                ret = false;
                page_vma_mapped_walk_done(&pvmw);
                break;
            }

#ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION
            subpage = folio_page(folio, pmd_pfn(*pvmw.pmd) - folio_pfn(folio));
            if (set_pmd_migration_entry(&pvmw, subpage)) {
                ret = false;
                page_vma_mapped_walk_done(&pvmw);
                break;
            }
            continue;
        }
#endif

        /* Unexpected PMD-mapped THP? */
        VM_BUG_ON_FOLIO(!pvmw.pte, folio);

        pteval = ptep_get(pvmw.pte);
        if (likely(pte_present(pteval))) {
            pfn = pte_pfn(pteval);
        } else {
            pfn = swp_offset_pfn(pte_to_swp_entry(pteval));
            VM_WARN_ON_FOLIO(folio_test_hugetlb(folio), folio);
        }

        subpage = folio_page(folio, pfn - folio_pfn(folio));
        address = pvmw.address;
        anon_exclusive = folio_test_anon(folio) && PageAnonExclusive(subpage);

        if (folio_test_hugetlb(folio)) {
            bool anon = folio_test_anon(folio);

            flush_cache_range(vma, range.start, range.end);

            if (!anon) {
                VM_BUG_ON(!(flags & TTU_RMAP_LOCKED));
                if (!hugetlb_vma_trylock_write(vma)) {
                    page_vma_mapped_walk_done(&pvmw);
                    ret = false;
                    break;
                }
                if (huge_pmd_unshare(mm, vma, address, pvmw.pte)) {
                    hugetlb_vma_unlock_write(vma);
                    /* unmap huge page backed by shared pte */
                    flush_tlb_range(vma, range.start, range.end);

                   /* The ref count of the PMD page was
                    * dropped which is part of the way map
                    * counting is done for shared PMDs.
                    * Return 'true' here.  When there is
                    * no other sharing, huge_pmd_unshare
                    * returns false and we will unmap the
                    * actual page and drop map count
                    * to zero. */
                    page_vma_mapped_walk_done(&pvmw);
                    break;
                }
                hugetlb_vma_unlock_write(vma);
            }
            /* Nuke the hugetlb page table entry */
            pteval = huge_ptep_clear_flush(vma, address, pvmw.pte);
            if (pte_dirty(pteval))
                folio_mark_dirty(folio);
            writable = pte_write(pteval);
        } else if (likely(pte_present(pteval))) {

        } else {
            pte_clear(mm, address, pvmw.pte);
            writable = is_writable_device_private_entry(pte_to_swp_entry(pteval));
        }

        /* Set the dirty flag on the folio now the pte is gone. */
        if (pte_dirty(pteval))
            folio_mark_dirty(folio);

        /* Update high watermark before we lower rss */
        update_hiwater_rss(mm);

        if (folio_is_device_private(folio)) {

        } else if (PageHWPoison(subpage)) {

        } else if (pte_unused(pteval) && !userfaultfd_armed(vma)) {

        } else {
            swp_entry_t entry;
            pte_t swp_pte;

            if (arch_unmap_one(mm, vma, address, pteval) < 0) {
                if (folio_test_hugetlb(folio))
                    set_huge_pte_at(mm, address, pvmw.pte, pteval, hsz);
                else
                    set_pte_at(mm, address, pvmw.pte, pteval);
                ret = false;
                page_vma_mapped_walk_done(&pvmw);
                break;
            }

            /* See folio_try_share_anon_rmap_pte(): clear PTE first. */
            if (folio_test_hugetlb(folio)) {

            } else if (anon_exclusive &&
                folio_try_share_anon_rmap_pte(folio, subpage)) {
                set_pte_at(mm, address, pvmw.pte, pteval);
                ret = false;
                page_vma_mapped_walk_done(&pvmw);
                break;
            }

            /* Store the pfn of the page in a special migration
             * pte. do_swap_page() will wait until the migration
             * pte is removed and then restart fault handling. */
            if (pte_write(pteval))
                entry = make_writable_migration_entry( page_to_pfn(subpage));
            else if (anon_exclusive)
                entry = make_readable_exclusive_migration_entry( page_to_pfn(subpage));
            else
                entry = make_readable_migration_entry(page_to_pfn(subpage));

            if (likely(pte_present(pteval))) {
                if (pte_young(pteval))
                    entry = make_migration_entry_young(entry);
                if (pte_dirty(pteval))
                    entry = make_migration_entry_dirty(entry);
                swp_pte = swp_entry_to_pte(entry);
                if (pte_soft_dirty(pteval))
                    swp_pte = pte_swp_mksoft_dirty(swp_pte);
                if (pte_uffd_wp(pteval))
                    swp_pte = pte_swp_mkuffd_wp(swp_pte);
            } else {
                swp_pte = swp_entry_to_pte(entry);
                if (pte_swp_soft_dirty(pteval))
                    swp_pte = pte_swp_mksoft_dirty(swp_pte);
                if (pte_swp_uffd_wp(pteval))
                    swp_pte = pte_swp_mkuffd_wp(swp_pte);
            }

            if (folio_test_hugetlb(folio))
                set_huge_pte_at(mm, address, pvmw.pte, swp_pte, hsz);
            else
                set_pte_at(mm, address, pvmw.pte, swp_pte);

            /* No need to invalidate here it will synchronize on
             * against the special swap migration pte. */
        }

        if (unlikely(folio_test_hugetlb(folio))) {
            hugetlb_remove_rmap(folio);
        } else {
            folio_remove_rmap_pte(folio, subpage, vma);
        }
        if (vma->vm_flags & VM_LOCKED)
            mlock_drain_local();
        folio_put(folio);
    }

    mmu_notifier_invalidate_range_end(&range);

    return ret;
}
```

## migrate_folio_move

```c
/* Migrate the folio to the newly allocated folio in dst. */
static int migrate_folio_move(free_folio_t put_new_folio, unsigned long private,
                struct folio *src, struct folio *dst,
                enum migrate_mode mode, enum migrate_reason reason,
                struct list_head *ret)
{
    int rc;
    int old_page_state = 0;
    struct anon_vma *anon_vma = NULL;
    bool is_lru = !__folio_test_movable(src);
    struct list_head *prev;

    __migrate_folio_extract(dst, &old_page_state, &anon_vma) {
        unsigned long private = (unsigned long)dst->private;
        *anon_vmap = (struct anon_vma *)(private & ~PAGE_OLD_STATES);
        *old_page_state = private & PAGE_OLD_STATES;
        dst->private = NULL;
    }

    prev = dst->lru.prev;
    list_del(&dst->lru);

    rc = move_to_new_folio(dst, src, mode) {
        bool is_lru = !__folio_test_movable(src) {
            return ((unsigned long)folio->mapping & PAGE_MAPPING_FLAGS)
                == PAGE_MAPPING_MOVABLE;
        }
        if (likely(is_lru)) {
            /* file or swap address space */
            struct address_space *mapping = folio_mapping(src);

            if (!mapping) {
                rc = migrate_folio(mapping, dst, src, mode);
            } else if (mapping_unmovable(mapping)) {
                rc = -EOPNOTSUPP;
            } else if (mapping->a_ops->migrate_folio) {
                rc = mapping->a_ops->migrate_folio(mapping, dst, src, mode);
            } else {
                rc = fallback_migrate_folio(mapping, dst, src, mode) {
                    if (folio_test_dirty(src)) {
                        switch (mode) {
                        case MIGRATE_SYNC:
                        case MIGRATE_SYNC_NO_COPY:
                            break;
                        default:
                            return -EBUSY;
                        }
                        return writeout(mapping, src);
                    }
                    if (!filemap_release_folio(src, GFP_KERNEL))
                        return mode == MIGRATE_SYNC ? -EAGAIN : -EBUSY;

                    return migrate_folio(mapping, dst, src, mode);
                }
            }
        } else {
            const struct movable_operations *mops;
            if (!folio_test_movable(src)) {
                rc = MIGRATEPAGE_SUCCESS;
                folio_clear_isolated(src);
                goto out;
            }

            mops = folio_movable_ops(src);
            rc = mops->migrate_page(&dst->page, &src->page, mode);
        }

        if (rc == MIGRATEPAGE_SUCCESS) {
            if (__folio_test_movable(src)) {
                VM_BUG_ON_FOLIO(!folio_test_isolated(src), src);

                folio_clear_isolated(src);
            }

            if (!folio_mapping_flags(src))
                src->mapping = NULL;

            if (likely(!folio_is_zone_device(dst)))
                flush_dcache_folio(dst);
        }
    out:
        return rc;
    }
    if (rc)
        goto out;

    if (unlikely(!is_lru))
        goto out_unlock_both;

    folio_add_lru(dst);
    if (old_page_state & PAGE_WAS_MLOCKED)
        lru_add_drain();

    if (old_page_state & PAGE_WAS_MAPPED)
        remove_migration_ptes(src, dst, false);

out_unlock_both:
    folio_unlock(dst);
    set_page_owner_migrate_reason(&dst->page, reason);

    folio_put(dst);


    list_del(&src->lru);
    /* Drop an anon_vma reference if we took one */
    if (anon_vma)
        put_anon_vma(anon_vma);
    folio_unlock(src);
    migrate_folio_done(src, reason);

    return rc;
out:
    if (rc == -EAGAIN) {
        list_add(&dst->lru, prev);
        __migrate_folio_record(dst, old_page_state, anon_vma);
        return rc;
    }

    migrate_folio_undo_src(src, old_page_state & PAGE_WAS_MAPPED, anon_vma, true, ret);
    migrate_folio_undo_dst(dst, true, put_new_folio, private);

    return rc;
}
```

### migrate_folio

```c
int migrate_folio(struct address_space *mapping, struct folio *dst,
    struct folio *src, enum migrate_mode mode)
{
    int rc, expected_count = folio_expected_refs(mapping, src);

    /* Check whether src does not have extra refs before we do more work */
    if (folio_ref_count(src) != expected_count)
        return -EAGAIN;

    rc = folio_mc_copy(dst, src);
    if (unlikely(rc))
        return rc;

    int rc = folio_migrate_mapping(mapping, dst, src, extra_count);
    if (rc != MIGRATEPAGE_SUCCESS)
        return rc;

    if (src_private)
        folio_attach_private(dst, folio_detach_private(src));

    folio_migrate_flags(dst, src) {
        int cpupid;

        if (folio_test_error(folio))
            folio_set_error(newfolio);
        if (folio_test_referenced(folio))
            folio_set_referenced(newfolio);
        if (folio_test_uptodate(folio))
            folio_mark_uptodate(newfolio);
        if (folio_test_clear_active(folio)) {
            VM_BUG_ON_FOLIO(folio_test_unevictable(folio), folio);
            folio_set_active(newfolio);
        } else if (folio_test_clear_unevictable(folio))
            folio_set_unevictable(newfolio);
        if (folio_test_workingset(folio))
            folio_set_workingset(newfolio);
        if (folio_test_checked(folio))
            folio_set_checked(newfolio);
        /* PG_anon_exclusive (-> PG_mappedtodisk) is always migrated via
            * migration entries. We can still have PG_anon_exclusive set on an
            * effectively unmapped and unreferenced first sub-pages of an
            * anonymous THP: we can simply copy it here via PG_mappedtodisk. */
        if (folio_test_mappedtodisk(folio))
            folio_set_mappedtodisk(newfolio);

        /* Move dirty on pages not done by folio_migrate_mapping() */
        if (folio_test_dirty(folio))
            folio_set_dirty(newfolio);

        if (folio_test_young(folio))
            folio_set_young(newfolio);
        if (folio_test_idle(folio))
            folio_set_idle(newfolio);

        /* Copy NUMA information to the new page, to prevent over-eager
            * future migrations of this same page. */
        cpupid = folio_xchg_last_cpupid(folio, -1);
        /* For memory tiering mode, when migrate between slow and fast
            * memory node, reset cpupid, because that is used to record
            * page access time in slow memory node. */
        if (sysctl_numa_balancing_mode & NUMA_BALANCING_MEMORY_TIERING) {
            bool f_toptier = node_is_toptier(folio_nid(folio));
            bool t_toptier = node_is_toptier(folio_nid(newfolio));

            if (f_toptier != t_toptier)
                cpupid = -1;
        }
        folio_xchg_last_cpupid(newfolio, cpupid);

        folio_migrate_ksm(newfolio, folio);
        /* Please do not reorder this without considering how mm/ksm.c's
            * get_ksm_page() depends upon ksm_migrate_page() and PageSwapCache(). */
        if (folio_test_swapcache(folio))
            folio_clear_swapcache(folio);
        folio_clear_private(folio);

        /* page->private contains hugetlb specific flags */
        if (!folio_test_hugetlb(folio))
            folio->private = NULL;

        /* If any waiters have accumulated on the new page then
        * wake them up. */
        if (folio_test_writeback(newfolio))
            folio_end_writeback(newfolio);

        /* PG_readahead shares the same bit with PG_reclaim.  The above
            * end_page_writeback() may clear PG_readahead mistakenly, so set the
            * bit after that. */
        if (folio_test_readahead(folio))
            folio_set_readahead(newfolio);

        folio_copy_owner(newfolio, folio);

        /* Transfer the memcg data from the old to the new folio */
        mem_cgroup_migrate(folio, newfolio) {
            struct mem_cgroup *memcg;

            if (mem_cgroup_disabled())
                return;

            memcg = folio_memcg(old);
            VM_WARN_ON_ONCE_FOLIO(!folio_test_hugetlb(old) && !memcg, old);
            if (!memcg)
                return;

            /* Transfer the charge and the css ref */
            commit_charge(new, memcg) {
                folio->memcg_data = (unsigned long)memcg;
            }

            /* Warning should never happen, so don't worry about refcount non-0 */
            WARN_ON_ONCE(folio_unqueue_deferred_split(old));
            old->memcg_data = 0;
        }
    }

    return MIGRATEPAGE_SUCCESS;
}
```

#### folio_migrate_mapping

```c
int rc = folio_migrate_mapping(mapping, dst, src, extra_count) {
    XA_STATE(xas, &mapping->i_pages, folio_index(folio));
    struct zone *oldzone, *newzone;
    int dirty;
    int expected_count = folio_expected_refs(mapping, folio) + extra_count;
    long nr = folio_nr_pages(folio);
    long entries, i;

    if (!mapping) { /* Anonymous page without mapping */
        if (folio_ref_count(folio) != expected_count)
            return -EAGAIN;

        /* No turning back from here */
        newfolio->index = folio->index;
        newfolio->mapping = folio->mapping;
        if (folio_test_swapbacked(folio))
            __folio_set_swapbacked(newfolio);

        return MIGRATEPAGE_SUCCESS;
    }

    oldzone = folio_zone(folio);
    newzone = folio_zone(newfolio);

    xas_lock_irq(&xas);
    if (!folio_ref_freeze(folio, expected_count)) {
        xas_unlock_irq(&xas);
        return -EAGAIN;
    }

    newfolio->index = folio->index;
    newfolio->mapping = folio->mapping;
    folio_ref_add(newfolio, nr); /* add cache reference */
    if (folio_test_swapbacked(folio)) {
        __folio_set_swapbacked(newfolio);
        if (folio_test_swapcache(folio)) {
            folio_set_swapcache(newfolio);
            newfolio->private = folio_get_private(folio);
        }
        entries = nr;
    } else {
        VM_BUG_ON_FOLIO(folio_test_swapcache(folio), folio);
        entries = 1;
    }

    /* Move dirty while page refs frozen and newpage not yet exposed */
    dirty = folio_test_dirty(folio);
    if (dirty) {
        folio_clear_dirty(folio);
        folio_set_dirty(newfolio);
    }

    /* Swap cache still stores N entries instead of a high-order entry */
    for (i = 0; i < entries; i++) {
        xas_store(&xas, newfolio);
        xas_next(&xas);
    }

    /* Drop cache reference from old page by unfreezing
        * to one less reference.
        * We know this isn't the last reference. */
    folio_ref_unfreeze(folio, expected_count - nr);

    xas_unlock(&xas);
    /* Leave irq disabled to prevent preemption while updating stats */

    if (newzone != oldzone) {
        struct lruvec *old_lruvec, *new_lruvec;
        struct mem_cgroup *memcg;

        memcg = folio_memcg(folio);
        old_lruvec = mem_cgroup_lruvec(memcg, oldzone->zone_pgdat);
        new_lruvec = mem_cgroup_lruvec(memcg, newzone->zone_pgdat);

        mod_lruvec_state(old_lruvec, NR_FILE_PAGES, -nr);
        mod_lruvec_state(new_lruvec, NR_FILE_PAGES, nr);
        if (folio_test_swapbacked(folio) && !folio_test_swapcache(folio)) {
            mod_lruvec_state(old_lruvec, NR_SHMEM, -nr);
            mod_lruvec_state(new_lruvec, NR_SHMEM, nr);

            if (folio_test_pmd_mappable(folio)) {
                mod_lruvec_state(old_lruvec, NR_SHMEM_THPS, -nr);
                mod_lruvec_state(new_lruvec, NR_SHMEM_THPS, nr);
            }
        }
#ifdef CONFIG_SWAP
        if (folio_test_swapcache(folio)) {
            mod_lruvec_state(old_lruvec, NR_SWAPCACHE, -nr);
            mod_lruvec_state(new_lruvec, NR_SWAPCACHE, nr);
        }
#endif
        if (dirty && mapping_can_writeback(mapping)) {
            mod_lruvec_state(old_lruvec, NR_FILE_DIRTY, -nr);
            __mod_zone_page_state(oldzone, NR_ZONE_WRITE_PENDING, -nr);
            mod_lruvec_state(new_lruvec, NR_FILE_DIRTY, nr);
            __mod_zone_page_state(newzone, NR_ZONE_WRITE_PENDING, nr);
        }
    }
    local_irq_enable();

    return MIGRATEPAGE_SUCCESS;
}
```

### remove_migration_ptes

```c
void remove_migration_ptes(struct folio *src, struct folio *dst, bool locked)
{
    struct rmap_walk_control rwc = {
        .rmap_one = remove_migration_pte,
        .arg = src,
    };

    if (locked)
        rmap_walk_locked(dst, &rwc);
    else
        rmap_walk(dst, &rwc);
}
```

```c
/* Restore a potential migration pte to a working pte entry */
bool remove_migration_pte(struct folio *folio,
    struct vm_area_struct *vma, unsigned long addr, void *old)
{
    DEFINE_FOLIO_VMA_WALK(pvmw, old, vma, addr, PVMW_SYNC | PVMW_MIGRATION);

    while (page_vma_mapped_walk(&pvmw)) {
        rmap_t rmap_flags = RMAP_NONE;
        pte_t old_pte;
        pte_t pte;
        swp_entry_t entry;
        struct page *new;
        unsigned long idx = 0;

        /* pgoff is invalid for ksm pages, but they are never large */
        if (folio_test_large(folio) && !folio_test_hugetlb(folio))
            idx = linear_page_index(vma, pvmw.address) - pvmw.pgoff;
        new = folio_page(folio, idx);

        folio_get(folio);
        pte = mk_pte(new, READ_ONCE(vma->vm_page_prot));
        old_pte = ptep_get(pvmw.pte);
        if (pte_swp_soft_dirty(old_pte))
            pte = pte_mksoft_dirty(pte);

        entry = pte_to_swp_entry(old_pte);
        if (!is_migration_entry_young(entry))
            pte = pte_mkold(pte);
        if (folio_test_dirty(folio) && is_migration_entry_dirty(entry))
            pte = pte_mkdirty(pte);
        if (is_writable_migration_entry(entry))
            pte = pte_mkwrite(pte, vma);
        else if (pte_swp_uffd_wp(old_pte))
            pte = pte_mkuffd_wp(pte);

        if (folio_test_anon(folio) && !is_readable_migration_entry(entry))
            rmap_flags |= RMAP_EXCLUSIVE;

        if (unlikely(is_device_private_page(new))) {
            if (pte_write(pte))
                entry = make_writable_device_private_entry(page_to_pfn(new));
            else
                entry = make_readable_device_private_entry(page_to_pfn(new));

            pte = swp_entry_to_pte(entry);
            if (pte_swp_soft_dirty(old_pte))
                pte = pte_swp_mksoft_dirty(pte);
            if (pte_swp_uffd_wp(old_pte))
                pte = pte_swp_mkuffd_wp(pte);
        }

        if (folio_test_anon(folio))
            folio_add_anon_rmap_pte(folio, new, vma, pvmw.address, rmap_flags);
        else
            folio_add_file_rmap_pte(folio, new, vma);
        set_pte_at(vma->vm_mm, pvmw.address, pvmw.pte, pte);

        if (vma->vm_flags & VM_LOCKED)
            mlock_drain_local();

        /* No need to invalidate - it was non-present before */
        update_mmu_cache(vma, pvmw.address, pvmw.pte);
    }

    return true;
}
```

# kcompactd

* The conditions to wakeup kcompactd
    1. Before kswapd run

        wakeup_kcompactd if the failure of kswapd comes from external fragments not low memory.

        ```c
        void wakeup_kswapd(struct zone *zone, gfp_t gfp_flags, int order,
            enum zone_type highest_zoneidx) {

            /* Hopeless node, leave it to direct reclaim if possible */
            if (pgdat->kswapd_failures >= MAX_RECLAIM_RETRIES
                || (pgdat_balanced(pgdat, order, highest_zoneidx)
                && !pgdat_watermark_boosted(pgdat, highest_zoneidx))) {

                if (!(gfp_flags & __GFP_DIRECT_RECLAIM))
                    wakeup_kcompactd(pgdat, order, highest_zoneidx);
                return;
            }
        }
        ```

    2. When kswapd run

        During the memory allocation process, if "memory stealing" occurs, memory reclaim may be triggered at the end of the allocation, and memory compaction may be triggered during the memory recycling process. This is a logic to reduce the risk of memory fragmentation in advance.

        ```c
        steal_suitable_fallback(zone, page, alloc_flags, start_migratetype, can_steal) {
            /* Boost watermarks to increase reclaim pressure to reduce the
             * likelihood of future fallbacks. Wake kswapd now as the node
             * may be balanced overall and kswapd will not wake naturally. */
            if (boost_watermark(zone) && (alloc_flags & ALLOC_KSWAPD))
                set_bit(ZONE_BOOSTED_WATERMARK, &zone->flags);
        }

        struct page *rmqueue(struct zone *preferred_zone,
            struct zone *zone, unsigned int order,
            gfp_t gfp_flags, unsigned int alloc_flags,
            int migratetype) {

            if ((alloc_flags & ALLOC_KSWAPD)
                && unlikely(test_bit(ZONE_BOOSTED_WATERMARK, &zone->flags))) {

                clear_bit(ZONE_BOOSTED_WATERMARK, &zone->flags);
                wakeup_kswapd(zone, 0, 0, zone_idx(zone));
            }

            return page;
        }

        static int balance_pgdat(pg_data_t *pgdat, int order, int highest_zoneidx) {
            nr_boost_reclaim = 0;
            for (i = 0; i <= highest_zoneidx; i++) {
                zone = pgdat->node_zones + i;
                if (!managed_zone(zone))
                    continue;
                nr_boost_reclaim += zone->watermark_boost;
                zone_boosts[i] = zone->watermark_boost;
            }
            boosted = nr_boost_reclaim;

            if (boosted) {
                wakeup_kcompactd(pgdat, pageblock_order, highest_zoneidx);
            }
        }

        ```

    3. After kswapd run

        After kswapd is finished running, it will routinely clear the cache information related to memory compaction, triggering memory compaction;

        ```c
        void kswapd_try_to_sleep(pg_data_t *pgdat, int alloc_order, int reclaim_order,
            unsigned int highest_zoneidx) {

            /* Return true if high watermarks have been met.
             * Since we have freed the memory, now we should compact it to make
             * allocation of the requested order possible. */
            if (prepare_kswapd_sleep(pgdat, reclaim_order, highest_zoneidx)) {
                wakeup_kcompactd(pgdat, alloc_order, highest_zoneidx);
            }
        }
        ```

```c
int kcompactd(void *p)
{
    pg_data_t *pgdat = (pg_data_t *)p;
    struct task_struct *tsk = current;
    long default_timeout = msecs_to_jiffies(HPAGE_FRAG_CHECK_INTERVAL_MSEC);
    long timeout = default_timeout;

    const struct cpumask *cpumask = cpumask_of_node(pgdat->node_id);

    if (!cpumask_empty(cpumask))
        set_cpus_allowed_ptr(tsk, cpumask);

    set_freezable();

    pgdat->kcompactd_max_order = 0;
    pgdat->kcompactd_highest_zoneidx = pgdat->nr_zones - 1;

    /* test_bit(KTHREAD_SHOULD_STOP, &to_kthread(current)->flags); */
    while (!kthread_should_stop()) {
        unsigned long pflags;

        if (!sysctl_compaction_proactiveness)
            timeout = MAX_SCHEDULE_TIMEOUT;

        ret = wait_event_freezable_timeout(pgdat->kcompactd_wait, kcompactd_work_requested(pgdat) {
            return pgdat->kcompactd_max_order > 0 || kthread_should_stop() ||
                pgdat->proactive_compact_trigger;
        }, timeout);

        if (ret && !pgdat->proactive_compact_trigger) {
            kcompactd_do_work(pgdat) {
                int zoneid;
                struct zone *zone;
                struct compact_control cc = {
                    .order = pgdat->kcompactd_max_order,
                    .search_order = pgdat->kcompactd_max_order,
                    .highest_zoneidx = pgdat->kcompactd_highest_zoneidx,
                    .mode = MIGRATE_SYNC_LIGHT,
                    .ignore_skip_hint = false,
                    .gfp_mask = GFP_KERNEL,
                };
                enum compact_result ret;

                for (zoneid = 0; zoneid <= cc.highest_zoneidx; zoneid++) {
                    zone = &pgdat->node_zones[zoneid];
                    if (!populated_zone(zone))
                        continue;

                    if (compaction_deferred(zone, cc.order))
                        continue;

                    /* check fragmentation index is under wmark */
                    ret = compaction_suit_allocation_order(zone, cc.order, zoneid, ALLOC_WMARK_MIN) {
                        unsigned long watermark;

                        watermark = wmark_pages(zone, alloc_flags & ALLOC_WMARK_MASK);
                        if (zone_watermark_ok(zone, order, watermark, highest_zoneidx, alloc_flags)) {
                            return COMPACT_SUCCESS;
                        }

                        ret = compaction_suitable(zone, order, highest_zoneidx) {
                            enum compact_result compact_result;
                            bool suitable;

                            suitable = __compaction_suitable(zone, order, highest_zoneidx, zone_page_state(zone, NR_FREE_PAGES)) {

                                unsigned long watermark = (order > PAGE_ALLOC_COSTLY_ORDER)
                                    ? low_wmark_pages(zone) : min_wmark_pages(zone);
                                watermark += compact_gap(order) { return 2UL << order; }
                                return __zone_watermark_ok(zone, 0, watermark, highest_zoneidx, ALLOC_CMA, wmark_target);
                            }

                            if (suitable) {
                                compact_result = COMPACT_CONTINUE;
                                if (order > PAGE_ALLOC_COSTLY_ORDER) {
                                    int fragindex = fragmentation_index(zone, order) {
                                        struct contig_page_info info;
                                        fill_contig_page_info(zone, order, &info);
                                            --->
                                        return __fragmentation_index(order, &info) {
                                            unsigned long requested = 1UL << order;

                                            if (WARN_ON_ONCE(order > MAX_PAGE_ORDER))
                                                return 0;

                                            if (!info->free_blocks_total)
                                                return 0;

                                            /* Fragmentation index only makes sense when a request would fail */
                                            if (info->free_blocks_suitable)
                                                return -1000;

                                            /* Index is between 0 and 1 so return within 3 decimal places
                                             *
                                             * 0 => fail due to lack of memory
                                             * 1000 => fail due to fragmentation */
                                            return 1000 - div_u64(
                                                (1000 +
                                                    (div_u64(
                                                        info->free_pages * 1000ULL,
                                                        requested)
                                                    )
                                                ),
                                                info->free_blocks_total
                                            );
                                        }
                                    }
                                    if (fragindex >= 0 && fragindex <= sysctl_extfrag_threshold) {
                                        suitable = false;
                                        compact_result = COMPACT_NOT_SUITABLE_ZONE;
                                    }
                                }
                            } else {
                                compact_result = COMPACT_SKIPPED;
                            }

                            return suitable;
                        }
                        if (!ret)
                            return COMPACT_SKIPPED;

                        return COMPACT_CONTINUE;
                    }
                    if (ret != COMPACT_CONTINUE)
                        continue;

                    if (kthread_should_stop())
                        return;

                    cc.zone = zone;
                    status = compact_zone(&cc, NULL);
                        --->

                    if (status == COMPACT_SUCCESS) {
                        compaction_defer_reset(zone, cc.order, false);
                    } else if (status == COMPACT_PARTIAL_SKIPPED || status == COMPACT_COMPLETE) {
                        drain_all_pages(zone);
                        defer_compaction(zone, cc.order);
                    }
                }

                if (pgdat->kcompactd_max_order <= cc.order)
                    pgdat->kcompactd_max_order = 0;
                if (pgdat->kcompactd_highest_zoneidx >= cc.highest_zoneidx)
                    pgdat->kcompactd_highest_zoneidx = pgdat->nr_zones - 1;
            }
            timeout = default_timeout;

            continue;
        }

        timeout = default_timeout;
        ret = should_proactive_compact_node(pgdat) {
            int wmark_high;

            if (!sysctl_compaction_proactiveness || kswapd_is_running(pgdat))
                return false;

            wmark_high = fragmentation_score_wmark(false/*low*/) {
                unsigned int wmark_low = max(100U - sysctl_compaction_proactiveness, 5U);
                return low ? wmark_low : min(wmark_low + 10, 100U);
            }
            return fragmentation_score_node(pgdat) {
                unsigned int score = 0;
                int zoneid;

                for (zoneid = 0; zoneid < MAX_NR_ZONES; zoneid++) {
                    struct zone *zone = &pgdat->node_zones[zoneid];
                    if (!populated_zone(zone))
                        continue;

                    score += fragmentation_score_zone_weighted(zone) {
                        unsigned long score = zone->present_pages * fragmentation_score_zone(zone) {
                            return extfrag_for_order(zone, COMPACTION_HPAGE_ORDER) {
                                struct contig_page_info info;

                                fill_contig_page_info(zone, order/*suitable_order*/, &info) {
                                    unsigned int order;

                                    info->free_pages = 0;
                                    info->free_blocks_total = 0;
                                    info->free_blocks_suitable = 0;

                                    for (order = 0; order < NR_PAGE_ORDERS; order++) {
                                        unsigned long blocks = data_race(zone->free_area[order].nr_free);
                                        info->free_blocks_total += blocks;
                                        info->free_pages += blocks << order;
                                        if (order >= suitable_order) {
                                            info->free_blocks_suitable
                                                += blocks << (order - suitable_order);
                                        }
                                    }
                                }
                                if (info.free_pages == 0)
                                    return 0;

                                return div_u64(
                                    (info.free_pages - (info.free_blocks_suitable << order)) * 100,
                                    info.free_pages
                                );
                            }
                        }
                        return div64_ul(score, zone->zone_pgdat->node_present_pages + 1);
                    }
                }

                return score;
            } > wmark_high;
        }
        if (ret) {
            unsigned int prev_score, score;

            prev_score = fragmentation_score_node(pgdat);
            proactive_compact_node(pgdat) {
                int zoneid;
                struct zone *zone;
                struct compact_control cc = {
                    .order = -1,
                    .mode = MIGRATE_SYNC_LIGHT,
                    .ignore_skip_hint = true,
                    .whole_zone = true,
                    .gfp_mask = GFP_KERNEL,
                    .proactive_compaction = true,
                };

                for (zoneid = 0; zoneid < MAX_NR_ZONES; zoneid++) {
                    zone = &pgdat->node_zones[zoneid];
                    if (!populated_zone(zone))
                        continue;
                    cc.zone = zone;
                    compact_zone(&cc, NULL);
                        --->
                }
            }
            score = fragmentation_score_node(pgdat);
            if (unlikely(score >= prev_score))
                timeout = default_timeout << COMPACT_MAX_DEFER_SHIFT;
        }
        if (unlikely(pgdat->proactive_compact_trigger))
            pgdat->proactive_compact_trigger = false;
    }

    return 0;
}
```


# kswapd

* [kswapd介绍 - 内核工匠](https://mp.weixin.qq.com/s/1iVJC8Ca5OQfdEIYQW-4GQ)

![](../images/kernel/mem-kswapd.png)

```c
int kswapd(void *p)
{
    unsigned int alloc_order, reclaim_order;
    unsigned int highest_zoneidx = MAX_NR_ZONES - 1;
    pg_data_t *pgdat = (pg_data_t *)p;
    struct task_struct *tsk = current;
    const struct cpumask *cpumask = cpumask_of_node(pgdat->node_id);

    if (!cpumask_empty(cpumask))
        set_cpus_allowed_ptr(tsk, cpumask);

    tsk->flags |= PF_MEMALLOC | PF_KSWAPD;
    set_freezable();

    WRITE_ONCE(pgdat->kswapd_order, 0);
    WRITE_ONCE(pgdat->kswapd_highest_zoneidx, MAX_NR_ZONES);
    atomic_set(&pgdat->nr_writeback_throttled, 0);

    for ( ; ; ) {
        bool ret;

        alloc_order = reclaim_order = READ_ONCE(pgdat->kswapd_order);
        highest_zoneidx = kswapd_highest_zoneidx(pgdat, highest_zoneidx);

kswapd_try_sleep:
        kswapd_try_to_sleep(pgdat, alloc_order, reclaim_order, highest_zoneidx);

        /* Read the new order and highest_zoneidx */
        alloc_order = READ_ONCE(pgdat->kswapd_order);
        highest_zoneidx = kswapd_highest_zoneidx(pgdat, highest_zoneidx);
        WRITE_ONCE(pgdat->kswapd_order, 0);
        WRITE_ONCE(pgdat->kswapd_highest_zoneidx, MAX_NR_ZONES);

        ret = try_to_freeze();
        if (kthread_should_stop())
            break;
        if (ret)
            continue;

        reclaim_order = balance_pgdat(pgdat, alloc_order, highest_zoneidx) {
            struct scan_control sc = {
                .gfp_mask = GFP_KERNEL,
                .order = order,
                .may_unmap = 1,
            };

            set_task_reclaim_state(current, &sc.reclaim_state);

            nr_boost_reclaim = 0;
            for (i = 0; i <= highest_zoneidx; i++) {
                zone = pgdat->node_zones + i;
                if (!managed_zone(zone))
                    continue;

                nr_boost_reclaim += zone->watermark_boost;
                zone_boosts[i] = zone->watermark_boost;
            }
            boosted = nr_boost_reclaim;

        restart:
            set_reclaim_active(pgdat, highest_zoneidx);
            sc.priority = DEF_PRIORITY;
            do {
                unsigned long nr_reclaimed = sc.nr_reclaimed;
                bool raise_priority = true;
                bool balanced;
                bool ret;

                sc.reclaim_idx = highest_zoneidx;
                if (buffer_heads_over_limit) {
                    for (i = MAX_NR_ZONES - 1; i >= 0; i--) {
                        zone = pgdat->node_zones + i;
                        if (!managed_zone(zone))
                            continue;

                        sc.reclaim_idx = i;
                        break;
                    }
                }

                balanced = pgdat_balanced(pgdat, sc.order, highest_zoneidx) {
                    int i;
                    unsigned long mark = -1;
                    struct zone *zone;

                    /* Check watermarks bottom-up as lower zones are more likely to
                     * meet watermarks. */
                    for (i = 0; i <= highest_zoneidx; i++) {
                        zone = pgdat->node_zones + i;

                        if (!managed_zone(zone))
                            continue;

                        if (sysctl_numa_balancing_mode & NUMA_BALANCING_MEMORY_TIERING)
                            mark = wmark_pages(zone, WMARK_PROMO);
                        else
                            mark = high_wmark_pages(zone);
                        if (zone_watermark_ok_safe(zone, order, mark, highest_zoneidx))
                            return true;
                    }

                    /* a node has no managed zone within highest_zoneidx */
                    if (mark == -1)
                        return true;

                    return false;
                }
                if (!balanced && nr_boost_reclaim) {
                    nr_boost_reclaim = 0;
                    goto restart;
                }

                if (!nr_boost_reclaim && balanced)
                    goto out;

                /* Limit the priority of boosting to avoid reclaim writeback */
                if (nr_boost_reclaim && sc.priority == DEF_PRIORITY - 2)
                    raise_priority = false;

                /* Do not writeback or swap pages for boosted reclaim. The
                 * intent is to relieve pressure not issue sub-optimal IO
                 * from reclaim context. If no pages are reclaimed, the
                 * reclaim will be aborted. */
                sc.may_writepage = !laptop_mode && !nr_boost_reclaim;
                sc.may_swap = !nr_boost_reclaim;

                /* Do some background aging, to give pages a chance to be
                 * referenced before reclaiming. All pages are rotated
                 * regardless of classzone as this is about consistent aging. */
                kswapd_age_node(pgdat, &sc) {
                    struct mem_cgroup *memcg;
                    struct lruvec *lruvec;

                    if (lru_gen_enabled()) {
                        lru_gen_age_node(pgdat, sc);
                        return;
                    }

                    if (!can_age_anon_pages(pgdat, sc))
                        return;

                    lruvec = mem_cgroup_lruvec(NULL, pgdat);
                    if (!inactive_is_low(lruvec, LRU_INACTIVE_ANON))
                        return;

                    memcg = mem_cgroup_iter(NULL, NULL, NULL);
                    do {
                        lruvec = mem_cgroup_lruvec(memcg, pgdat);
                        shrink_active_list(SWAP_CLUSTER_MAX, lruvec, sc, LRU_ACTIVE_ANON);
                        memcg = mem_cgroup_iter(NULL, memcg, NULL);
                    } while (memcg);
                }

                /* If we're getting trouble reclaiming, start doing writepage
                * even in laptop mode. */
                if (sc.priority < DEF_PRIORITY - 2)
                    sc.may_writepage = 1;

                /* Call soft limit reclaim before calling shrink_node. */
                sc.nr_scanned = 0;
                nr_soft_scanned = 0;
                nr_soft_reclaimed = memcg1_soft_limit_reclaim(
                    pgdat, sc.order, sc.gfp_mask, &nr_soft_scanned
                );
                sc.nr_reclaimed += nr_soft_reclaimed;

                /* There should be no need to raise the scanning priority if
                 * enough pages are already being scanned that that high
                 * watermark would be met at 100% efficiency. */
                ret = kswapd_shrink_node(pgdat, &sc) {
                    /* Reclaim a number of pages proportional to the number of zones */
                    sc->nr_to_reclaim = 0;
                    for (z = 0; z <= sc->reclaim_idx; z++) {
                        zone = pgdat->node_zones + z;
                        if (!managed_zone(zone))
                            continue;

                        sc->nr_to_reclaim += max(high_wmark_pages(zone), SWAP_CLUSTER_MAX);
                    }

                    shrink_node(pgdat, sc);
                        --->

                    /* Fragmentation may mean that the system cannot be rebalanced for
                     * high-order allocations. If twice the allocation size has been
                     * reclaimed then recheck watermarks only at order-0 to prevent
                     * excessive reclaim. Assume that a process requested a high-order
                     * can direct reclaim/compact. */
                    if (sc->order && sc->nr_reclaimed >= compact_gap(sc->order))
                        sc->order = 0;

                    /* account for progress from mm_account_reclaimed_pages() */
                    return max(sc->nr_scanned, sc->nr_reclaimed - nr_reclaimed) >= sc->nr_to_reclaim;
                }
                if (ret)
                    raise_priority = false;

                /* If the low watermark is met there is no need for processes
                 * to be throttled on pfmemalloc_wait as they should not be
                 * able to safely make forward progress. Wake them */
                if (waitqueue_active(&pgdat->pfmemalloc_wait) && allow_direct_reclaim(pgdat))
                    wake_up_all(&pgdat->pfmemalloc_wait);

                /* Check if kswapd should be suspending */
                __fs_reclaim_release(_THIS_IP_);
                ret = try_to_freeze();
                __fs_reclaim_acquire(_THIS_IP_);

                if (ret || kthread_should_stop())
                    break;

                /* Raise priority if scanning rate is too low or there was no
                 * progress in reclaiming pages */
                nr_reclaimed = sc.nr_reclaimed - nr_reclaimed;
                nr_boost_reclaim -= min(nr_boost_reclaim, nr_reclaimed);

                /* If reclaim made no progress for a boost, stop reclaim as
                 * IO cannot be queued and it could be an infinite loop in
                 * extreme circumstances. */
                if (nr_boost_reclaim && !nr_reclaimed)
                    break;

                if (raise_priority || !nr_reclaimed) {
                    sc.priority--;
                }
            } while (sc.priority >= 1);

            if (!sc.nr_reclaimed)
                pgdat->kswapd_failures++;

        out:
            clear_reclaim_active(pgdat, highest_zoneidx);

            /* If reclaim was boosted, account for the reclaim done in this pass */
            if (boosted) {
                unsigned long flags;

                for (i = 0; i <= highest_zoneidx; i++) {
                    if (!zone_boosts[i])
                        continue;

                    /* Increments are under the zone lock */
                    zone = pgdat->node_zones + i;
                    spin_lock_irqsave(&zone->lock, flags);
                    zone->watermark_boost -= min(zone->watermark_boost, zone_boosts[i]);
                    spin_unlock_irqrestore(&zone->lock, flags);
                }

                /* As there is now likely space, wakeup kcompact to defragment pageblocks. */
                wakeup_kcompactd(pgdat, pageblock_order, highest_zoneidx);
            }

            snapshot_refaults(NULL, pgdat);
            __fs_reclaim_release(_THIS_IP_);
            psi_memstall_leave(&pflags);
            set_task_reclaim_state(current, NULL);

            return sc.order;
        }

        if (reclaim_order < alloc_order)
            goto kswapd_try_sleep;
    }

    tsk->flags &= ~(PF_MEMALLOC | PF_KSWAPD);

    return 0;
}
```

# swap

* https://blog.csdn.net/qkhhyga2016/article/details/88722458

![](../images/kernel/mem-swap-arch.svg)

* [[LSF/MM/BPF TOPIC] Integrate Swap Cache, Swap Maps with Swap Allocator](https://lore.kernel.org/all/CAMgjq7BvQ0ZXvyLGp2YP96+i+6COCBBJCYmjXHGBnfisCAb8VA@mail.gmail.com/)
    * [[PATCH v4 00/13] mm, swap: rework of swap allocator locks](https://lore.kernel.org/linux-mm/20250113175732.48099-1-ryncsn@gmail.com/)
    * [[PATCH v3 0/7] mm, swap: remove swap slot cache Kairui Song](https://lore.kernel.org/all/20250313165935.63303-1-ryncsn@gmail.com/)
    * [[PATCH 00/28] mm, swap: introduce swap table](https://lore.kernel.org/linux-mm/20250514201729.48420-1-ryncsn@gmail.com/)
    * [[PATCH 0/9] mm, swap: introduce swap table as swap cache (phase I)](https://lore.kernel.org/lkml/20250822192023.13477-1-ryncsn@gmail.com/)

```md
+-----------------------------------------------------+
| +-----------------------------------------------+   |
| |                 memory management             |   |
| +-----------------------------------------------+   |
|                          +-----------------------+  |
|                          |    +--------------+   |  |
|    +--------------+      |    |  swap core   |   |  |
|    |  swap cache  |      |    +--------------+   |  |
|    +--------------+      |    +--------------+   |  |
|                          |    | block driver |   |  |
|                          |    +--------------+   |  |
|  ------------------------------------------------|  |
|                          |                       |  |
|   +----------------+     | +------+   +-------+  |  |
|   |      ddr       |     | |disk 1|   |disk 2 |  |  |
|   +----------------+     | +------+   +-------+  |  |
|                          +-----------------------+  |
+-----------------------------------------------------+

```

```c
/* One swap address space for each 64M swap space */
#define SWAP_ADDRESS_SPACE_SHIFT    14
#define SWAP_ADDRESS_SPACE_PAGES    (1 << SWAP_ADDRESS_SPACE_SHIFT)

#define SWAPFILE_CLUSTER            256

#define SWAP_BATCH                  64
#define SWAP_SLOTS_CACHE_SIZE       SWAP_BATCH

static unsigned int     nr_swapfiles;
atomic_long_t           nr_swap_pages;

struct swap_info_struct *swap_info[MAX_SWAPFILES];

static PLIST_HEAD(swap_active_head);
static struct plist_head *swap_avail_heads;

/* Describe a swap partition */
struct swap_info_struct {
    struct percpu_ref users;    /* indicate and keep swap device valid. */
    unsigned long    flags;     /* SWP_USED etc: see above */
    signed short    prio;       /* swap priority of this type */
    struct plist_node list;     /* entry in swap_active_head */
    signed char    type;        /* index of swap_info */
    unsigned int    max;        /* extent of the swap_map */
    unsigned char *swap_map; {  /* vmalloc'ed array of usage counts */
        #define SWAP_HAS_CACHE  0x40 /* Flag page is cached, in first swap_map */
        #define COUNT_CONTINUED 0x80 /* Flag swap_map continuation for full count */
        /* Special value in first swap_map */
        #define SWAP_MAP_MAX    0x3e /* Max count */
        #define SWAP_MAP_BAD    0x3f /* Note page is bad */
        #define SWAP_MAP_SHMEM  0xbf /* Owned by shmem/tmpfs */

        /* Special value in each swap_map continuation */
        #define SWAP_CONT_MAX   0x7f /* Max count */

        /* refcnt, inc at try_to_unmap_one */
    }
    struct swap_cluster_info *cluster_info; /* cluster info. Only for SSD */
    struct swap_cluster_list free_clusters; /* free clusters list */

    unsigned int lowest_bit;    /* index of first free in swap_map */
    unsigned int highest_bit;    /* index of last free in swap_map */
    unsigned int pages;        /* total of usable pages of swap */
    unsigned int inuse_pages;    /* number of those currently in use */
    unsigned int cluster_next;    /* likely index for next allocation */
    unsigned int cluster_nr;    /* countdown to next cluster search */
    unsigned int __percpu *cluster_next_cpu; /*percpu index for next allocation */
    struct percpu_cluster __percpu *percpu_cluster; /* per cpu's swap location */

    struct rb_root swap_extent_root;/* root of the swap extent rbtree */

    struct bdev_handle *bdev_handle;/* open handle of the bdev */
    struct block_device *bdev;    /* swap device or bdev of swap file */
    struct file *swap_file;        /* seldom referenced */
    unsigned int old_block_size;    /* seldom referenced */
    struct completion comp;        /* seldom referenced */
    spinlock_t lock;
    spinlock_t cont_lock;
    struct work_struct discard_work; /* discard worker */
    struct swap_cluster_list discard_clusters; /* discard clusters list */

    /* entries in swap_avail_heads, one entry per node. */
    struct plist_node       avail_lists[];
};

/* Describe multiple consecutive storage blocks
 * and the mapping relationship with the sectors in the block device */
struct swap_extent {
    struct rb_node  rb_node; /* insert into swap_extent_root */
    pgoff_t         start_page;
    pgoff_t         nr_pages;
    sector_t        start_block;
};

struct swap_cluster_list {
    struct swap_cluster_info head;
    struct swap_cluster_info tail;
};

struct swap_cluster_info {
    spinlock_t      lock;
    /* stores next cluster if the cluster is free
     * or cluster usage counter otherwise */
    unsigned int    data:24;
    unsigned int    flags:8;
};

struct percpu_cluster {
    struct swap_cluster_info index; /* Current cluster index */
    unsigned int next; /* Likely next allocation offset */
};

static const struct address_space_operations swap_aops = {
    .writepage      = swap_writepage,
    .dirty_folio    = noop_dirty_folio,
    .migrate_folio  = migrate_folio,
};

/* a address_space manages 64MB sized swap cache space */
struct address_space *swapper_spaces[MAX_SWAPFILES];
static unsigned int nr_swapper_spaces[MAX_SWAPFILES];

static DEFINE_PER_CPU(struct swap_slots_cache, swp_slots);

struct swap_slots_cache {
    bool            lock_initialized;
    struct mutex    alloc_lock; /* protects slots, nr, cur */
    swp_entry_t     *slots;
    int             nr;
    int             cur;

    spinlock_t      free_lock;  /* protects slots_ret, n_ret */
    swp_entry_t     *slots_ret;
    int             n_ret;
};
```

```c
static inline swp_entry_t swp_entry(unsigned long type, pgoff_t offset)
{
    swp_entry_t ret;
    ret.val = (type << SWP_TYPE_SHIFT) | (offset & SWP_OFFSET_MASK);
    return ret;
}

static inline unsigned swp_type(swp_entry_t entry)
{
    return (entry.val >> SWP_TYPE_SHIFT);
}

static inline pgoff_t swp_offset(swp_entry_t entry)
{
    return entry.val & SWP_OFFSET_MASK;
}
```

## swapon

```c
SYSCALL_DEFINE2(swapon, const char __user *, specialfile, int, swap_flags)
{
    struct swap_info_struct *p;
    struct filename *name;
    struct file *swap_file = NULL;
    struct address_space *mapping;
    struct dentry *dentry;
    int prio;
    int error;
    union swap_header *swap_header;
    int nr_extents;
    sector_t span;
    unsigned long maxpages;
    unsigned char *swap_map = NULL;
    struct swap_cluster_info *cluster_info = NULL;
    struct page *page = NULL;
    struct inode *inode = NULL;
    bool inced_nr_rotate_swap = false;

    p = alloc_swap_info();
    INIT_WORK(&p->discard_work, swap_discard_work);

    name = getname(specialfile);
    swap_file = file_open_name(name, O_RDWR|O_LARGEFILE, 0);

    p->swap_file = swap_file;
    mapping = swap_file->f_mapping;
    dentry = swap_file->f_path.dentry;
    inode = mapping->host;

    error = claim_swapfile(p, inode) {
        if (S_ISBLK(inode->i_mode)) {
            p->bdev_file = bdev_file_open_by_dev(inode->i_rdev,
                BLK_OPEN_READ | BLK_OPEN_WRITE, p, NULL
            );

            p->bdev = file_bdev(p->bdev_file);
            p->old_block_size = block_size(p->bdev);
            error = set_blocksize(p->bdev, PAGE_SIZE);

            if (bdev_is_zoned(p->bdev))
                return -EINVAL;
            p->flags |= SWP_BLKDEV;
        } else if (S_ISREG(inode->i_mode)) {
            p->bdev = inode->i_sb->s_bdev;
        }
    }

    inode_lock(inode);
    page = read_mapping_page(mapping, 0, swap_file);
    swap_header = kmap(page);

    maxpages = read_swap_header(p, swap_header, inode);

    /* OK, set up the swap map and apply the bad block list */
    swap_map = vzalloc(maxpages);

    if (p->bdev && bdev_stable_writes(p->bdev))
        p->flags |= SWP_STABLE_WRITES;

    if (p->bdev && bdev_synchronous(p->bdev))
        p->flags |= SWP_SYNCHRONOUS_IO;

    if (p->bdev && bdev_nonrot(p->bdev)) {
        int cpu;
        unsigned long ci, nr_cluster;

        p->flags |= SWP_SOLIDSTATE;
        p->cluster_next_cpu = alloc_percpu(unsigned int);

        /* select a random position to start with to help wear leveling SSD */
        for_each_possible_cpu(cpu) {
            per_cpu(*p->cluster_next_cpu, cpu) =
                get_random_u32_inclusive(1, p->highest_bit);
        }
        nr_cluster = DIV_ROUND_UP(maxpages, SWAPFILE_CLUSTER);

        cluster_info = kvcalloc(nr_cluster, sizeof(*cluster_info), GFP_KERNEL);

        p->percpu_cluster = alloc_percpu(struct percpu_cluster);
        for_each_possible_cpu(cpu) {
            struct percpu_cluster *cluster;
            cluster = per_cpu_ptr(p->percpu_cluster, cpu);
            cluster_set_null(&cluster->index);
        }
    } else {
        atomic_inc(&nr_rotate_swap);
        inced_nr_rotate_swap = true;
    }

    error = swap_cgroup_swapon(p->type, maxpages);

    nr_extents = setup_swap_map_and_extents(p, swap_header, swap_map,
        cluster_info, maxpages, &span
    );

    if ((swap_flags & SWAP_FLAG_DISCARD) &&
        p->bdev && bdev_max_discard_sectors(p->bdev)) {

        p->flags |= (SWP_DISCARDABLE | SWP_AREA_DISCARD | SWP_PAGE_DISCARD);
        if (swap_flags & SWAP_FLAG_DISCARD_ONCE)
            p->flags &= ~SWP_PAGE_DISCARD;
        else if (swap_flags & SWAP_FLAG_DISCARD_PAGES)
            p->flags &= ~SWP_AREA_DISCARD;

        /* issue a swapon-time discard if it's still required */
        if (p->flags & SWP_AREA_DISCARD) {
            int err = discard_swap(p);
        }
    }

    error = init_swap_address_space(p->type, maxpages/*nr_pages*/) {
        struct address_space *spaces, *space;
        unsigned int i, nr;

        nr = DIV_ROUND_UP(nr_pages, SWAP_ADDRESS_SPACE_PAGES);
        spaces = kvcalloc(nr, sizeof(struct address_space), GFP_KERNEL);

        for (i = 0; i < nr; i++) {
            space = spaces + i;
            xa_init_flags(&space->i_pages, XA_FLAGS_LOCK_IRQ);
            atomic_set(&space->i_mmap_writable, 0);
            space->a_ops = &swap_aops;
            /* swap cache doesn't use writeback related tags */
            mapping_set_no_writeback_tags(space);
        }
        nr_swapper_spaces[type] = nr;
        swapper_spaces[type] = spaces;

        return 0;
    }

    error = zswap_swapon(p->type, maxpages);

    inode->i_flags |= S_SWAPFILE;
    error = inode_drain_writes(inode);

    mutex_lock(&swapon_mutex);
    prio = -1;
    if (swap_flags & SWAP_FLAG_PREFER) {
        prio = (swap_flags & SWAP_FLAG_PRIO_MASK) >> SWAP_FLAG_PRIO_SHIFT;
    }

    enable_swap_info(p, prio, swap_map, cluster_info) {
        setup_swap_info(p, prio, swap_map, cluster_info) {
            if (prio >= 0)
                p->prio = prio;
            else
                p->prio = --least_priority;

            /* the plist prio is negated because plist ordering is
             * low-to-high, while swap ordering is high-to-low */
            p->list.prio = -p->prio;
            for_each_node(i) {
                if (p->prio >= 0)
                    p->avail_lists[i].prio = -p->prio;
                else {
                    if (swap_node(p) == i)
                        p->avail_lists[i].prio = 1;
                    else
                        p->avail_lists[i].prio = -p->prio;
                }
            }
            p->swap_map = swap_map;
            p->cluster_info = cluster_info;
        }

        _enable_swap_info(p) {
            p->flags |= SWP_WRITEOK;
            atomic_long_add(p->pages, &nr_swap_pages);
            total_swap_pages += p->pages;

            assert_spin_locked(&swap_lock);

            plist_add(&p->list, &swap_active_head);

            /* add to available list iff swap device is not full */
            if (p->highest_bit) {
                add_to_avail_list(p) {
                    for_each_node(nid) {
                        plist_add(&p->avail_lists[nid], &swap_avail_heads[nid]);
                    }
                }
            }
        }
    }

    mutex_unlock(&swapon_mutex);
    atomic_inc(&proc_poll_event);
    wake_up_interruptible(&proc_poll_wait);

    return error;
}
```

### setup_swap_map_and_extents

```c
int setup_swap_map_and_extents(
    struct swap_info_struct *p,
    union swap_header *swap_header,
    unsigned char *swap_map,
    struct swap_cluster_info *cluster_info,
    unsigned long maxpages,
    sector_t *span
) {
    unsigned int j, k;
    unsigned int nr_good_pages;
    int nr_extents;
    unsigned long nr_clusters = DIV_ROUND_UP(maxpages, SWAPFILE_CLUSTER);
    unsigned long col = p->cluster_next / SWAPFILE_CLUSTER % SWAP_CLUSTER_COLS;
    unsigned long i, idx;

    nr_good_pages = maxpages - 1; /* omit header page */

    cluster_list_init(&p->free_clusters);
    cluster_list_init(&p->discard_clusters);

    for (i = 0; i < swap_header->info.nr_badpages; i++) {
        unsigned int page_nr = swap_header->info.badpages[i];
        if (page_nr == 0 || page_nr > swap_header->info.last_page)
            return -EINVAL;
        if (page_nr < maxpages) {
            swap_map[page_nr] = SWAP_MAP_BAD;
            nr_good_pages--;
            inc_cluster_info_page(p, cluster_info, page_nr);
        }
    }

    /* Haven't marked the cluster free yet, no list operation involved */
    for (i = maxpages; i < round_up(maxpages, SWAPFILE_CLUSTER); i++) {
        inc_cluster_info_page(p, cluster_info, i);
    }

    if (nr_good_pages) {
        swap_map[0] = SWAP_MAP_BAD;
        inc_cluster_info_page(p, cluster_info, 0);
        p->max = maxpages;
        p->pages = nr_good_pages;

        nr_extents = setup_swap_extents(p/*sis*/, span) {
            struct file *swap_file = sis->swap_file;
            struct address_space *mapping = swap_file->f_mapping;
            struct inode *inode = mapping->host;
            int ret;

            if (S_ISBLK(inode->i_mode)) {
                ret = add_swap_extent(sis, 0, sis->max, 0);
                *span = sis->pages;
                return ret;
            }

            if (mapping->a_ops->swap_activate) {
                ret = mapping->a_ops->swap_activate(sis, swap_file, span);
                if (ret < 0)
                    return ret;
                sis->flags |= SWP_ACTIVATED;
                if ((sis->flags & SWP_FS_OPS) &&
                    sio_pool_init() != 0) {
                    destroy_swap_extents(sis);
                    return -ENOMEM;
                }
                return ret;
            }

            return generic_swapfile_activate(sis, swap_file, span) {
                struct address_space *mapping = swap_file->f_mapping;
                struct inode *inode = mapping->host;
                unsigned blocks_per_page;
                unsigned long page_no;
                unsigned blkbits;
                sector_t probe_block;
                sector_t last_block;
                sector_t lowest_block = -1;
                sector_t highest_block = 0;
                int nr_extents = 0;
                int ret;

                blkbits = inode->i_blkbits;
                blocks_per_page = PAGE_SIZE >> blkbits;

                /* Map all the blocks into the extent tree.  This code doesn't try
                 * to be very smart. */
                probe_block = 0;
                page_no = 0;
                last_block = i_size_read(inode) >> blkbits;
                while ((probe_block + blocks_per_page) <= last_block && page_no < sis->max) {
                    unsigned block_in_page;
                    sector_t first_block;

                    cond_resched();

                    first_block = probe_block;
                    first_block >>= (PAGE_SHIFT - blkbits);
                    if (page_no) { /* exclude the header page */
                        if (first_block < lowest_block)
                            lowest_block = first_block;
                        if (first_block > highest_block)
                            highest_block = first_block;
                    }

                    /* We found a PAGE_SIZE-length, PAGE_SIZE-aligned run of blocks */
                    ret = add_swap_extent(sis, page_no/*start_page*/, 1/*nr_pages*/, first_block/*start_block*/) {
                        struct rb_node **link = &sis->swap_extent_root.rb_node, *parent = NULL;
                        struct swap_extent *se;
                        struct swap_extent *new_se;

                        while (*link) {
                            parent = *link;
                            link = &parent->rb_right;
                        }

                        if (parent) {
                            se = rb_entry(parent, struct swap_extent, rb_node);
                            if (se->start_block + se->nr_pages == start_block) {
                                se->nr_pages += nr_pages; /* Merge it */
                                return 0;
                            }
                        }

                        /* No merge, insert a new extent. */
                        new_se = kmalloc(sizeof(*se), GFP_KERNEL);
                        new_se->start_page = start_page;
                        new_se->nr_pages = nr_pages;
                        new_se->start_block = start_block;

                        rb_link_node(&new_se->rb_node, parent, link);
                        rb_insert_color(&new_se->rb_node, &sis->swap_extent_root);
                        return 1;
                    }
                    nr_extents += ret;
                    page_no++;
                    probe_block += blocks_per_page;
            reprobe:
                    continue;
                }

                ret = nr_extents;
                *span = 1 + highest_block - lowest_block;
                if (page_no == 0)
                    page_no = 1; /* force Empty message */
                sis->max = page_no;
                sis->pages = page_no - 1;
                sis->highest_bit = page_no - 1;
            out:
                return ret;
            }
        }
        nr_good_pages = p->pages;
    }

    /* Reduce false cache line sharing between cluster_info and
     * sharing same address space. */
    for (k = 0; k < SWAP_CLUSTER_COLS; k++) {
        j = (k + col) % SWAP_CLUSTER_COLS;
        for (i = 0; i < DIV_ROUND_UP(nr_clusters, SWAP_CLUSTER_COLS); i++) {
            idx = i * SWAP_CLUSTER_COLS + j;
            if (idx >= nr_clusters)
                continue;
            if (cluster_count(&cluster_info[idx]))
                continue;
            cluster_set_flag(&cluster_info[idx], CLUSTER_FLAG_FREE);
            cluster_list_add_tail(&p->free_clusters/*list*/, cluster_info/*ci*/, idx) {
                if (cluster_list_empty(list)) {
                    cluster_set_next_flag(&list->head, idx, 0);
                    cluster_set_next_flag(&list->tail, idx, 0);
                } else {
                    struct swap_cluster_info *ci_tail;
                    unsigned int tail = cluster_next(&list->tail);

                    ci_tail = ci + tail;
                    cluster_set_next(ci_tail/*info*/, idx) {
                        info->data = next;
                    }
                    cluster_set_next_flag(&list->tail, idx, 0) {
                        info->flags = flag;
                        info->data = next;
                    }
                }
            }
        }
    }

    enable_swap_slots_cache(cpu) {
        slots = kvcalloc(SWAP_SLOTS_CACHE_SIZE, sizeof(swp_entry_t), GFP_KERNEL);
        slots_ret = kvcalloc(SWAP_SLOTS_CACHE_SIZE, sizeof(swp_entry_t), GFP_KERNEL);

        mutex_lock(&swap_slots_cache_mutex);
        cache = &per_cpu(swp_slots, cpu);

        cache->nr = 0;
        cache->cur = 0;
        cache->n_ret = 0;

        mb();
        cache->slots = slots;
        cache->slots_ret = slots_ret;
        mutex_unlock(&swap_slots_cache_mutex);
        return 0;
    }

    return nr_extents;
}
```

## swapoff

```c
SYSCALL_DEFINE1(swapoff, const char __user *, specialfile)
{
    struct swap_info_struct *p = NULL;
    unsigned char *swap_map;
    struct swap_cluster_info *cluster_info;
    struct file *swap_file, *victim;
    struct address_space *mapping;
    struct inode *inode;
    struct filename *pathname;
    int err, found = 0;
    unsigned int old_block_size;

    if (!capable(CAP_SYS_ADMIN))
        return -EPERM;

    pathname = getname(specialfile);

    victim = file_open_name(pathname, O_RDWR|O_LARGEFILE, 0);
    err = PTR_ERR(victim);

    mapping = victim->f_mapping;
    plist_for_each_entry(p, &swap_active_head, list) {
        if (p->flags & SWP_WRITEOK) {
            if (p->swap_file->f_mapping == mapping) {
                found = 1;
                break;
            }
        }
    }
    if (!found) {
        err = -EINVAL;
        spin_unlock(&swap_lock);
        goto out_dput;
    }

    spin_lock(&p->lock);
    del_from_avail_list(p);
    if (p->prio < 0) {
        struct swap_info_struct *si = p;
        int nid;

        plist_for_each_entry_continue(si, &swap_active_head, list) {
            si->prio++;
            si->list.prio--;
            for_each_node(nid) {
                if (si->avail_lists[nid].prio != 1)
                    si->avail_lists[nid].prio--;
            }
        }
        least_priority++;
    }
    plist_del(&p->list, &swap_active_head);
    atomic_long_sub(p->pages, &nr_swap_pages);
    total_swap_pages -= p->pages;
    p->flags &= ~SWP_WRITEOK;
    spin_unlock(&p->lock);
    spin_unlock(&swap_lock);

    disable_swap_slots_cache_lock();

    set_current_oom_origin();
    err = try_to_unuse(p->type);
    clear_current_oom_origin();

    if (err) {
        /* re-insert swap space back into swap_list */
        reinsert_swap_info(p);
        reenable_swap_slots_cache_unlock();
        goto out_dput;
    }

    reenable_swap_slots_cache_unlock();

    percpu_ref_kill(&p->users);
    synchronize_rcu();
    wait_for_completion(&p->comp);

    flush_work(&p->discard_work);

    destroy_swap_extents(p) {
        while (!RB_EMPTY_ROOT(&sis->swap_extent_root)) {
            struct rb_node *rb = sis->swap_extent_root.rb_node;
            struct swap_extent *se = rb_entry(rb, struct swap_extent, rb_node);

            rb_erase(rb, &sis->swap_extent_root);
            kfree(se);
        }

        if (sis->flags & SWP_ACTIVATED) {
            struct file *swap_file = sis->swap_file;
            struct address_space *mapping = swap_file->f_mapping;

            sis->flags &= ~SWP_ACTIVATED;
            if (mapping->a_ops->swap_deactivate)
                mapping->a_ops->swap_deactivate(swap_file);
        }
    }

    if (p->flags & SWP_CONTINUED)
        free_swap_count_continuations(p);

    if (!p->bdev || !bdev_nonrot(p->bdev))
        atomic_dec(&nr_rotate_swap);

    mutex_lock(&swapon_mutex);
    spin_lock(&swap_lock);
    spin_lock(&p->lock);
    drain_mmlist();

    /* wait for anyone still in scan_swap_map_slots */
    p->highest_bit = 0; /* cuts scans short */
    while (p->flags >= SWP_SCANNING) {
        spin_unlock(&p->lock);
        spin_unlock(&swap_lock);
        schedule_timeout_uninterruptible(1);
        spin_lock(&swap_lock);
        spin_lock(&p->lock);
    }

    swap_file = p->swap_file;
    old_block_size = p->old_block_size;
    p->swap_file = NULL;
    p->max = 0;
    swap_map = p->swap_map;
    p->swap_map = NULL;
    cluster_info = p->cluster_info;
    p->cluster_info = NULL;
    spin_unlock(&p->lock);
    spin_unlock(&swap_lock);
    arch_swap_invalidate_area(p->type);
    zswap_swapoff(p->type);
    mutex_unlock(&swapon_mutex);
    free_percpu(p->percpu_cluster);
    p->percpu_cluster = NULL;
    free_percpu(p->cluster_next_cpu);
    p->cluster_next_cpu = NULL;
    vfree(swap_map);
    kvfree(cluster_info);
    /* Destroy swap account information */
    swap_cgroup_swapoff(p->type);

    exit_swap_address_space(p->type);

    inode = mapping->host;
    if (p->bdev_file) {
        set_blocksize(p->bdev, old_block_size);
        fput(p->bdev_file);
        p->bdev_file = NULL;
    }

    inode_lock(inode);
    inode->i_flags &= ~S_SWAPFILE;
    inode_unlock(inode);
    filp_close(swap_file, NULL);

    spin_lock(&swap_lock);
    p->flags = 0;
    spin_unlock(&swap_lock);

    err = 0;
    atomic_inc(&proc_poll_event);
    wake_up_interruptible(&proc_poll_wait);

out_dput:
    filp_close(victim, NULL);
out:
    putname(pathname);
    return err;
}
```

### swap_discard_work

```c
void swap_discard_work(struct work_struct *work)
{
    struct swap_info_struct *si;

    si = container_of(work, struct swap_info_struct, discard_work);

    spin_lock(&si->lock);
    swap_do_scheduled_discard(si) {
        struct swap_cluster_info *info, *ci;
        unsigned int idx;

        info = si->cluster_info;

        while (!cluster_list_empty(&si->discard_clusters)) {
            idx = cluster_list_del_first(&si->discard_clusters, info);
            spin_unlock(&si->lock);

            discard_swap_cluster(si, idx * SWAPFILE_CLUSTER/*start_page*/, SWAPFILE_CLUSTER/*nr_pages*/) {
                struct swap_extent *se = offset_to_swap_extent(si, start_page) {
                    struct swap_extent *se;
                    struct rb_node *rb;

                    rb = sis->swap_extent_root.rb_node;
                    while (rb) {
                        se = rb_entry(rb, struct swap_extent, rb_node);
                        if (offset < se->start_page)
                            rb = rb->rb_left;
                        else if (offset >= se->start_page + se->nr_pages)
                            rb = rb->rb_right;
                        else
                            return se;
                    }
                }

                while (nr_pages) {
                    pgoff_t offset = start_page - se->start_page;
                    sector_t start_block = se->start_block + offset;
                    sector_t nr_blocks = se->nr_pages - offset;

                    if (nr_blocks > nr_pages)
                        nr_blocks = nr_pages;
                    start_page += nr_blocks;
                    nr_pages -= nr_blocks;

                    start_block <<= PAGE_SHIFT - 9;
                    nr_blocks <<= PAGE_SHIFT - 9;
                    if (blkdev_issue_discard(si->bdev, start_block, nr_blocks, GFP_NOIO))
                        break;

                    se = next_se(se);
                }
            }

            spin_lock(&si->lock);
            ci = lock_cluster(si, idx * SWAPFILE_CLUSTER);
            __free_cluster(si, idx) {
                struct swap_cluster_info *ci = si->cluster_info;

                cluster_set_flag(ci + idx, CLUSTER_FLAG_FREE);
                cluster_list_add_tail(&si->free_clusters, ci, idx);
            }
            memset(si->swap_map + idx * SWAPFILE_CLUSTER, 0, SWAPFILE_CLUSTER);
        }
    }
    spin_unlock(&si->lock);
}
```

### try_to_unuse

```c
int try_to_unuse(unsigned int type)
{
    struct mm_struct *prev_mm;
    struct mm_struct *mm;
    struct list_head *p;
    int retval = 0;
    struct swap_info_struct *si = swap_info[type];
    struct folio *folio;
    swp_entry_t entry;
    unsigned int i;

    if (!READ_ONCE(si->inuse_pages))
        goto success;

retry:
    retval = shmem_unuse(type);
    if (retval)
        return retval;

    prev_mm = &init_mm;
    mmget(prev_mm);

    spin_lock(&mmlist_lock);
    p = &init_mm.mmlist;
    while (READ_ONCE(si->inuse_pages) && !signal_pending(current)
        && (p = p->next) != &init_mm.mmlist) {

        mm = list_entry(p, struct mm_struct, mmlist);
        if (!mmget_not_zero(mm))
            continue;
        spin_unlock(&mmlist_lock);
        mmput(prev_mm);
        prev_mm = mm;
        retval = unuse_mm(mm, type);
        if (retval) {
            mmput(prev_mm);
            return retval;
        }

        cond_resched();
        spin_lock(&mmlist_lock);
    }
    spin_unlock(&mmlist_lock);

    mmput(prev_mm);

    i = 0;
    while (READ_ONCE(si->inuse_pages) && !signal_pending(current)
        && (i = find_next_to_unuse(si, i)) != 0) {

        entry = swp_entry(type, i);
        folio = filemap_get_folio(swap_address_space(entry), i);
        if (IS_ERR(folio))
            continue;

        folio_lock(folio);
        folio_wait_writeback(folio);
        folio_free_swap(folio);
        folio_unlock(folio);
        folio_put(folio);
    }

    /* Lets check again to see if there are still swap entries in the map.
    * If yes, we would need to do retry the unuse logic again.
    * Under global memory pressure, swap entries can be reinserted back
    * into process space after the mmlist loop above passes over them.
    *
    * Limit the number of retries? No: when mmget_not_zero()
    * above fails, that mm is likely to be freeing swap from
    * exit_mmap(), which proceeds at its own independent pace;
    * and even shmem_writepage() could have been preempted after
    * folio_alloc_swap(), temporarily hiding that swap.  It's easy
    * and robust (though cpu-intensive) just to keep retrying. */
    if (READ_ONCE(si->inuse_pages)) {
        if (!signal_pending(current))
            goto retry;
        return -EINTR;
    }

success:
    /* Make sure that further cleanups after try_to_unuse() returns happen
    * after swap_range_free() reduces si->inuse_pages to 0. */
    smp_mb();
    return 0;
}
```

```c
int unuse_mm(struct mm_struct *mm, unsigned int type)
{
    struct vm_area_struct *vma;
    int ret = 0;
    VMA_ITERATOR(vmi, mm, 0);

    mmap_read_lock(mm);
    for_each_vma(vmi, vma) {
        if (vma->anon_vma) {
            ret = unuse_vma(vma, type) {
                pgd_t *pgd;
                unsigned long addr, end, next;
                int ret;

                addr = vma->vm_start;
                end = vma->vm_end;

                pgd = pgd_offset(vma->vm_mm, addr);
                do {
                    next = pgd_addr_end(addr, end);
                    if (pgd_none_or_clear_bad(pgd))
                        continue;
                    ret = unuse_p4d_range(vma, pgd, addr, next, type) {
                        p4d_t *p4d;
                        unsigned long next;
                        int ret;

                        p4d = p4d_offset(pgd, addr);
                        do {
                            next = p4d_addr_end(addr, end);
                            if (p4d_none_or_clear_bad(p4d))
                                continue;
                            ret = unuse_pud_range(vma, p4d, addr, next, type) {
                                pud_t *pud;
                                unsigned long next;
                                int ret;

                                pud = pud_offset(p4d, addr);
                                do {
                                    next = pud_addr_end(addr, end);
                                    if (pud_none_or_clear_bad(pud))
                                        continue;
                                    ret = unuse_pmd_range(vma, pud, addr, next, type) {
                                        pmd_t *pmd;
                                        unsigned long next;
                                        int ret;

                                        pmd = pmd_offset(pud, addr);
                                        do {
                                            cond_resched();
                                            next = pmd_addr_end(addr, end);
                                            ret = unuse_pte_range(vma, pmd, addr, next, type)
                                                --->
                                            if (ret)
                                                return ret;
                                        } while (pmd++, addr = next, addr != end);
                                        return 0;
                                    }
                                    if (ret)
                                        return ret;
                                } while (pud++, addr = next, addr != end);
                                return 0;
                            }
                            if (ret)
                                return ret;
                        } while (p4d++, addr = next, addr != end);
                        return 0;
                    }
                    if (ret)
                        return ret;
                } while (pgd++, addr = next, addr != end);
                return 0;
            }
            if (ret)
                break;
        }

        cond_resched();
    }
    mmap_read_unlock(mm);
    return ret;
}
```

```c
int unuse_pte_range(struct vm_area_struct *vma, pmd_t *pmd,
            unsigned long addr, unsigned long end,
            unsigned int type)
{
    pte_t *pte = NULL;
    struct swap_info_struct *si;

    si = swap_info[type];
    do {
        struct folio *folio;
        unsigned long offset;
        unsigned char swp_count;
        swp_entry_t entry;
        int ret;
        pte_t ptent;

        if (!pte++) {
            pte = pte_offset_map(pmd, addr);
            if (!pte)
                break;
        }

        ptent = ptep_get_lockless(pte);

        if (!is_swap_pte(ptent))
            continue;

        entry = pte_to_swp_entry(ptent);
        if (swp_type(entry) != type)
            continue;

        offset = swp_offset(entry);
        pte_unmap(pte);
        pte = NULL;

        folio = swap_cache_get_folio(entry, vma, addr);
        if (!folio) {
            struct page *page;
            struct vm_fault vmf = {
                .vma = vma,
                .address = addr,
                .real_address = addr,
                .pmd = pmd,
            };

            page = swapin_readahead(entry, GFP_HIGHUSER_MOVABLE, &vmf);
            if (page)
                folio = page_folio(page);
        }
        if (!folio) {
            swp_count = READ_ONCE(si->swap_map[offset]);
            if (swp_count == 0 || swp_count == SWAP_MAP_BAD)
                continue;
            return -ENOMEM;
        }

        folio_lock(folio);
        folio_wait_writeback(folio);
        ret = unuse_pte(vma, pmd, addr, entry, folio);
        if (ret < 0) {
            folio_unlock(folio);
            folio_put(folio);
            return ret;
        }

        folio_free_swap(folio);
        folio_unlock(folio);
        folio_put(folio);
    } while (addr += PAGE_SIZE, addr != end);

    if (pte)
        pte_unmap(pte);
    return 0;
}
```

## folio_alloc_swap

## add_to_swap

```c
bool add_to_swap(struct folio *folio)
{
    swp_entry_t entry;
    int err;

    entry = folio_alloc_swap(folio) {
        swp_entry_t entry;
        struct swap_slots_cache *cache;

        entry.val = 0;

        if (folio_test_large(folio)) {
            if (IS_ENABLED(CONFIG_THP_SWAP) && arch_thp_swp_supported()) {
                get_swap_pages(1, &entry, folio_nr_pages(folio));
            }
            goto out;
        }

        cache = raw_cpu_ptr(&swp_slots);

        if (likely(check_cache_active() && cache->slots)) {
            if (cache->slots) {
    repeat:
                if (cache->nr) {
                    entry = cache->slots[cache->cur];
                    cache->slots[cache->cur++].val = 0;
                    cache->nr--;
                } else if (refill_swap_slots_cache(cache) {
                    if (!use_swap_slot_cache)
                        return 0;
                    cache->cur = 0;
                    if (swap_slot_cache_active)
                        cache->nr = get_swap_pages(SWAP_SLOTS_CACHE_SIZE, cache->slots, 1);
                            --->
                    return cache->nr;
                }) {
                    goto repeat;
                }
            }
            if (entry.val)
                goto out;
        }

        get_swap_pages(1, &entry, 1);
    out:
        if (mem_cgroup_try_charge_swap(folio, entry)) {
            put_swap_folio(folio, entry);
            entry.val = 0;
        }
        return entry;
    }

    err = add_to_swap_cache(folio, entry,
        __GFP_HIGH|__GFP_NOMEMALLOC|__GFP_NOWARN, NULL) {

        struct address_space *address_space = swap_address_space(entry) {
            return &swapper_spaces[swp_type(entry)][swp_offset(entry)
                >> SWAP_ADDRESS_SPACE_SHIFT];
        }
        pgoff_t idx = swp_offset(entry);
        XA_STATE_ORDER(xas, &address_space->i_pages, idx, folio_order(folio));
        unsigned long i, nr = folio_nr_pages(folio);
        void *old;

        xas_set_update(&xas, workingset_update_node);

        folio_ref_add(folio, nr);
        folio_set_swapcache(folio);
        folio->swap = entry;

        do {
            xas_lock_irq(&xas);
            xas_create_range(&xas);
            if (xas_error(&xas))
                goto unlock;
            for (i = 0; i < nr; i++) {
                VM_BUG_ON_FOLIO(xas.xa_index != idx + i, folio);
                if (shadowp) {
                    old = xas_load(&xas);
                    if (xa_is_value(old))
                        *shadowp = old;
                }
                xas_store(&xas, folio);
                xas_next(&xas);
            }
            address_space->nrpages += nr;
            __node_stat_mod_folio(folio, NR_FILE_PAGES, nr);
            __lruvec_stat_mod_folio(folio, NR_SWAPCACHE, nr);
    unlock:
            xas_unlock_irq(&xas);
        } while (xas_nomem(&xas, gfp));

        if (!xas_error(&xas))
            return 0;

        folio_clear_swapcache(folio);
        folio_ref_sub(folio, nr);
        return xas_error(&xas);
    }

    folio_mark_dirty(folio);

    return true;

fail:
    put_swap_folio(folio, entry);
    return false;
}
```

###  get_swap_pages

```c
int get_swap_pages(int n_goal, swp_entry_t swp_entries[], int entry_order) {
    unsigned long size = 1 << swap_entry_order(entry_order); /* nr_page = 1 << page_order */
    struct swap_info_struct *si, *next;
    long avail_pgs;
    int n_ret = 0;
    int node;

    avail_pgs = atomic_long_read(&nr_swap_pages) / size;
    if (avail_pgs <= 0) {
        spin_unlock(&swap_avail_lock);
        goto noswap;
    }

    n_goal = min3((long)n_goal, (long)SWAP_BATCH, avail_pgs);

    atomic_long_sub(n_goal * size, &nr_swap_pages);

start_over:
    node = numa_node_id();
    plist_for_each_entry_safe(si, next, &swap_avail_heads[node], avail_lists[node]) {
        /* requeue si to after same-priority siblings
         * to avoid contention with other cpus */
        plist_requeue(&si->avail_lists[node], &swap_avail_heads[node]);

        if (!si->highest_bit || !(si->flags & SWP_WRITEOK)) {
            if (plist_node_empty(&si->avail_lists[node])) {
                spin_unlock(&si->lock);
                goto nextsi;
            }
              /* this swap partition is used up, rm it from available list */
            __del_from_avail_list(si);
            spin_unlock(&si->lock);
            goto nextsi;
        }
        if (size == SWAPFILE_CLUSTER) {
            if (si->flags & SWP_BLKDEV) {
                n_ret = swap_alloc_cluster(si, swp_entries/*slot*/);
            }
        } else {
            n_ret = scan_swap_map_slots(si, SWAP_HAS_CACHE, n_goal, swp_entries);
        }
        spin_unlock(&si->lock);
        if (n_ret || size == SWAPFILE_CLUSTER)
            goto check_out;
        cond_resched();

        spin_lock(&swap_avail_lock);
nextsi:
        if (plist_node_empty(&next->avail_lists[node]))
            goto start_over;
    }

    spin_unlock(&swap_avail_lock);

check_out:
    if (n_ret < n_goal)
        atomic_long_add((long)(n_goal - n_ret) * size, &nr_swap_pages);
noswap:
    return n_ret;
}
```

### scan_swap_map_slots

```c
scan_swap_map_slots(
    struct swap_info_struct *si,
    unsigned char usage, int nr,
    swp_entry_t slots[])
{
    struct swap_cluster_info *ci;
    unsigned long offset;
    unsigned long scan_base;
    unsigned long last_in_cluster = 0;
    int latency_ration = LATENCY_LIMIT;
    int n_ret = 0;
    bool scanned_many = false;

    si->flags += SWP_SCANNING;

    /* Use percpu scan base for SSD to reduce lock contention on
     * cluster and swap cache.  For HDD, sequential access is more
     * important. */
    if (si->flags & SWP_SOLIDSTATE)
        scan_base = this_cpu_read(*si->cluster_next_cpu);
    else
        scan_base = si->cluster_next;
    offset = scan_base;

    if (si->cluster_info) { /* SSD algorithm */
        if (!scan_swap_map_try_ssd_cluster(si, &offset, &scan_base))
            goto scan;
    } else if (unlikely(!si->cluster_nr--)) {
        if (si->pages - si->inuse_pages < SWAPFILE_CLUSTER) {
            si->cluster_nr = SWAPFILE_CLUSTER - 1;
            goto checks;
        }

        scan_base = offset = si->lowest_bit;
        last_in_cluster = offset + SWAPFILE_CLUSTER - 1;

        /* Locate the first empty (unaligned) cluster */
        for (; last_in_cluster <= si->highest_bit; offset++) {
            if (si->swap_map[offset]) {/* swp_map[offset] != 0 means inuse */
                last_in_cluster = offset + SWAPFILE_CLUSTER;
            } else if (offset == last_in_cluster) {
                spin_lock(&si->lock);
                offset -= SWAPFILE_CLUSTER - 1;
                si->cluster_next = offset;
                si->cluster_nr = SWAPFILE_CLUSTER - 1;
                goto checks;
            }
            if (unlikely(--latency_ration < 0)) {
                cond_resched();
                latency_ration = LATENCY_LIMIT;
            }
        }

        offset = scan_base;
        spin_lock(&si->lock);
        si->cluster_nr = SWAPFILE_CLUSTER - 1;
    }

checks:
    if (si->cluster_info) { /* SSD */
        /* avoid using a free cluster in the middle of free cluster list */
        scan_swap_map_ssd_cluster_conflict(si, offset) {
            struct percpu_cluster *percpu_cluster;
            bool conflict;

            offset /= SWAPFILE_CLUSTER;
            conflict = !cluster_list_empty(&si->free_clusters)
                && offset != cluster_list_first(&si->free_clusters)
                && cluster_is_free(&si->cluster_info[offset]);

            if (!conflict)
                return false;

            percpu_cluster = this_cpu_ptr(si->percpu_cluster);
            cluster_set_null(&percpu_cluster->index);
            return true;
        }
        while (scan_swap_map_ssd_cluster_conflict()) {
        /* take a break if we already got some slots */
            if (n_ret)
                goto done;
            if (!scan_swap_map_try_ssd_cluster(si, &offset, &scan_base))
                goto scan;
        }
    }
    if (!(si->flags & SWP_WRITEOK))
        goto no_page;
    if (!si->highest_bit)
        goto no_page;
    if (offset > si->highest_bit)
        scan_base = offset = si->lowest_bit;

    ci = lock_cluster(si, offset);
    /* reuse swap entry of cache-only swap if not busy. */
    if (vm_swap_full() && si->swap_map[offset] == SWAP_HAS_CACHE) {
        int swap_was_freed;
        swap_was_freed = __try_to_reclaim_swap(si, offset, TTRS_ANYWAY);
        spin_lock(&si->lock);
        /* entry was freed successfully, try to use this again */
        if (swap_was_freed)
            goto checks;
        goto scan; /* check next one */
    }

    if (si->swap_map[offset]) {
        if (!n_ret)
            goto scan;
        else
            goto done;
    }
    WRITE_ONCE(si->swap_map[offset], usage);
    inc_cluster_info_page(si, si->cluster_info, offset) {
        unsigned long idx = page_nr / SWAPFILE_CLUSTER;
        if (!cluster_info)
            return;

        /* The info about idx cluster may be stored in three independent places:
         * 1. percpu_cluster, 2. cluster_info[idx], 3. free_cluster.head
         *
         * free_cluster.head is advanced when 2 is null,
         * alloc_cluster always sets the flags of free_cluster.head to 0 */
        if (cluster_is_free(&cluster_info[idx])) { /* info->flags & CLUSTER_FLAG_FREE */
            alloc_cluster(p, idx) {
                struct swap_cluster_info *ci = si->cluster_info;
                cluster_list_del_first(&si->free_clusters, ci) {
                    idx = cluster_next(&list->head);
                    if (cluster_next(&list->tail) == idx) {
                        cluster_set_null(&list->head);
                        cluster_set_null(&list->tail);
                    } else {
                        cluster_set_next_flag(&list->head, cluster_next(&ci[idx]), 0);
                    }
                }
                cluster_set_count_flag(ci + idx, 0, 0);
            }
        }

        cluster_set_count(&cluster_info[idx], cluster_count(&cluster_info[idx]) + 1);
    }

    swap_range_alloc(si, offset, 1) {
        unsigned int end = offset + nr_entries - 1;

        if (offset == si->lowest_bit)
            si->lowest_bit += nr_entries;
        if (end == si->highest_bit)
            WRITE_ONCE(si->highest_bit, si->highest_bit - nr_entries);

        WRITE_ONCE(si->inuse_pages, si->inuse_pages + nr_entries);
        if (si->inuse_pages == si->pages) {
            si->lowest_bit = si->max;
            si->highest_bit = 0;
            del_from_avail_list(si) {
                for_each_node(nid)
                    plist_del(&p->avail_lists[nid], &swap_avail_heads[nid]);
            }
        }
    }
    slots[n_ret++] = swp_entry(si->type, offset);

    /* got enough slots or reach max slots? */
    if ((n_ret == nr) || (offset >= si->highest_bit))
        goto done;

    /* try to get more slots in cluster */
    if (si->cluster_info) {
        if (scan_swap_map_try_ssd_cluster(si, &offset, &scan_base)) {
            /* return true for successful scan */
            goto checks;
        }
    } else if (si->cluster_nr && !si->swap_map[++offset]) {
        /* non-ssd case, still more slots in cluster? */
        --si->cluster_nr;
        goto checks;
    }

    if (!scanned_many) {
        unsigned long scan_limit;

        if (offset < scan_base)
            scan_limit = scan_base;
        else
            scan_limit = si->highest_bit;
        for (; offset <= scan_limit && --latency_ration > 0; offset++) {
            if (!si->swap_map[offset])
                goto checks;
        }
    }

done:
    set_cluster_next(si, offset + 1) {
        unsigned long prev;
        if (!(si->flags & SWP_SOLIDSTATE)) {
            si->cluster_next = next;
            return;
        }

        prev = this_cpu_read(*si->cluster_next_cpu);
        if ((prev >> SWAP_ADDRESS_SPACE_SHIFT) != (next >> SWAP_ADDRESS_SPACE_SHIFT)) {
            /* No free swap slots available */
            if (si->highest_bit <= si->lowest_bit)
                return;
            next = get_random_u32_inclusive(si->lowest_bit, si->highest_bit);
            next = ALIGN_DOWN(next, SWAP_ADDRESS_SPACE_PAGES);
            next = max_t(unsigned int, next, si->lowest_bit);
        }
        this_cpu_write(*si->cluster_next_cpu, next);
    }
    si->flags -= SWP_SCANNING;

    return n_ret;

scan:
    spin_unlock(&si->lock);
    while (++offset <= READ_ONCE(si->highest_bit)) {
        if (unlikely(--latency_ration < 0)) {
            cond_resched();
            latency_ration = LATENCY_LIMIT;
            scanned_many = true;
        }
        if (swap_offset_available_and_locked(si, offset) {
            if (data_race(!si->swap_map[offset])) {
                spin_lock(&si->lock);
                return true;
            }

            if (vm_swap_full() && READ_ONCE(si->swap_map[offset]) == SWAP_HAS_CACHE) {
                spin_lock(&si->lock);
                return true;
            }

            return false;
        }) {
            goto checks;
        }
    }

    offset = si->lowest_bit;
    while (offset < scan_base) {
        if (unlikely(--latency_ration < 0)) {
            cond_resched();
            latency_ration = LATENCY_LIMIT;
            scanned_many = true;
        }
        if (swap_offset_available_and_locked(si, offset))
            goto checks;
        offset++;
    }
    spin_lock(&si->lock);

no_page:
    si->flags -= SWP_SCANNING;
    return n_ret;
}
```

#### scan_swap_map_try_ssd_cluster

```c
bool scan_swap_map_try_ssd_cluster(struct swap_info_struct *si,
    unsigned long *offset, unsigned long *scan_base)
{
    struct percpu_cluster *cluster;
    struct swap_cluster_info *ci;
    unsigned long tmp, max;

new_cluster:
    cluster = this_cpu_ptr(si->percpu_cluster);
    if (cluster_is_null(&cluster->index)) {
        /* The info about idx cluster may be stored in three independent places:
         * 1. percpu_cluster, 2. cluster_info[idx], 3. free_cluster.head
         *
         * Here 1 and 2 are null, but 3 is not.
         * inc_cluster_info_page will advance free_cluster.head when 2 is null,
         * alloc_cluster always sets the flags of free_cluster.head to 0 */
        if (!cluster_list_empty(&si->free_clusters)) {
            /* After assignment 1 and 3 will be nonull */
            cluster->index = si->free_clusters.head;
            cluster->next = cluster_next(&cluster->index) * SWAPFILE_CLUSTER;
        } else if (!cluster_list_empty(&si->discard_clusters)) {
            swap_do_scheduled_discard(si);
            *scan_base = this_cpu_read(*si->cluster_next_cpu);
            *offset = *scan_base;
            goto new_cluster;
        } else {
            return false;
        }
    }

    /* Other CPUs can use our cluster if they can't find a free cluster,
     * check if there is still free entry in the cluster */
    tmp = cluster->next;
    max = min_t(unsigned long, si->max,
            (cluster_next(&cluster->index) + 1) * SWAPFILE_CLUSTER);
    if (tmp < max) {
        ci = lock_cluster(si, tmp);
        while (tmp < max) {
            if (!si->swap_map[tmp])
                break;
            tmp++;
        }
        unlock_cluster(ci);
    }
    if (tmp >= max) {
        cluster_set_null(&cluster->index);
        goto new_cluster;
    }
    cluster->next = tmp + 1;
    *offset = tmp;
    *scan_base = tmp;
    return true;
}
```

### swap_alloc_cluster

```c
n_ret = swap_alloc_cluster(si, swp_entries/*slot*/) {
    if (cluster_list_empty(&si->free_clusters))
        return 0;

    idx = cluster_list_first(&si->free_clusters);
    offset = idx * SWAPFILE_CLUSTER;
    ci = lock_cluster(si, offset);
    alloc_cluster(si, idx) {
        struct swap_cluster_info *ci = si->cluster_info;
        cluster_list_del_first(&si->free_clusters, ci) {
            idx = cluster_next(&list->head);
            if (cluster_next(&list->tail) == idx) {
                cluster_set_null(&list->head);
                cluster_set_null(&list->tail);
            } else {
                cluster_set_next_flag(&list->head, cluster_next(&ci[idx]), 0);
            }
        }
        cluster_set_count_flag(ci + idx, 0, 0);
    }
    cluster_set_count_flag(ci, SWAPFILE_CLUSTER, CLUSTER_FLAG_HUGE);

    memset(si->swap_map + offset, SWAP_HAS_CACHE, SWAPFILE_CLUSTER);

    swap_range_alloc(si, offset, SWAPFILE_CLUSTER/*nr_entries*/) {
        unsigned int end = offset + nr_entries - 1;

        if (offset == si->lowest_bit)
            si->lowest_bit += nr_entries;
        if (end == si->highest_bit) {
            WRITE_ONCE(si->highest_bit, si->highest_bit - nr_entries);
        }
        WRITE_ONCE(si->inuse_pages, si->inuse_pages + nr_entries);
        if (si->inuse_pages == si->pages) {
            si->lowest_bit = si->max;
            si->highest_bit = 0;
            del_from_avail_list(si) {
                for_each_node(nid) {
                    plist_del(&p->avail_lists[nid], &swap_avail_heads[nid]);
                }
            }
        }
    }

    *slot = swp_entry(si->type, offset);
}
```

## swap_free

```c
/* free a swap entry to cpu swap slot cache,
 * and global entry pool if cpu slot cache is full */
void swap_free(swp_entry_t entry)
{
    struct swap_info_struct *p;

    p = _swap_info_get(entry) {
        return swp_swap_info(entry) {
            return swap_type_to_swap_info(swp_type(entry)) {
                if (type >= MAX_SWAPFILES)
                    return NULL;

                return READ_ONCE(swap_info[type]);
            }
        }
    }

    __swap_entry_free(p, entry) {
        struct swap_cluster_info *ci;
        unsigned long offset = swp_offset(entry);
        unsigned char usage;

        ci = lock_cluster_or_swap_info(p, offset);
        usage = __swap_entry_free_locked(p, offset, 1/*usage*/) {
            unsigned char count;
            unsigned char has_cache;

            count = p->swap_map[offset];

            has_cache = count & SWAP_HAS_CACHE;
            count &= ~SWAP_HAS_CACHE;

            if (usage == SWAP_HAS_CACHE) {
                VM_BUG_ON(!has_cache);
                has_cache = 0;
            } else if (count == SWAP_MAP_SHMEM) {
                /* Or we could insist on shmem.c using a special
                 * swap_shmem_free() and free_shmem_swap_and_cache()... */
                count = 0;
            } else if ((count & ~COUNT_CONTINUED) <= SWAP_MAP_MAX) {
                if (count == COUNT_CONTINUED) {
                    if (swap_count_continued(p, offset, count))
                        count = SWAP_MAP_MAX | COUNT_CONTINUED;
                    else
                        count = SWAP_MAP_MAX;
                } else {
                    count--;
                }
            }

            usage = count | has_cache;
            if (usage)
                WRITE_ONCE(p->swap_map[offset], usage);
            else
                WRITE_ONCE(p->swap_map[offset], SWAP_HAS_CACHE);

            return usage;
        }
        unlock_cluster_or_swap_info(p, ci);

        if (!usage) {
            free_swap_slot(entry) {
                struct swap_slots_cache *cache;

                cache = raw_cpu_ptr(&swp_slots);
                if (likely(use_swap_slot_cache && cache->slots_ret)) {
                    spin_lock_irq(&cache->free_lock);
                    /* Swap slots cache may be deactivated before acquiring lock */
                    if (!use_swap_slot_cache || !cache->slots_ret) {
                        spin_unlock_irq(&cache->free_lock);
                        goto direct_free;
                    }
                    if (cache->n_ret >= SWAP_SLOTS_CACHE_SIZE) {
                        swapcache_free_entries(cache->slots_ret, cache->n_ret);
                        cache->n_ret = 0;
                    }
                    cache->slots_ret[cache->n_ret++] = entry;
                    spin_unlock_irq(&cache->free_lock);
                } else {
            direct_free:
                    /* Return slots to global pool. */
                    swapcache_free_entries(&entry/*entries*/, 1/*n*/) {
                        struct swap_info_struct *p, *prev;
                        int i;

                        if (n <= 0)
                            return;

                        prev = NULL;
                        p = NULL;

                        if (nr_swapfiles > 1) {
                            sort(entries, n, sizeof(entries[0]), swp_entry_cmp, NULL);
                        }
                        for (i = 0; i < n; ++i) {
                            p = swap_info_get_cont(entries[i], prev);
                            swap_entry_free(p, entries[i]/*entry*/) {
                                struct swap_cluster_info *ci;
                                unsigned long offset = swp_offset(entry);
                                unsigned char count;

                                ci = lock_cluster(p, offset);
                                count = p->swap_map[offset];
                                VM_BUG_ON(count != SWAP_HAS_CACHE);
                                p->swap_map[offset] = 0;

                                dec_cluster_info_page(p, p->cluster_info, offset) {
                                    unsigned long idx = page_nr / SWAPFILE_CLUSTER;

                                    VM_BUG_ON(cluster_count(&cluster_info[idx]) == 0);
                                    cluster_set_count(&cluster_info[idx], cluster_count(&cluster_info[idx]) - 1);

                                    if (cluster_count(&cluster_info[idx]) == 0) {
                                        free_cluster(p, idx) {
                                            struct swap_cluster_info *ci = si->cluster_info + idx;
                                            if ((si->flags & (SWP_WRITEOK | SWP_PAGE_DISCARD))
                                                == (SWP_WRITEOK | SWP_PAGE_DISCARD)) {

                                                swap_cluster_schedule_discard(si, idx);
                                                return;
                                            }

                                            __free_cluster(si, idx) {
                                                struct swap_cluster_info *ci = si->cluster_info;

                                                cluster_set_flag(ci + idx, CLUSTER_FLAG_FREE);
                                                cluster_list_add_tail(&si->free_clusters, ci, idx);
                                            }
                                        }
                                    }
                                }
                                unlock_cluster(ci);

                                mem_cgroup_uncharge_swap(entry, 1);
                                swap_range_free(p, offset, 1/*nr_entries*/) {
                                    unsigned long begin = offset;
                                    unsigned long end = offset + nr_entries - 1;
                                    void (*swap_slot_free_notify)(struct block_device *, unsigned long);

                                    if (offset < si->lowest_bit)
                                        si->lowest_bit = offset;
                                    if (end > si->highest_bit) {
                                        bool was_full = !si->highest_bit;

                                        WRITE_ONCE(si->highest_bit, end);
                                        if (was_full && (si->flags & SWP_WRITEOK))
                                            add_to_avail_list(si);
                                    }
                                    atomic_long_add(nr_entries, &nr_swap_pages);
                                    WRITE_ONCE(si->inuse_pages, si->inuse_pages - nr_entries);
                                    if (si->flags & SWP_BLKDEV)
                                        swap_slot_free_notify =
                                            si->bdev->bd_disk->fops->swap_slot_free_notify;
                                    else
                                        swap_slot_free_notify = NULL;

                                    while (offset <= end) {
                                        arch_swap_invalidate_page(si->type, offset);
                                        zswap_invalidate(si->type, offset);
                                        if (swap_slot_free_notify)
                                            swap_slot_free_notify(si->bdev, offset);
                                        offset++;
                                    }
                                    clear_shadow_from_swap_cache(si->type, begin, end) {
                                        unsigned long curr = begin;
                                        void *old;

                                        for (;;) {
                                            swp_entry_t entry = swp_entry(type, curr);
                                            struct address_space *address_space = swap_address_space(entry);
                                            XA_STATE(xas, &address_space->i_pages, curr);

                                            xas_set_update(&xas, workingset_update_node);

                                            xa_lock_irq(&address_space->i_pages);
                                            xas_for_each(&xas, old, end) {
                                                if (!xa_is_value(old))
                                                    continue;
                                                xas_store(&xas, NULL);
                                            }
                                            xa_unlock_irq(&address_space->i_pages);

                                            /* search the next swapcache until we meet end */
                                            curr >>= SWAP_ADDRESS_SPACE_SHIFT;
                                            curr++;
                                            curr <<= SWAP_ADDRESS_SPACE_SHIFT;
                                            if (curr > end)
                                                break;
                                        }
                                    }
                                }
                            }
                            prev = p;
                        }
                        if (p)
                            spin_unlock(&p->lock);
                    }
                }
            }
        }

        return usage;
    }
}
```

## folio_free_swap

```c
/* delete folio from swap cache */
bool folio_free_swap(struct folio *folio) {
    if (!folio_test_swapcache(folio))
        return false;
    if (folio_test_writeback(folio))
        return false;
    if (folio_swapped(folio))
        return false;

    delete_from_swap_cache(folio) {
        swp_entry_t entry = folio->swap;
        struct address_space *address_space = swap_address_space(entry);

        xa_lock_irq(&address_space->i_pages);
        __delete_from_swap_cache(folio, entry, NULL/*shadow*/) {
            struct address_space *address_space = swap_address_space(entry);
            int i;
            long nr = folio_nr_pages(folio);
            pgoff_t idx = swp_offset(entry);
            XA_STATE(xas, &address_space->i_pages, idx);

            xas_set_update(&xas, workingset_update_node);

            VM_BUG_ON_FOLIO(!folio_test_locked(folio), folio);
            VM_BUG_ON_FOLIO(!folio_test_swapcache(folio), folio);
            VM_BUG_ON_FOLIO(folio_test_writeback(folio), folio);

            for (i = 0; i < nr; i++) {
                void *entry = xas_store(&xas, shadow);
                VM_BUG_ON_PAGE(entry != folio, entry);
                xas_next(&xas);
            }
            folio->swap.val = 0;
            folio_clear_swapcache(folio);
            address_space->nrpages -= nr;
            __node_stat_mod_folio(folio, NR_FILE_PAGES, -nr);
            __lruvec_stat_mod_folio(folio, NR_SWAPCACHE, -nr);
        }
        xa_unlock_irq(&address_space->i_pages);

        put_swap_folio(folio, entry);
        folio_ref_sub(folio, folio_nr_pages(folio));
    }

    folio_set_dirty(folio);
    return true;
}
```

## swap_writeout

```c
static pageout_t pageout(struct folio *folio, struct address_space *mapping,
             struct swap_iocb **plug, struct list_head *folio_list)
{
    int (*writeout)(struct folio *, struct writeback_control *);

    /* We no longer attempt to writeback filesystem folios here, other
    * than tmpfs/shmem.  That's taken care of in page-writeback.
    * If we find a dirty filesystem folio at the end of the LRU list,
    * typically that means the filesystem is saturating the storage
    * with contiguous writes and telling it to write a folio here
    * would only make the situation worse by injecting an element
    * of random access.
    *
    * If the folio is swapcache, write it back even if that would
    * block, for some throttling. This happens by accident, because
    * swap_backing_dev_info is bust: it doesn't reflect the
    * congestion state of the swapdevs.  Easy to fix, if needed. */
    if (!is_page_cache_freeable(folio))
        return PAGE_KEEP;
    if (!mapping) {
        /* Some data journaling orphaned folios can have
        * folio->mapping == NULL while being dirty with clean buffers. */
        if (folio_test_private(folio)) {
            if (try_to_free_buffers(folio)) {
                folio_clear_dirty(folio);
                pr_info("%s: orphaned folio\n", __func__);
                return PAGE_CLEAN;
            }
        }
        return PAGE_KEEP;
    }
    if (shmem_mapping(mapping))
        writeout = shmem_writeout;
    else if (folio_test_anon(folio))
        writeout = swap_writeout;
    else
        return PAGE_ACTIVATE;

    if (folio_clear_dirty_for_io(folio)) {
        int res;
        struct writeback_control wbc = {
            .sync_mode = WB_SYNC_NONE,
            .nr_to_write = SWAP_CLUSTER_MAX,
            .range_start = 0,
            .range_end = LLONG_MAX,
            .for_reclaim = 1,
            .swap_plug = plug,
        };

        /* The large shmem folio can be split if CONFIG_THP_SWAP is
        * not enabled or contiguous swap entries are failed to
        * allocate. */
        if (shmem_mapping(mapping) && folio_test_large(folio))
            wbc.list = folio_list;

        folio_set_reclaim(folio);
        res = writeout(folio, &wbc);
        if (res < 0)
            handle_write_error(mapping, folio, res);
        if (res == AOP_WRITEPAGE_ACTIVATE) {
            folio_clear_reclaim(folio);
            return PAGE_ACTIVATE;
        }

        if (!folio_test_writeback(folio)) {
            /* synchronous write? */
            folio_clear_reclaim(folio);
        }
        trace_mm_vmscan_write_folio(folio);
        node_stat_add_folio(folio, NR_VMSCAN_WRITE);
        return PAGE_SUCCESS;
    }

    return PAGE_CLEAN;
}
```

```c
int swap_writeout(struct page *page, struct writeback_control *wbc)
{
    struct folio *folio = page_folio(page);
    int ret;

    if (folio_free_swap(folio)) {
        folio_unlock(folio);
        return 0;
    }
    /* Arch code may have to preserve more data than just the page
     * contents, e.g. memory tags. */
    ret = arch_prepare_to_swap(folio);
    if (ret) {
        folio_mark_dirty(folio);
        folio_unlock(folio);
        return ret;
    }

    /* Use a bitmap (zeromap) to avoid doing IO for zero-filled pages.
     * The bits in zeromap are protected by the locked swapcache folio
     * and atomic updates are used to protect against read-modify-write
     * corruption due to other zero swap entries seeing concurrent updates. */
    if (is_folio_zero_filled(folio)) {
        swap_zeromap_folio_set(folio);
        folio_unlock(folio);
        return 0;
    } else {
        /* Clear bits this folio occupies in the zeromap to prevent
         * zero data being read in from any previous zero writes that
         * occupied the same swap entries. */
        swap_zeromap_folio_clear(folio);
    }
    if (zswap_store(folio)) {
        count_mthp_stat(folio_order(folio), MTHP_STAT_ZSWPOUT);
        folio_unlock(folio);
        return 0;
    }
    if (!mem_cgroup_zswap_writeback_enabled(folio_memcg(folio))) {
        folio_mark_dirty(folio);
        return AOP_WRITEPAGE_ACTIVATE;
    }

    __swap_writepage(folio, wbc) {
        struct swap_info_struct *sis = swp_swap_info(folio->swap);

        VM_BUG_ON_FOLIO(!folio_test_swapcache(folio), folio);
        /* ->flags can be updated non-atomicially (scan_swap_map_slots),
        * but that will never affect SWP_FS_OPS, so the data_race
        * is safe. */
        if (data_race(sis->flags & SWP_FS_OPS))
            swap_writepage_fs(folio, wbc);

        /* ->flags can be updated non-atomicially (scan_swap_map_slots),
        * but that will never affect SWP_SYNCHRONOUS_IO, so the data_race
        * is safe. */
        else if (data_race(sis->flags & SWP_SYNCHRONOUS_IO))
            swap_writepage_bdev_sync(folio, wbc, sis);
        else
            swap_writepage_bdev_async(folio, wbc, sis);
    }
    return 0;
}

static void swap_writepage_bdev_sync(struct folio *folio,
        struct writeback_control *wbc, struct swap_info_struct *sis)
{
    struct bio_vec bv;
    struct bio bio;

    bio_init(&bio, sis->bdev, &bv, 1,
        REQ_OP_WRITE | REQ_SWAP | wbc_to_write_flags(wbc));
    bio.bi_iter.bi_sector = swap_folio_sector(folio) {
        struct swap_info_struct *sis = swp_swap_info(folio->swap);
        struct swap_extent *se;
        sector_t sector;
        pgoff_t offset;

        offset = swp_offset(folio->swap);
        se = offset_to_swap_extent(sis, offset) {
            struct swap_extent *se;
            struct rb_node *rb;

            rb = sis->swap_extent_root.rb_node;
            while (rb) {
                se = rb_entry(rb, struct swap_extent, rb_node);
                if (offset < se->start_page)
                    rb = rb->rb_left;
                else if (offset >= se->start_page + se->nr_pages)
                    rb = rb->rb_right;
                else
                    return se;
            }
            /* It *must* be present */
            BUG();
        }
        sector = se->start_block + (offset - se->start_page);
        return sector << (PAGE_SHIFT - 9);
    }
    bio_add_folio_nofail(&bio, folio, folio_size(folio), 0);

    bio_associate_blkg_from_page(&bio, folio);
    count_swpout_vm_event(folio);

    folio_start_writeback(folio);
    folio_unlock(folio);

    submit_bio_wait(&bio) {
        DECLARE_COMPLETION_ONSTACK_MAP(done, bio->bi_bdev->bd_disk->lockdep_map);

        bio->bi_private = &done;
        bio->bi_end_io = submit_bio_wait_endio;
        bio->bi_opf |= REQ_SYNC;
        submit_bio(bio);
        blk_wait_io(&done);

        return blk_status_to_errno(bio->bi_status);
    }
    __end_swap_bio_write(&bio);
}
```

# mm_core_init

```c
void start_kernel(void) {
    mm_core_init() {
        build_all_zonelists(NULL);
            --->
        page_alloc_init_cpuhp();

        page_ext_init_flatmem();
        mem_debugging_and_hardening_init();
        kfence_alloc_pool();
        report_meminit();
        kmsan_init_shadow();
        stack_depot_early_init();

        mem_init() {
            /* release free pages to the buddy allocator */
            memblock_free_all();
                --->
        }

        mem_init_print_info();
        kmem_cache_init();

        page_ext_init_flatmem_late();
        kmemleak_init();
        ptlock_cache_init();
        pgtable_cache_init();
        debug_objects_mem_init();
        vmalloc_init()
            --->
        /* If no deferred init page_ext now, as vmap is fully initialized */
        if (!deferred_struct_pages)
            page_ext_init();
        /* Should be run before the first non-init thread is created */
        init_espfix_bsp();
        /* Should be run after espfix64 is set up. */
        pti_init();
        kmsan_init_runtime();
        mm_cache_init();
    }
}
```

# memblock

```c
struct memblock {
    bool bottom_up;  /* is bottom up direction? */
    phys_addr_t current_limit;
    struct memblock_type memory;
    struct memblock_type reserved;
};

struct memblock_type {
    unsigned long cnt;
    unsigned long max;
    phys_addr_t total_size;
    struct memblock_region *regions;
    char *name;
};

struct memblock_region {
    phys_addr_t base;
    phys_addr_t size;
    enum memblock_flags flags;
    int nid;
};

static struct memblock_region memblock_memory_init_regions[INIT_MEMBLOCK_MEMORY_REGIONS];
static struct memblock_region memblock_reserved_init_regions[INIT_MEMBLOCK_RESERVED_REGIONS];

struct memblock memblock __initdata_memblock = {
    .memory.regions     = memblock_memory_init_regions,
    .memory.max         = INIT_MEMBLOCK_MEMORY_REGIONS,
    .memory.name        = "memory",

    .reserved.regions   = memblock_reserved_init_regions,
    .reserved.max       = INIT_MEMBLOCK_RESERVED_REGIONS,
    .reserved.name      = "reserved",

    .bottom_up          = false,
    .current_limit      = MEMBLOCK_ALLOC_ANYWHERE,
};
```

## memblock_add

```c
memblock_add(phys_addr_t base, phys_addr_t size)
{
    phys_addr_t end = base + size - 1;

    return memblock_add_range(&memblock.memory, base, size, MAX_NUMNODES, 0) {
        bool insert = false;
        phys_addr_t obase = base;
        phys_addr_t end = base + memblock_cap_size(base, &size);
        int idx, nr_new, start_rgn = -1, end_rgn;
        struct memblock_region *rgn;

        if (!size)
            return 0;

        /* special case for empty array */
        if (type->regions[0].size == 0) {
            WARN_ON(type->cnt != 1 || type->total_size);
            type->regions[0].base = base;
            type->regions[0].size = size;
            type->regions[0].flags = flags;
            memblock_set_region_node(&type->regions[0], nid);
            type->total_size = size;
            return 0;
        }

        if (type->cnt * 2 + 1 <= type->max)
            insert = true;

    repeat:
        /* The following is executed twice.
         * 1. with %false @insert: counts the number of regions needed
         * 2. with %true: inserts them. */
        base = obase;
        nr_new = 0;

        for_each_memblock_type(idx, type, rgn) {
            phys_addr_t rbase = rgn->base;
            phys_addr_t rend = rbase + rgn->size;

            if (rbase >= end)
                break;
            if (rend <= base)
                continue;

            /* @rgn overlaps.  If it separates the lower part of new
             * area, insert that portion. */
            if (rbase > base) {
                nr_new++;
                if (insert) {
                    if (start_rgn == -1)
                        start_rgn = idx;
                    end_rgn = idx + 1;

                    memblock_insert_region(type, idx++, base, rbase - base, nid, flags) {
                        struct memblock_region *rgn = &type->regions[idx];

                        memmove(rgn + 1, rgn, (type->cnt - idx) * sizeof(*rgn));
                        rgn->base = base;
                        rgn->size = size;
                        rgn->flags = flags;
                        memblock_set_region_node(rgn, nid);
                        type->cnt++;
                        type->total_size += size;
                    }
                }
            }
            /* area below @rend is dealt with, forget about it */
            base = min(rend, end);
        }

        /* insert the remaining portion */
        if (base < end) {
            nr_new++;
            if (insert) {
                if (start_rgn == -1)
                    start_rgn = idx;
                end_rgn = idx + 1;
                memblock_insert_region(type, idx, base, end - base, nid, flags);
            }
        }

        if (!nr_new)
            return 0;

        /* If this was the first round, resize array and repeat for actual
         * insertions; otherwise, merge and return. */
        if (!insert) {
            while (type->cnt + nr_new > type->max) {
                ret = memblock_double_array(type, obase, size) {
                    struct memblock_region *new_array, *old_array;
                    phys_addr_t old_alloc_size, new_alloc_size;
                    phys_addr_t old_size, new_size, addr, new_end;
                    int use_slab = slab_is_available();
                    int *in_slab;

                    if (!memblock_can_resize)
                        panic("memblock: cannot resize %s array\n", type->name);

                    /* Calculate new doubled size */
                    old_size = type->max * sizeof(struct memblock_region);
                    new_size = old_size << 1;

                    old_alloc_size = PAGE_ALIGN(old_size);
                    new_alloc_size = PAGE_ALIGN(new_size);

                    if (type == &memblock.memory)
                        in_slab = &memblock_memory_in_slab;
                    else
                        in_slab = &memblock_reserved_in_slab;

                    /* Try to find some space for it */
                    if (use_slab) {
                        new_array = kmalloc(new_size, GFP_KERNEL);
                        addr = new_array ? __pa(new_array) : 0;
                    } else {
                        /* only exclude range when trying to double reserved.regions */
                        if (type != &memblock.reserved)
                            new_area_start = new_area_size = 0;

                        addr = memblock_find_in_range(
                            new_area_start + new_area_size,
                            memblock.current_limit,
                            new_alloc_size, PAGE_SIZE ) {

                        }
                        if (!addr && new_area_size)
                            addr = memblock_find_in_range(0,
                                min(new_area_start, memblock.current_limit),
                                new_alloc_size, PAGE_SIZE);

                        new_array = addr ? __va(addr) : NULL;
                    }
                    if (!addr) {
                        return -1;
                    }

                    new_end = addr + new_size - 1;

                    memcpy(new_array, type->regions, old_size);
                    memset(new_array + type->max, 0, old_size);
                    old_array = type->regions;
                    type->regions = new_array;
                    type->max <<= 1;

                    /* Free old array. We needn't free it if the array is the static one */
                    if (*in_slab)
                        kfree(old_array);
                    else if (old_array != memblock_memory_init_regions &&
                        old_array != memblock_reserved_init_regions)
                        memblock_free(old_array, old_alloc_size);

                    if (!use_slab) {
                        memblock_reserve(addr, new_alloc_size);
                    }

                    *in_slab = use_slab;

                    return 0;
                }
                if (ret < 0)
                    return -ENOMEM;
            }
            insert = true;
            goto repeat;
        } else {
            memblock_merge_regions(type, start_rgn, end_rgn) {
                int i = 0;
                if (start_rgn)
                    i = start_rgn - 1;
                end_rgn = min(end_rgn, type->cnt - 1);
                while (i < end_rgn) {
                    struct memblock_region *this = &type->regions[i];
                    struct memblock_region *next = &type->regions[i + 1];

                    if ((this->base + this->size != next->base)
                        || (memblock_get_region_node(this) != memblock_get_region_node(next))
                        || this->flags != next->flags) {

                        i++;
                        continue;
                    }

                    this->size += next->size;
                    /* move forward from next + 1, index of which is i + 2 */
                    memmove(next, next + 1, (type->cnt - (i + 2)) * sizeof(*next));
                    type->cnt--;
                    end_rgn--;
                }
            }
            return 0;
        }
    }
}
```

## memblock_remove

```c
int memblock_remove(phys_addr_t base, phys_addr_t size)
{
    phys_addr_t end = base + size - 1;

    return memblock_remove_range(&memblock.memory, base, size) {
        int start_rgn, end_rgn;
        int i, ret;

        /* isolate given range into disjoint memblocks */
        ret = memblock_isolate_range(type, base, size, &start_rgn, &end_rgn) {
            phys_addr_t end = base + memblock_cap_size(base, &size);
            int idx;
            struct memblock_region *rgn;

            *start_rgn = *end_rgn = 0;

            if (!size)
                return 0;

            /* we'll create at most two more regions */
            while (type->cnt + 2 > type->max) {
                if (memblock_double_array(type, base, size) < 0)
                    return -ENOMEM;
            }

            for_each_memblock_type(idx, type, rgn) {
                phys_addr_t rbase = rgn->base;
                phys_addr_t rend = rbase + rgn->size;

                if (rbase >= end)
                    break;
                if (rend <= base)
                    continue;

                if (rbase < base) {
                    rgn->base = base;
                    rgn->size -= base - rbase;
                    type->total_size -= base - rbase;
                    memblock_insert_region(
                        type, idx, rbase, base - rbase,
                        memblock_get_region_node(rgn),
                        rgn->flags) {

                        struct memblock_region *rgn = &type->regions[idx];

                        memmove(rgn + 1, rgn, (type->cnt - idx) * sizeof(*rgn));
                        rgn->base = base;
                        rgn->size = size;
                        rgn->flags = flags;
                        memblock_set_region_node(rgn, nid);
                        type->cnt++;
                        type->total_size += size;
                    }
                } else if (rend > end) {
                    rgn->base = end;
                    rgn->size -= end - rbase;
                    type->total_size -= end - rbase;
                    memblock_insert_region(type, idx--, rbase, end - rbase,
                                memblock_get_region_node(rgn),
                                rgn->flags);
                } else {
                    if (!*end_rgn)
                        *start_rgn = idx;
                    *end_rgn = idx + 1;
                }
            }

            return 0;
        }
        if (ret)
            return ret;

        for (i = end_rgn - 1; i >= start_rgn; i--) {
            memblock_remove_region(type, i/*r*/) {
                type->total_size -= type->regions[r].size;
                memmove(&type->regions[r], &type->regions[r + 1],
                    (type->cnt - (r + 1)) * sizeof(type->regions[r]));
                type->cnt--;

                /* Special case for empty arrays */
                if (type->cnt == 0) {
                    type->cnt = 1;
                    type->regions[0].base = 0;
                    type->regions[0].size = 0;
                    type->regions[0].flags = 0;
                    memblock_set_region_node(&type->regions[0], MAX_NUMNODES);
                }
            }
        }
        return 0;
    }
}
```

## memblock_free

```c

```

## memblock_free_all

* start_kernel -> mm_core_init -> mem_init -> memblock_free_all

```c
memblock_free_all() {
    /* The mem_map array can get very big.
        * Free the unused area of the memory map. */
    free_unused_memmap() {
        for_each_mem_pfn_range(i, MAX_NUMNODES, &start, &end, NULL) {
            free_memmap(prev_end, start) {
                /* Free boot memory block previously allocated by memblock_phys_alloc_xx() API.
                * The freeing memory will not be released to the buddy allocator. */
                memblock_phys_free(pg, pgend - pg);
                    memblock_remove_range(&memblock.reserved, base, size) {
                        memblock_isolate_range()
                        memblock_remove_region()
                    }
            }
        }
    }
    reset_all_zones_managed_pages();
        atomic_long_set(&z->managed_pages, 0);

    pages = free_low_memory_core_early() {
        memmap_init_reserved_pages() {
            /* initialize struct pages for the reserved regions */
            for_each_reserved_mem_range(i, &start, &end)
                reserve_bootmem_region(start, end) {
                    for (; start_pfn < end_pfn; start_pfn++) {
                        struct page *page = pfn_to_page(start_pfn);
                        init_reserved_page(start_pfn) {

                        }
                        /* Avoid false-positive PageTail() */
                        INIT_LIST_HEAD(&page->lru);

                        __SetPageReserved(page);

                    }
                }

            /* and also treat struct pages for the NOMAP regions as PageReserved */
            for_each_mem_region(region) {
                if (memblock_is_nomap(region)) {
                    start = region->base;
                    end = start + region->size;
                    reserve_bootmem_region(start, end);
                }
            }
        }

        for_each_free_mem_range() {
            __free_pages()
        }
    }

    totalram_pages_add(pages) {
        atomic_long_add(count, &_totalram_pages);
    }
}
```

# hotplug

* [kern - Memory hotplug](https://docs.kernel.org/core-api/memory-hotplug.html)

## add_memory

```c
int add_memory(int nid, u64 start, u64 size, mhp_t mhp_flags) {
    int rc;

    lock_device_hotplug();
    rc = __add_memory(nid, start, size, mhp_flags) {
        struct resource *res;
        int ret;

        res = register_memory_resource(start, size, "System RAM");
        if (IS_ERR(res))
            return PTR_ERR(res);

        ret = add_memory_resource(nid, res, mhp_flags) {
            ret = memblock_add_node(start, size, nid, memblock_flags) {
                return memblock_add_range(&memblock.memory, base, size, nid, flags);
                    --->
            }

            __try_online_node(nid, false) {
                pg_data_t *pgdat;
                int ret = 1;

                if (node_online(nid))
                    return 0;

                pgdat = hotadd_init_pgdat(nid) {
                    struct pglist_data *pgdat;
                    pgdat = NODE_DATA(nid);
                    free_area_init_core_hotplug(pgdat);
                    build_all_zonelists(pgdat);
                }

                if (set_node_online) {
                    node_set_online(nid);
                    ret = register_one_node(nid);
                    BUG_ON(ret);
                }
            }

            arch_add_memory(nid, start, size, &params) {
                int ret, flags = NO_EXEC_MAPPINGS;

                if (can_set_direct_map())
                    flags |= NO_BLOCK_MAPPINGS | NO_CONT_MAPPINGS;

                __create_pgd_mapping(swapper_pg_dir, start, __phys_to_virt(start),
                    size, params->pgprot, __pgd_pgtable_alloc, flags
                );

                memblock_clear_nomap(start, size);

                ret = __add_pages(nid, start >> PAGE_SHIFT, size >> PAGE_SHIFT, params) {
                    for (; pfn < end_pfn; pfn += cur_nr_pages) {
                        /* Select all remaining pages up to the next section boundary */
                        cur_nr_pages = min(end_pfn - pfn, SECTION_ALIGN_UP(pfn + 1) - pfn);
                        err = sparse_add_section(nid, pfn, cur_nr_pages, altmap, params->pgmap);
                            --->
                        if (err)
                            break;
                        cond_resched();
                    }
                }

                if (ret)
                    __remove_pgd_mapping(swapper_pg_dir, __phys_to_virt(start), size);
                else {
                    max_pfn = PFN_UP(start + size);
                    max_low_pfn = max_pfn;
                }

                return ret;
            }

            ret = create_memory_block_devices(start, size, NULL, group) {
                start_block_id = pfn_to_block_id(PFN_DOWN(start));
                  end_block_id = pfn_to_block_id(PFN_DOWN(start + size)) {
                    return memory_block_id(pfn_to_section_nr(pfn)) {
                        return section_nr / sections_per_block;
                    }
                }
                for (block_id = start_block_id; block_id != end_block_id; block_id++) {
                    ret = add_hotplug_memory_block(block_id, altmap, group) {
                        add_memory_block();
                            --->
                    }
                }
            }

            if (new_node) {
                node_set_online(nid);
                ret = __register_one_node(nid);
                BUG_ON(ret);
            }
            register_memory_blocks_under_node(nid, PFN_DOWN(start),
                PFN_UP(start + size - 1), MEMINIT_HOTPLUG
            );
        }
        if (ret < 0)
            release_memory_resource(res);
        return ret;
    }
    unlock_device_hotplug();

    return rc;
}
```

### add_memory_block

```c

int add_memory_block(unsigned long block_id, unsigned long state,
        struct vmem_altmap *altmap,
        struct memory_group *group)
{
    struct memory_block *mem;
    int ret = 0;

    mem = find_memory_block_by_id(block_id);
    if (mem) {
        put_device(&mem->dev);
        return -EEXIST;
    }
    mem = kzalloc(sizeof(*mem), GFP_KERNEL);
    if (!mem)
        return -ENOMEM;

    mem->start_section_nr = block_id * sections_per_block;
    mem->state = state;
    mem->nid = NUMA_NO_NODE;
    mem->altmap = altmap;
    INIT_LIST_HEAD(&mem->group_next);

#ifndef CONFIG_NUMA
    if (state == MEM_ONLINE)
        /* MEM_ONLINE at this point implies early memory. With NUMA,
        * we'll determine the zone when setting the node id via
        * memory_block_add_nid(). Memory hotplug updated the zone
        * manually when memory onlining/offlining succeeds. */
        mem->zone = early_node_zone_for_memory_block(mem, NUMA_NO_NODE);
#endif /* CONFIG_NUMA */

    ret = __add_memory_block(mem/*memory*/) {
        int ret;

        memory->dev.bus = &memory_subsys;
        memory->dev.id = memory->start_section_nr / sections_per_block;
        memory->dev.release = memory_block_release;
        memory->dev.groups = memory_memblk_attr_groups;
        memory->dev.offline = memory->state == MEM_OFFLINE;

        ret = device_register(&memory->dev) {
            device_initialize(dev);
            return device_add(dev);
        }
        xa_store(&memory_blocks, memory->dev.id, memory, GFP_KERNEL)

        return ret;
    }
    if (ret)
        return ret;

    if (group) {
        mem->group = group;
        list_add(&mem->group_next, &group->memory_blocks);
    }

    return 0;
}
```

## remove_memory

```c
int remove_memory(u64 start, u64 size) {
    int rc;

    lock_device_hotplug();
    rc = try_remove_memory(start, size) {
        int rc, nid = NUMA_NO_NODE;

        rc = walk_memory_blocks(start, size, &nid, check_memblock_offlined_cb);
        if (rc)
            return rc;

        /* remove memmap entry */
        firmware_map_remove(start, start + size, "System RAM");

        mem_hotplug_begin();

        rc = memory_blocks_have_altmaps(start, size);
        if (rc < 0) {
            mem_hotplug_done();
            return rc;
        } else if (!rc) {
            remove_memory_block_devices(start, size);
            arch_remove_memory(start, size, NULL) {
                unsigned long start_pfn = start >> PAGE_SHIFT;
                unsigned long nr_pages = size >> PAGE_SHIFT;

                __remove_pages(start_pfn, nr_pages, altmap);
                __remove_pgd_mapping(swapper_pg_dir, __phys_to_virt(start), size);
            }
        } else {
            /* all memblocks in the range have altmaps */
            remove_memory_blocks_and_altmaps(start, size);
        }

        if (IS_ENABLED(CONFIG_ARCH_KEEP_MEMBLOCK)) {
            memblock_phys_free(start, size);
            memblock_remove(start, size);
        }

        release_mem_region_adjustable(start, size);

        if (nid != NUMA_NO_NODE)
            try_offline_node(nid);

        mem_hotplug_done();
        return 0;
    }
    unlock_device_hotplug();

    return rc;
}
```

## memory_subsys

```c
static const struct bus_type memory_subsys = {
    .name       = MEMORY_CLASS_NAME,
    .dev_name   = MEMORY_CLASS_NAME,
    .online     = memory_subsys_online,
    .offline    = memory_subsys_offline,
};

struct memory_block {
    unsigned long start_section_nr;
    unsigned long state;    /* serialized by the dev->lock */
    int online_type;        /* for passing data to online routine */
    int nid;                /* NID for this memory block */

    struct zone *zone;
    struct device dev;      /* dev.bus = &memory_subsys */
    struct vmem_altmap *altmap;
    struct memory_group *group;     /* group (if any) for this block */
    struct list_head group_next;    /* next block inside memory group */
};

int memory_subsys_online(struct device *dev)
{
    /* container_of(dev, struct memory_block, dev) */
    struct memory_block *mem = to_memory_block(dev);
    int ret;

    if (mem->state == MEM_ONLINE)
        return 0;

    if (mem->online_type == MMOP_OFFLINE)
        mem->online_type = MMOP_ONLINE;

    ret = memory_block_change_state(mem, MEM_ONLINE, MEM_OFFLINE) {
        switch (action) {
        case MEM_ONLINE:
            ret = memory_block_online(mem);
            break;
        case MEM_OFFLINE:
            ret = memory_block_offline(mem);
            break;
        default:
            ret = -EINVAL;
        }
    }
    mem->online_type = MMOP_OFFLINE;

    return ret;
}
```

### memory_block_online

```c
int memory_block_online(struct memory_block *mem)
{
    unsigned long start_pfn = section_nr_to_pfn(mem->start_section_nr);
    unsigned long nr_pages = PAGES_PER_SECTION * sections_per_block;
    unsigned long nr_vmemmap_pages = 0;
    struct zone *zone;
    int ret;

    zone = zone_for_pfn_range(mem->online_type, mem->nid, mem->group, start_pfn, nr_pages);

    if (mem->altmap)
        nr_vmemmap_pages = mem->altmap->free;

    mem_hotplug_begin();
    if (nr_vmemmap_pages) {
        ret = mhp_init_memmap_on_memory(start_pfn, nr_vmemmap_pages, zone);
        if (ret)
            goto out;
    }

    ret = online_pages(start_pfn + nr_vmemmap_pages/*pfn*/,
        nr_pages - nr_vmemmap_pages/*nr_pages*/, zone, mem->group) {

        unsigned long flags;
        int need_zonelists_rebuild = 0;
        const int nid = zone_to_nid(zone);
        int ret;
        struct memory_notify arg;

        /* associate pfn range with the zone */
        move_pfn_range_to_zone(zone, pfn, nr_pages, NULL, MIGRATE_ISOLATE/*migratetype*/) {
            struct pglist_data *pgdat = zone->zone_pgdat;
            int nid = pgdat->node_id;

            clear_zone_contiguous(zone);

            if (zone_is_empty(zone))
                init_currently_empty_zone(zone, start_pfn, nr_pages);
            resize_zone_range(zone, start_pfn, nr_pages);
            resize_pgdat_range(pgdat, start_pfn, nr_pages);

            if (zone_is_zone_device(zone)) {
                if (!IS_ALIGNED(start_pfn, PAGES_PER_SECTION))
                    section_taint_zone_device(start_pfn);
                if (!IS_ALIGNED(start_pfn + nr_pages, PAGES_PER_SECTION))
                    section_taint_zone_device(start_pfn + nr_pages);
            }

            memmap_init_range(nr_pages, nid, zone_idx(zone), start_pfn, 0,
                    MEMINIT_HOTPLUG, altmap, migratetype);
                --->

            set_zone_contiguous(zone);
        }

        arg.start_pfn = pfn;
        arg.nr_pages = nr_pages;
        node_states_check_changes_online(nr_pages, zone, &arg);

        ret = memory_notify(MEM_GOING_ONLINE, &arg);
        ret = notifier_to_errno(ret);

        spin_lock_irqsave(&zone->lock, flags);
        zone->nr_isolate_pageblock += nr_pages / pageblock_nr_pages;
        spin_unlock_irqrestore(&zone->lock, flags);

        if (!populated_zone(zone)) { /* return zone->present_pages; */
            need_zonelists_rebuild = 1;
            setup_zone_pageset(zone) {
                if (sizeof(struct per_cpu_zonestat) > 0)
                    zone->per_cpu_zonestats = alloc_percpu(struct per_cpu_zonestat);

                zone->per_cpu_pageset = alloc_percpu(struct per_cpu_pages);
                for_each_possible_cpu(cpu) {
                    struct per_cpu_pages *pcp;
                    struct per_cpu_zonestat *pzstats;

                    pcp = per_cpu_ptr(zone->per_cpu_pageset, cpu);
                    pzstats = per_cpu_ptr(zone->per_cpu_zonestats, cpu);
                    per_cpu_pages_init(pcp, pzstats);
                }

                zone_set_pageset_high_and_batch(zone, 0);
            }
        }

        online_pages_range(pfn, nr_pages) {
            const unsigned long end_pfn = start_pfn + nr_pages;
            unsigned long pfn;

            for (pfn = start_pfn; pfn < end_pfn;) {
                int order;

                if (pfn)
                    order = min_t(int, MAX_PAGE_ORDER, __ffs(pfn));
                else
                    order = MAX_PAGE_ORDER;

                (*online_page_callback)(pfn_to_page(pfn), order);
                generic_online_page(struct page *page, unsigned int order) {
                    debug_pagealloc_map_pages(page, 1 << order);
                    __free_pages_core(page, order);
                    totalram_pages_add(1UL << order);
                }
                pfn += (1UL << order);
            }

            /* mark all involved sections as online */
            online_mem_sections(start_pfn, end_pfn) {
                for (pfn = start_pfn; pfn < end_pfn; pfn += PAGES_PER_SECTION) {
                    unsigned long section_nr = pfn_to_section_nr(pfn);
                    struct mem_section *ms;

                    /* onlining code should never touch invalid ranges */
                    if (WARN_ON(!valid_section_nr(section_nr)))
                        continue;

                    ms = __nr_to_section(section_nr);
                    ms->section_mem_map |= SECTION_IS_ONLINE;
                }
            }
        }
        adjust_present_page_count(pfn_to_page(pfn), group, nr_pages);

        node_states_set_node(nid, &arg);
        if (need_zonelists_rebuild) {
            build_all_zonelists(NULL);
        }

        /* Basic onlining is complete, allow allocation of onlined pages. */
        undo_isolate_page_range(pfn, pfn + nr_pages, MIGRATE_MOVABLE);

        shuffle_zone(zone) {
            unsigned long i, flags;
            unsigned long start_pfn = z->zone_start_pfn;
            unsigned long end_pfn = zone_end_pfn(z);
            const int order = SHUFFLE_ORDER;
            const int order_pages = 1 << order;

            spin_lock_irqsave(&z->lock, flags);
            start_pfn = ALIGN(start_pfn, order_pages);
            for (i = start_pfn; i < end_pfn; i += order_pages) {
                unsigned long j;
                int migratetype, retry;
                struct page *page_i, *page_j;

                page_i = shuffle_valid_page(z, i, order);
                if (!page_i)
                    continue;

                for (retry = 0; retry < SHUFFLE_RETRY; retry++) {
                    j = z->zone_start_pfn +
                        ALIGN_DOWN(get_random_long() % z->spanned_pages, order_pages);
                    page_j = shuffle_valid_page(z, j, order);
                    if (page_j && page_j != page_i)
                        break;
                }
                if (retry >= SHUFFLE_RETRY) {
                    continue;
                }

                migratetype = get_pageblock_migratetype(page_i);
                if (get_pageblock_migratetype(page_j) != migratetype) {
                    continue;
                }

                list_swap(&page_i->lru, &page_j->lru);

                if ((i % (100 * order_pages)) == 0) {
                    spin_unlock_irqrestore(&z->lock, flags);
                    cond_resched();
                    spin_lock_irqsave(&z->lock, flags);
                }
            }
        }

        /* reinitialise watermarks and update pcp limits */
        init_per_zone_wmark_min() {
            calculate_min_free_kbytes();
            setup_per_zone_wmarks();
            refresh_zone_stat_thresholds();
            setup_per_zone_lowmem_reserve();

            setup_min_unmapped_ratio();
            setup_min_slab_ratio();

            khugepaged_min_free_kbytes_update();
        }

        kswapd_run(nid);
        kcompactd_run(nid);

        writeback_set_ratelimit();

        memory_notify(MEM_ONLINE, &arg);
        return 0;

    failed_addition:
        memory_notify(MEM_CANCEL_ONLINE, &arg);
        remove_pfn_range_from_zone(zone, pfn, nr_pages);
        return ret;
    }

    if (ret) {
        if (nr_vmemmap_pages)
            mhp_deinit_memmap_on_memory(start_pfn, nr_vmemmap_pages);
        goto out;
    }

    if (nr_vmemmap_pages)
        adjust_present_page_count(pfn_to_page(start_pfn), mem->group,
                    nr_vmemmap_pages);

    mem->zone = zone;
out:
    mem_hotplug_done();
    return ret;
}
```

### memory_block_offline

```c
int memory_block_offline(struct memory_block *mem)
{
    unsigned long start_pfn = section_nr_to_pfn(mem->start_section_nr);
    unsigned long nr_pages = PAGES_PER_SECTION * sections_per_block;
    unsigned long nr_vmemmap_pages = 0;
    int ret;

    if (!mem->zone)
        return -EINVAL;

    if (mem->altmap)
        nr_vmemmap_pages = mem->altmap->free;

    mem_hotplug_begin();
    if (nr_vmemmap_pages)
        adjust_present_page_count(pfn_to_page(start_pfn), mem->group,
                    -nr_vmemmap_pages);

    ret = offline_pages(start_pfn + nr_vmemmap_pages,
        nr_pages - nr_vmemmap_pages, mem->zone, mem->group) {

        const unsigned long end_pfn = start_pfn + nr_pages;
        unsigned long pfn, system_ram_pages = 0;
        const int node = zone_to_nid(zone);
        unsigned long flags;
        struct memory_notify arg;
        char *reason;
        int ret;

        walk_system_ram_range(start_pfn, nr_pages, &system_ram_pages,
            count_system_ram_pages_cb);
        if (system_ram_pages != nr_pages) {
            ret = -EINVAL;
            reason = "memory holes";
            goto failed_removal;
        }

        if (WARN_ON_ONCE(page_zone(pfn_to_page(start_pfn)) != zone ||
                page_zone(pfn_to_page(end_pfn - 1)) != zone)) {
            ret = -EINVAL;
            reason = "multizone range";
            goto failed_removal;
        }

        zone_pcp_disable(zone);
        lru_cache_disable();

        /* set above range as isolated */
        ret = start_isolate_page_range(start_pfn, end_pfn,
            MIGRATE_MOVABLE,
            MEMORY_OFFLINE | REPORT_FAILURE,
            GFP_USER | __GFP_MOVABLE | __GFP_RETRY_MAYFAIL);
            --->

        if (ret) {
            reason = "failure to isolate range";
            goto failed_removal_pcplists_disabled;
        }

        arg.start_pfn = start_pfn;
        arg.nr_pages = nr_pages;
        node_states_check_changes_offline(nr_pages, zone, &arg);

        ret = memory_notify(MEM_GOING_OFFLINE, &arg);
        ret = notifier_to_errno(ret);
        if (ret) {
            reason = "notifier failure";
            goto failed_removal_isolated;
        }

        do {
            pfn = start_pfn;
            do {
                if (signal_pending(current)) {
                    ret = -EINTR;
                    reason = "signal backoff";
                    goto failed_removal_isolated;
                }

                cond_resched();

                ret = scan_movable_pages(pfn, end_pfn, &pfn) {

                }
                if (!ret) {
                    do_migrate_range(pfn, end_pfn) {
                        unsigned long pfn;
                        struct page *page, *head;
                        LIST_HEAD(source);
                        static DEFINE_RATELIMIT_STATE(migrate_rs, DEFAULT_RATELIMIT_INTERVAL,
                            DEFAULT_RATELIMIT_BURST);

                        for (pfn = start_pfn; pfn < end_pfn; pfn++) {
                            struct folio *folio;
                            bool isolated;

                            if (!pfn_valid(pfn))
                                continue;
                            page = pfn_to_page(pfn);
                            folio = page_folio(page);
                            head = &folio->page;

                            if (PageHuge(page)) {
                                pfn = page_to_pfn(head) + compound_nr(head) - 1;
                                isolate_hugetlb(folio, &source);
                                continue;
                            } else if (PageTransHuge(page))
                                pfn = page_to_pfn(head) + thp_nr_pages(page) - 1;

                            /* HWPoison pages have elevated reference counts so the migration would
                            * fail on them. It also doesn't make any sense to migrate them in the
                            * first place. Still try to unmap such a page in case it is still mapped
                            * (e.g. current hwpoison implementation doesn't unmap KSM pages but keep
                            * the unmap as the catch all safety net). */
                            if (PageHWPoison(page)) {
                                if (WARN_ON(folio_test_lru(folio)))
                                    folio_isolate_lru(folio);
                                if (folio_mapped(folio))
                                    try_to_unmap(folio, TTU_IGNORE_MLOCK);
                                continue;
                            }

                            if (!get_page_unless_zero(page))
                                continue;
                            /* We can skip free pages. And we can deal with pages on
                             * LRU and non-lru movable pages. */
                            if (PageLRU(page))
                                isolated = isolate_lru_page(page);
                            else
                                isolated = isolate_movable_page(page, ISOLATE_UNEVICTABLE);
                            if (isolated) {
                                list_add_tail(&page->lru, &source);
                                if (!__PageMovable(page))
                                    inc_node_page_state(page, NR_ISOLATED_ANON +
                                                page_is_file_lru(page));

                            } else {
                                if (__ratelimit(&migrate_rs)) {
                                    pr_warn("failed to isolate pfn %lx\n", pfn);
                                    dump_page(page, "isolation failed");
                                }
                            }
                            put_page(page);
                        }
                        if (!list_empty(&source)) {
                            nodemask_t nmask = node_states[N_MEMORY];
                            struct migration_target_control mtc = {
                                .nmask = &nmask,
                                .gfp_mask = GFP_USER | __GFP_MOVABLE | __GFP_RETRY_MAYFAIL,
                            };
                            int ret;

                            /* We have checked that migration range is on a single zone so
                             * we can use the nid of the first page to all the others. */
                            mtc.nid = page_to_nid(list_first_entry(&source, struct page, lru));

                            /* try to allocate from a different node but reuse this node
                             * if there are no other online nodes to be used (e.g. we are
                             * offlining a part of the only existing node) */
                            node_clear(mtc.nid, nmask);
                            if (nodes_empty(nmask))
                                node_set(mtc.nid, nmask);
                            ret = migrate_pages(&source, alloc_migration_target, NULL,
                                (unsigned long)&mtc, MIGRATE_SYNC, MR_MEMORY_HOTPLUG, NULL);
                            if (ret) {
                                list_for_each_entry(page, &source, lru) {
                                    if (__ratelimit(&migrate_rs)) {
                                        pr_warn("migrating pfn %lx failed ret:%d\n",
                                            page_to_pfn(page), ret);
                                        dump_page(page, "migration failure");
                                    }
                                }
                                putback_movable_pages(&source);
                            }
                        }
                    }
                }
            } while (!ret);

            if (ret != -ENOENT) {
                reason = "unmovable page";
                goto failed_removal_isolated;
            }

            /* Dissolve free hugepages in the memory block before doing
             * offlining actually in order to make hugetlbfs's object
             * counting consistent. */
            ret = dissolve_free_huge_pages(start_pfn, end_pfn);
            if (ret) {
                reason = "failure to dissolve huge pages";
                goto failed_removal_isolated;
            }

            ret = test_pages_isolated(start_pfn, end_pfn, MEMORY_OFFLINE);

        } while (ret);

        /* Mark all sections offline and remove free pages from the buddy. */
        __offline_isolated_pages(start_pfn, end_pfn);
        pr_debug("Offlined Pages %ld\n", nr_pages);

        /* The memory sections are marked offline, and the pageblock flags
        * effectively stale; nobody should be touching them. Fixup the number
        * of isolated pageblocks, memory onlining will properly revert this. */
        spin_lock_irqsave(&zone->lock, flags);
        zone->nr_isolate_pageblock -= nr_pages / pageblock_nr_pages;
        spin_unlock_irqrestore(&zone->lock, flags);

        lru_cache_enable();
        zone_pcp_enable(zone);

        /* removal success */
        adjust_managed_page_count(pfn_to_page(start_pfn), -nr_pages);
        adjust_present_page_count(pfn_to_page(start_pfn), group, -nr_pages);

        /* reinitialise watermarks and update pcp limits */
        init_per_zone_wmark_min();

        /* Make sure to mark the node as memory-less before rebuilding the zone
        * list. Otherwise this node would still appear in the fallback lists. */
        node_states_clear_node(node, &arg);
        if (!populated_zone(zone)) {
            zone_pcp_reset(zone);
            build_all_zonelists(NULL);
        }

        if (arg.status_change_nid >= 0) {
            kcompactd_stop(node);
            kswapd_stop(node);
        }

        writeback_set_ratelimit();

        memory_notify(MEM_OFFLINE, &arg);
        remove_pfn_range_from_zone(zone, start_pfn, nr_pages);
        return 0;

    failed_removal_isolated:
        /* pushback to free area */
        undo_isolate_page_range(start_pfn, end_pfn, MIGRATE_MOVABLE);
        memory_notify(MEM_CANCEL_OFFLINE, &arg);
    failed_removal_pcplists_disabled:
        lru_cache_enable();
        zone_pcp_enable(zone);
    failed_removal:
        pr_debug("memory offlining [mem %#010llx-%#010llx] failed due to %s\n",
            (unsigned long long) start_pfn << PAGE_SHIFT,
            ((unsigned long long) end_pfn << PAGE_SHIFT) - 1,
            reason);
        return ret;

    }

    mem->zone = NULL;
out:
    mem_hotplug_done();
    return ret;
}
```

# segment

```c
#define GDT_ENTRY_INIT(flags, base, limit) { { { \
    .a = ((limit) & 0xffff) | (((base) & 0xffff) << 16), \
    .b = (((base) & 0xff0000) >> 16) | (((flags) & 0xf0ff) << 8) | \
      ((limit) & 0xf0000) | ((base) & 0xff000000), \
  } } }

DEFINE_PER_CPU_PAGE_ALIGNED(struct gdt_page, gdt_page) = { .gdt = {
#ifdef CONFIG_X86_64
  [GDT_ENTRY_KERNEL32_CS]       = GDT_ENTRY_INIT(0xc09b, 0, 0xfffff),
  [GDT_ENTRY_KERNEL_CS]         = GDT_ENTRY_INIT(0xa09b, 0, 0xfffff),
  [GDT_ENTRY_KERNEL_DS]         = GDT_ENTRY_INIT(0xc093, 0, 0xfffff),
  [GDT_ENTRY_DEFAULT_USER32_CS] = GDT_ENTRY_INIT(0xc0fb, 0, 0xfffff),
  [GDT_ENTRY_DEFAULT_USER_DS]   = GDT_ENTRY_INIT(0xc0f3, 0, 0xfffff),
  [GDT_ENTRY_DEFAULT_USER_CS]   = GDT_ENTRY_INIT(0xa0fb, 0, 0xfffff),
#else
  [GDT_ENTRY_KERNEL_CS]         = GDT_ENTRY_INIT(0xc09a, 0, 0xfffff),
  [GDT_ENTRY_KERNEL_DS]         = GDT_ENTRY_INIT(0xc092, 0, 0xfffff),
  [GDT_ENTRY_DEFAULT_USER_CS]   = GDT_ENTRY_INIT(0xc0fa, 0, 0xfffff),
  [GDT_ENTRY_DEFAULT_USER_DS]   = GDT_ENTRY_INIT(0xc0f2, 0, 0xfffff),

#endif
} };
EXPORT_PER_CPU_SYMBOL_GPL(gdt_page);

#define __KERNEL_CS      (GDT_ENTRY_KERNEL_CS*8)
#define __KERNEL_DS      (GDT_ENTRY_KERNEL_DS*8)
#define __USER_DS      (GDT_ENTRY_DEFAULT_USER_DS*8 + 3)
#define __USER_CS      (GDT_ENTRY_DEFAULT_USER_CS*8 + 3)
```
![](../images/kernel/mem-segment.png)

# paging

![](../images/kernel/mem-segment-page.png)

![](../images/kernel/mem-kernel-page-table.png)

# user virtual space

```c
/* User space process size. 47bits minus one guard page. */
#define TASK_SIZE_MAX  ((1UL << 47) - PAGE_SIZE)
#define TASK_SIZE    (test_thread_flag(TIF_ADDR32) ? IA32_PAGE_OFFSET : TASK_SIZE_MAX)

struct mm_struct {
    unsigned long mmap_base;  /* base of mmap area */
    unsigned long total_vm;   /* Total pages mapped */
    unsigned long locked_vm;  /* Pages that have PG_mlocked set */
    unsigned long pinned_vm;  /* Refcount permanently increased */
    unsigned long data_vm;    /* VM_WRITE & ~VM_SHARED & ~VM_STACK */
    unsigned long exec_vm;    /* VM_EXEC & ~VM_WRITE & ~VM_STACK */
    unsigned long stack_vm;   /* VM_STACK */
    unsigned long start_code, end_code, start_data, end_data;
    unsigned long start_brk, brk, start_stack;
    unsigned long arg_start, arg_end, env_start, env_end;

    unsigned long task_size; /* size of task vm space */

    struct vm_area_struct *mmap;    /* list of VMAs */
    struct rb_root mm_rb;

    /* Architecture-specific MM context */
    mm_context_t {
        atomic64_t  id;
        refcount_t  pinned;
        void        *vdso;
        unsigned long   flags;
    } context;
};

struct vm_area_struct {
    unsigned long vm_start; /* Our start address within vm_mm. */
    unsigned long vm_end; /* The first byte after our end address within vm_mm. */

    /* linked list of VM areas per task, sorted by address */
    struct vm_area_struct *vm_next, *vm_prev;
    struct rb_node vm_rb;

    struct mm_struct *vm_mm; /* The address space we belong to. */
    struct list_head anon_vma_chain;
    const struct vm_operations_struct *vm_ops;

    struct anon_vma *anon_vma;  /* Serialized by page_table_lock */
    unsigned long vm_pgoff;     /* Offset (within vm_file) in PAGE_SIZE units */
    struct file * vm_file;      /* File we map to (can be NULL). */
    void * vm_private_data;     /* was vm_pte (shared mem) */
} __randomize_layout;
```

![](../images/kernel/mem-vm.png)

# kernel virtual space

```c
/* Documentation/arch/arm64/memory.rst
 *
 * PAGE_OFFSET - the virtual address of the start of the linear map, at the
 *               start of the TTBR1 address space.
 * PAGE_END - the end of the linear map, where all other kernel mappings begin.
 * KIMAGE_VADDR - the virtual address of the start of the kernel image.
 * VA_BITS - the maximum number of bits for virtual addresses. */

 AArch64 Linux memory layout with 4KB pages + 4 levels (48-bit):

  Start                 End                 Size        Use
  -----------------------------------------------------------------------
  0000000000000000      0000ffffffffffff    256TB       user
  ffff000000000000      ffff7fffffffffff    128TB       kernel logical memory map
 [ffff600000000000      ffff7fffffffffff]    32TB       [kasan shadow region]
  ffff800000000000      ffff80007fffffff      2GB       modules
  ffff800080000000      fffffbffefffffff    124TB       vmalloc
  fffffbfff0000000      fffffbfffdffffff    224MB       fixed mappings (top down)
  fffffbfffe000000      fffffbfffe7fffff      8MB       [guard region]
  fffffbfffe800000      fffffbffff7fffff     16MB       PCI I/O space
  fffffbffff800000      fffffbffffffffff      8MB       [guard region]
  fffffc0000000000      fffffdffffffffff      2TB       vmemmap
  fffffe0000000000      ffffffffffffffff      2TB       [guard region]
```

![](../images/kernel/mem-user-kernel-64.png)

# cma

![](../images/kernel/mem-cam.png)

* [wowo tech](http://www.wowotech.net/memory_management/cma.html)
* [CMA技术原理分析 - 内核工匠](https://mp.weixin.qq.com/s/kNHys4p2sXFV6wwV7VDFqQ)
* [LoyenWang - Linux内存管理之CMA](https://www.cnblogs.com/LoyenWang/p/12182594.html)
* [Leo - Linux内存管理之CMA](https://mp.weixin.qq.com/s/ZRC554JSD3K-K6X2QRoKoA)

```c
struct cma {
    unsigned long   count; /* page count */
    unsigned long   available_count;
    unsigned int    order_per_bit; /* Order of pages represented by one bit */
    spinlock_t      lock;
    struct mutex    alloc_mutex;
#ifdef CONFIG_CMA_DEBUGFS
    struct hlist_head       mem_head;
    spinlock_t              mem_head_lock;
#endif
    char                    name[CMA_MAX_NAME];
    int                     nranges;
    struct cma_memrange     ranges[CMA_MAX_RANGES];
#ifdef CONFIG_CMA_SYSFS
    /* the number of CMA page successful allocations */
    atomic64_t              nr_pages_succeeded;
    /* the number of CMA page allocation failures */
    atomic64_t              nr_pages_failed;
    /* the number of CMA page released */
    atomic64_t              nr_pages_released;
    /* kobject requires dynamic object */
    struct cma_kobject      *cma_kobj;
#endif
    unsigned long           flags;
    /* NUMA node (NUMA_NO_NODE if unspecified) */
    int                     nid;
};

struct cma_memrange {
    unsigned long base_pfn;
    unsigned long early_pfn;
    unsigned long count;
    unsigned long *bitmap;
#ifdef CONFIG_CMA_DEBUGFS
    struct debugfs_u32_array dfs_bitmap;
#endif
};
#define CMA_MAX_RANGES 8

# define MAX_CMA_AREAS 20
struct cma cma_areas[MAX_CMA_AREAS];
unsigned cma_area_count;

RESERVEDMEM_OF_DECLARE(cma, "shared-dma-pool", rmem_cma_setup);
RESERVEDMEM_OF_DECLARE(tegra210_emc_table, "nvidia,tegra210-emc-table", tegra210_emc_table_init);
RESERVEDMEM_OF_DECLARE(dma, "restricted-dma-pool", rmem_swiotlb_setup);

/* setup_arch -> arm64_memblock_init -> early_init_fdt_scan_reserved_mem
 * -> fdt_init_reserved_mem -> __reserved_mem_init_node */
#define #define RESERVEDMEM_OF_DECLARE(name, compat, init) \
    _OF_DECLARE(reservedmem, name, compat, init, reservedmem_of_init_fn)

#define _OF_DECLARE(table, name, compat, fn, fn_type) \
    static const struct of_device_id __of_table_##name \
        __used __section("__" #table "_of_table") \ /* __reservedmem_of_table */
        __aligned(__alignof__(struct of_device_id)) \
        = { .compatible = compat,   \
            .data = (fn == (fn_type)NULL) ? fn : fn  }

struct of_device_id {
    char        name[32];
    char        type[32];
    char        compatible[128];
    const void  *data;
};

/* arch/arm64/boot/dts/broadcom/bcm2712.dtsi */
cma: linux,cma {
    compatible = "shared-dma-pool";
    size = <0x0 0x4000000>; /* 64MB */
    reusable;
    linux,cma-default;
    alloc-ranges = <0x0 0x00000000 0x0 0x40000000>;
};

static int __init __reserved_mem_init_node(struct reserved_mem *rmem)
{
    extern const struct of_device_id __reservedmem_of_table[];
    const struct of_device_id *i;
    int ret = -ENOENT;

    for (i = __reservedmem_of_table; i < &__rmem_of_table_sentinel; i++) {
        reservedmem_of_init_fn initfn = i->data;
        const char *compat = i->compatible;

        if (!of_flat_dt_is_compatible(rmem->fdt_node, compat))
            continue;

        ret = initfn(rmem); /* rmem_cma_setup */
        if (ret == 0) {
            break;
        }
    }
    return ret;
}
```

## rmem_cma_setup

```c
struct reserved_mem {
    const char                      *name;
    unsigned long                   fdt_node;
    const struct reserved_mem_ops   *ops; /* rmem_cma_ops */
    phys_addr_t                     base;
    phys_addr_t                     size;
    void                            *priv; /* point to cma */
};

static int __init rmem_cma_setup(struct reserved_mem *rmem)
{
    unsigned long node = rmem->fdt_node;
    bool default_cma = of_get_flat_dt_prop(node, "linux,cma-default", NULL);
    struct cma *cma;
    int err;

    if (size_cmdline != -1 && default_cma) {
        pr_info("Reserved memory: bypass %s node, using cmdline CMA params instead\n",
            rmem->name);
        return -EBUSY;
    }

    if (!of_get_flat_dt_prop(node, "reusable", NULL) || of_get_flat_dt_prop(node, "no-map", NULL))
        return -EINVAL;

    if (!IS_ALIGNED(rmem->base | rmem->size, CMA_MIN_ALIGNMENT_BYTES)) {
        pr_err("Reserved memory: incorrect alignment of CMA region\n");
        return -EINVAL;
    }

    err = cma_init_reserved_mem(rmem->base, rmem->size, 0, rmem->name, &cma/*res_cma*/) {
        cma = &cma_areas[cma_area_count];

        snprintf(cma->name, CMA_MAX_NAME,  "cma%d\n", cma_area_count);

        cma->base_pfn = PFN_DOWN(base);
        cma->count = size >> PAGE_SHIFT;
        cma->order_per_bit = order_per_bit;
        *res_cma = cma;
        cma_area_count++;
        totalcma_pages += (size / PAGE_SIZE);
    }

    /* Architecture specific contiguous memory fixup. */
    dma_contiguous_early_fixup(rmem->base, rmem->size);

    if (default_cma)
        dma_contiguous_default_area = cma;

    rmem->ops = &rmem_cma_ops;
    rmem->priv = cma;

    pr_info("Reserved memory: created CMA memory pool at %pa, size %ld MiB\n",
        &rmem->base, (unsigned long)rmem->size / SZ_1M);

    return 0;
}
```

```c
static const struct reserved_mem_ops rmem_cma_ops = {
    .device_init    = rmem_cma_device_init(struct reserved_mem *rmem, struct device *dev) {
        dev->cma_area = rmem->priv/* cma */;
    },
    .device_release = rmem_cma_device_release(struct reserved_mem *rmem, struct device *dev) {
        dev->cma_area = NULL;
    },
};
```

## cma_init_reserved_areas

![](../images/kernel/mem-cma_init_reserved_areas.png)

```c
core_initcall(cma_init_reserved_areas);

cma_init_reserved_areas(void) {
    for (i = 0; i < cma_area_count; i++) {
        cma_activate_area(&cma_areas[i]/*cma*/) {
            unsigned long base_pfn = cma->base_pfn, pfn;
            struct zone *zone;

            cma->bitmap = bitmap_zalloc(cma_bitmap_maxno(cma), GFP_KERNEL);

            zone = page_zone(pfn_to_page(base_pfn));
            for (pfn = base_pfn + 1; pfn < base_pfn + cma->count; pfn++) {
                if (page_zone(pfn_to_page(pfn)) != zone)
                    goto not_in_zone;
            }

            for (pfn = base_pfn; pfn < base_pfn + cma->count; pfn += pageblock_nr_pages) {
                init_cma_reserved_pageblock(pfn_to_page(pfn)) {
                    unsigned i = pageblock_nr_pages;
                    struct page *p = page;

                    do {
                        __ClearPageReserved(p);
                        set_page_count(p, 0);
                    } while (++p, --i);

                    init_pageblock_migratetype(page, MIGRATE_CMA, false) {
                        if (unlikely(page_group_by_mobility_disabled && migratetype < MIGRATE_PCPTYPES)) {
                            migratetype = MIGRATE_UNMOVABLE;
                        }

                        flags = migratetype;

                    #ifdef CONFIG_MEMORY_ISOLATION
                        if (migratetype == MIGRATE_ISOLATE) {
                            VM_WARN_ONCE( 1, "Set isolate=true to isolate pageblock with a migratetype");
                            return;
                        }
                        if (isolate)
                            flags |= BIT(PB_migrate_isolate);
                    #endif

                        __set_pfnblock_flags_mask(page, page_to_pfn(page), flags, MIGRATETYPE_AND_ISO_MASK) {
                            unsigned long *bitmap_word;
                            unsigned long bitidx;
                            unsigned long word;

                            get_pfnblock_bitmap_bitidx(page, pfn, &bitmap_word, &bitidx) {
                                	unsigned long *bitmap;
                                unsigned long word_bitidx;

                            #ifdef CONFIG_MEMORY_ISOLATION
                                BUILD_BUG_ON(NR_PAGEBLOCK_BITS != 8);
                            #else
                                BUILD_BUG_ON(NR_PAGEBLOCK_BITS != 4);
                            #endif
                                BUILD_BUG_ON(__MIGRATE_TYPE_END > MIGRATETYPE_MASK);
                                VM_BUG_ON_PAGE(!zone_spans_pfn(page_zone(page), pfn), page);

                                bitmap = get_pageblock_bitmap(page, pfn);
                                *bitidx = pfn_to_bitidx(page, pfn);
                                word_bitidx = *bitidx / BITS_PER_LONG;
                                *bitidx &= (BITS_PER_LONG - 1);
                                *bitmap_word = &bitmap[word_bitidx];
                            }

                            mask <<= bitidx;
                            flags <<= bitidx;

                            word = READ_ONCE(*bitmap_word);
                            do {
                            } while (!try_cmpxchg(bitmap_word, &word, (word & ~mask) | flags));
                        }
                    }
                    set_page_refcounted(page);
                    /* pages were reserved and not allocated */
                    clear_page_tag_ref(page);

                    /* free pages to buddy system */
                    __free_pages(page, pageblock_order);

                    adjust_managed_page_count(page, pageblock_nr_pages);
                    page_zone(page)->cma_pages += pageblock_nr_pages;
                }
            }

            return;

        not_in_zone:
            bitmap_free(cma->bitmap);
        out_error:
            /* Expose all pages to the buddy, they are useless for CMA. */
            if (!cma->reserve_pages_on_error) {
                for (pfn = base_pfn; pfn < base_pfn + cma->count; pfn++) {
                    free_reserved_page(pfn_to_page(pfn)) {

                    }
                }
            }
            totalcma_pages -= cma->count;
            cma->count = 0;
            pr_err("CMA area %s could not be activated\n", cma->name);
            return;
        }
    }
}
```

## cma_alloc

![](../images/kernel/mem-cma_alloc.svg)

```c
cma_alloc(cma_heap->cma, pagecount, align, false);
cma_alloc(kvm_cma, nr_pages, order_base_2(HPT_ALIGN_PAGES), false);
cma_alloc(dev_get_cma_area(dev), count, align, no_warn);

struct page *cma_alloc(struct cma *cma, unsigned long count,
            unsigned int align, bool no_warn)
{
    return __cma_alloc(cma, count, align, GFP_KERNEL | (no_warn ? __GFP_NOWARN : 0));
}

static struct page *__cma_alloc(struct cma *cma, unsigned long count,
            unsigned int align, gfp_t gfp)
{
    struct page *page = NULL;
    int ret = -ENOMEM, r;
    unsigned long i;
    const char *name = cma ? cma->name : NULL;

    trace_cma_alloc_start(name, count, align);

    if (!cma || !cma->count)
        return page;

    if (!count)
        return page;

    for (r = 0; r < cma->nranges; r++) {
        page = NULL;
        ret = cma_range_alloc(cma, &cma->ranges[r], count, align, &page, gfp);
        if (ret != -EBUSY || page)
            break;
    }

    if (page) {
        for (i = 0; i < count; i++)
            page_kasan_tag_reset(nth_page(page, i));
    }

    if (ret && !(gfp & __GFP_NOWARN)) {
        cma_debug_show_areas(cma);
    }

    trace_cma_alloc_finish(name, page ? page_to_pfn(page) : 0,
                page, count, align, ret);
    if (page) {
        count_vm_event(CMA_ALLOC_SUCCESS);
        cma_sysfs_account_success_pages(cma, count);
    } else {
        count_vm_event(CMA_ALLOC_FAIL);
        cma_sysfs_account_fail_pages(cma, count);
    }

    return page;
}

static int cma_range_alloc(struct cma *cma, struct cma_memrange *cmr,
                unsigned long count, unsigned int align,
                struct page **pagep, gfp_t gfp) {

    unsigned long mask, offset;
    unsigned long pfn = -1;
    unsigned long start = 0;
    unsigned long bitmap_maxno, bitmap_no, bitmap_count;
    unsigned long i;
    struct page *page = NULL;
    int ret = -ENOMEM;

    mask = cma_bitmap_aligned_mask(cma, align) {
        if (align_order <= cma->order_per_bit)
            return 0;
        return (1UL << (align_order - cma->order_per_bit)) - 1;

    }
    offset = cma_bitmap_aligned_offset(cma, align) {
        return (cmr->base_pfn & ((1UL << align_order) - 1))
            >> cma->order_per_bit;
    }
    bitmap_maxno = cma_bitmap_maxno(cma, cmr) {
        return cmr->count >> cma->order_per_bit;
    }
    bitmap_count = cma_bitmap_pages_to_bits(cma, count) {
        return ALIGN(pages, 1UL << cma->order_per_bit) >> cma->order_per_bit;
    }

    if (bitmap_count > bitmap_maxno)
        goto out;

    for (;;) {
        spin_lock_irq(&cma->lock);
        if (count > cma->available_count) {
            spin_unlock_irq(&cma->lock);
            break;
        }

        bitmap_no = bitmap_find_next_zero_area_off(cma->bitmap,
            bitmap_maxno, start, bitmap_count, mask, offset
        );

        bitmap_set(cma->bitmap, bitmap_no, bitmap_count);
        cma->available_count -= count;
        spin_unlock_irq(&cma->lock);

        pfn = cma->base_pfn + (bitmap_no << cma->order_per_bit);

        mutex_lock(&cma->alloc_mutex);
        ret = alloc_contig_range(pfn, pfn + count, MIGRATE_CMA, gfp_mask) {
            unsigned long outer_start, outer_end;
            int ret = 0;

            struct compact_control cc = {
                .nr_migratepages = 0,
                .order = -1,
                .zone = page_zone(pfn_to_page(start)),
                .mode = MIGRATE_SYNC,
                .ignore_skip_hint = true,
                .no_set_skip_hint = true,
                .gfp_mask = current_gfp_context(gfp_mask),
                .alloc_contig = true,
            };
            INIT_LIST_HEAD(&cc.migratepages);

/* 1. moves free pages to the MIGRATE_ISOLATE freelist */
            gfp_mask = current_gfp_context(gfp_mask);
            ret = start_isolate_page_range(start, end, migratetype, 0/*flag*/, gfp_mask) {
                unsigned long pfn;
                struct page *page;
                /* isolation is done at page block granularity */
                unsigned long isolate_start = pageblock_start_pfn(start_pfn);
                unsigned long isolate_end = pageblock_align(end_pfn);
                int ret;
                bool skip_isolation = false;

                drain_all_pages(cc.zone);

                /* start_pfn = 0x1200, end_pfn = 0x2800, and pageblock_nr_pages = 0x400 (1024 pages)
                 * isolate_start = pageblock_start_pfn(0x1200) = 0x1000
                 * isolate_end = pageblock_align(0x2800) = 0x2c00
                 * Range: [0x1000, 0x2c00) spans three blocks:
                 *      First Pageblock:        [0x1000, 0x1400)
                 *      Last Pageblock:         [0x1400, 0x1800)
                 *      Intermediate Pageblock: [0x1800, 0x2c00) */

                /* isolate [isolate_start, isolate_start + pageblock_nr_pages) pageblock */
                ret = isolate_single_pageblock(isolate_start, flags/*0*/, false, skip_isolation, migratetype);
                if (ret) /* failed */
                    return ret;

                if (isolate_start == isolate_end - pageblock_nr_pages)
                    skip_isolation = true;

                /* isolate [isolate_end - pageblock_nr_pages, isolate_end) pageblock */
                ret = isolate_single_pageblock(isolate_end, flags, true, skip_isolation, migratetype);
                if (ret) {
                    unset_migratetype_isolate(pfn_to_page(isolate_start), migratetype);
                    return ret;
                }

                /* skip isolated pageblocks at the beginning and end */
                for (pfn = isolate_start + pageblock_nr_pages;
                    pfn < isolate_end - pageblock_nr_pages;
                    pfn += pageblock_nr_pages) {

                    page = __first_valid_page(pfn, pageblock_nr_pages);
                    if (page && set_migratetype_isolate()) {
                        undo_isolate_page_range();
                        unset_migratetype_isolate(
                            pfn_to_page(isolate_end - pageblock_nr_pages),
                            migratetype
                        );
                        return -EBUSY;
                    }
                }
            }
            if (ret)
                goto done;

            drain_all_pages(cc.zone);

/* 2. migrate any in-use pages in the range to other locations, freeing up the range for allocation */
            ret = __alloc_contig_migrate_range(&cc, start, end, migratetype/*MIGRATE_CMA*/);
            if (ret && ret != -EBUSY)
                goto done;

            /* locate a large free memory block that precedes or includes the given start_pfn.*/
            outer_start = find_large_buddy(unsigned long start_pfn) {
                int order = 0;
                struct page *page;
                unsigned long pfn = start_pfn;

                while (!PageBuddy(page = pfn_to_page(pfn))) {
                    /* Nothing found */
                    if (++order > MAX_PAGE_ORDER)
                        return start_pfn;
                    pfn &= ~0UL << order;
                }

                /* Found a preceding buddy, but does it straddle? */
                if (pfn + (1 << buddy_order(page)) > start_pfn)
                    return pfn;

                /* Nothing found */
                return start_pfn;
            }

            /* Make sure the range is really isolated. */
            if (test_pages_isolated(outer_start, end, 0)) {
                ret = -EBUSY;
                goto done;
            }

/* 3. moves pages from the MIGRATE_ISOLATE freelist to cc->freepages
 * for the specific allocation request, removing them from the buddy allocator’s control. */
            outer_end = isolate_freepages_range(&cc, outer_start, end);
            if (!outer_end) {
                ret = -EBUSY;
                goto done;
            }

            if (!(gfp_mask & __GFP_COMP)) {
                split_free_pages(cc.freepages, gfp_mask);

                /* Free head and tail (if any) */
                if (start != outer_start)
                    free_contig_range(outer_start, start - outer_start);
                if (end != outer_end)
                    free_contig_range(end, outer_end - end);
            } else if (start == outer_start && end == outer_end && is_power_of_2(end - start)) {
                struct page *head = pfn_to_page(start);
                int order = ilog2(end - start);

                check_new_pages(head, order);
                prep_new_page(head, order, gfp_mask, 0);
                set_page_refcounted(head);
            } else {
                ret = -EINVAL;
                WARN(true, "PFN range: requested [%lu, %lu), allocated [%lu, %lu)\n",
                    start, end, outer_start, outer_end);
            }
        done:
/* 4. reset the migrateype to MIGRATE_CMA */
            undo_isolate_page_range(start, end, migratetype);
            return ret;
        }
        mutex_unlock(&cma->alloc_mutex);

        if (ret == 0) {
            page = pfn_to_page(pfn);
            break;
        }

        cma_clear_bitmap(cma, pfn, count);
        if (ret != -EBUSY)
            break;

        /* try again with a bit different memory target */
        start = bitmap_no + mask + 1;
    }

out:
    *pagep = page;
    return ret;
}
```

### isolate_single_pageblock

```c
int isolate_single_pageblock(unsigned long boundary_pfn, int flags,
        bool isolate_before, bool skip_isolation, int migratetype)
{
    unsigned long start_pfn;
    unsigned long isolate_pageblock;
    unsigned long pfn;
    struct zone *zone;
    int ret;

    VM_BUG_ON(!pageblock_aligned(boundary_pfn));

    if (isolate_before)
        isolate_pageblock = boundary_pfn - pageblock_nr_pages;
    else
        isolate_pageblock = boundary_pfn;

    /* scan at the beginning of MAX_ORDER_NR_PAGES aligned range to avoid
     * only isolating a subset of pageblocks from a bigger than pageblock
     * free or in-use page. Also make sure all to-be-isolated pageblocks
     * are within the same zone. */
    zone = page_zone(pfn_to_page(isolate_pageblock));
    start_pfn  = max(ALIGN_DOWN(isolate_pageblock, MAX_ORDER_NR_PAGES), zone->zone_start_pfn);

    if (skip_isolation) {
        int mt __maybe_unused = get_pageblock_migratetype(pfn_to_page(isolate_pageblock));

        VM_BUG_ON(!is_migrate_isolate(mt));
    } else {
        ret = set_migratetype_isolate(pfn_to_page(isolate_pageblock), migratetype,
            flags, isolate_pageblock, isolate_pageblock + pageblock_nr_pages
        );
        if (ret)
            return ret;
    }

    if (isolate_before) {
        if (!pfn_to_online_page(boundary_pfn))
            return 0;
    } else {
        if (!pfn_to_online_page(boundary_pfn - 1))
            return 0;
    }

    for (pfn = start_pfn; pfn < boundary_pfn;) {
        struct page *page = __first_valid_page(pfn, boundary_pfn - pfn);

        VM_BUG_ON(!page);
        pfn = page_to_pfn(page);

        if (PageUnaccepted(page)) {
            pfn += MAX_ORDER_NR_PAGES;
            continue;
        }

        if (PageBuddy(page)) {
            int order = buddy_order(page);
            VM_WARN_ON_ONCE(pfn + (1 << order) > boundary_pfn);
            pfn += 1UL << order;
            continue;
        }

        /* If a compound page is straddling our block, attempt
        * to migrate it out of the way.
        *
        * We don't have to worry about this creating a large
        * free page that straddles into our block: gigantic
        * pages are freed as order-0 chunks, and LRU pages
        * (currently) do not exceed pageblock_order.
        *
        * The block of interest has already been marked
        * MIGRATE_ISOLATE above, so when migration is done it
        * will free its pages onto the correct freelists. */
        if (PageCompound(page)) {
            struct page *head = compound_head(page);
            unsigned long head_pfn = page_to_pfn(head);
            unsigned long nr_pages = compound_nr(head);

            if (head_pfn + nr_pages <= boundary_pfn || PageHuge(page)) {
                pfn = head_pfn + nr_pages;
                continue;
            }

            /* These pages are movable too, but they're
            * not expected to exceed pageblock_order.
            *
            * Let us know when they do, so we can add
            * proper free and split handling for them. */
            VM_WARN_ON_ONCE_PAGE(PageLRU(page), page);
            VM_WARN_ON_ONCE_PAGE(__PageMovable(page), page);

            goto failed;
        }

        pfn++;
    }
    return 0;

failed:
    /* restore the original migratetype */
    if (!skip_isolation)
        unset_migratetype_isolate(pfn_to_page(isolate_pageblock), migratetype);
    return -EBUSY;
}
```

#### set_migratetype_isolate

```c
static int set_migratetype_isolate(struct page *page, int migratetype, int isol_flags,
            unsigned long start_pfn, unsigned long end_pfn)
{
    struct zone *zone = page_zone(page);
    struct page *unmovable;
    unsigned long flags;
    unsigned long check_unmovable_start, check_unmovable_end;

    if (PageUnaccepted(page))
        accept_page(page);

    spin_lock_irqsave(&zone->lock, flags);

    /* We assume the caller intended to SET migrate type to isolate.
    * If it is already set, then someone else must have raced and
    * set it before us. */
    if (is_migrate_isolate_page(page)) {
        spin_unlock_irqrestore(&zone->lock, flags);
        return -EBUSY;
    }

    /* FIXME: Now, memory hotplug doesn't call shrink_slab() by itself.
    * We just check MOVABLE pages.
    *
    * Pass the intersection of [start_pfn, end_pfn) and the page's pageblock
    * to avoid redundant checks. */
    check_unmovable_start = max(page_to_pfn(page), start_pfn);
    check_unmovable_end = min(pageblock_end_pfn(page_to_pfn(page)), end_pfn);

    /* return the first unmovable page */
    unmovable = has_unmovable_pages(check_unmovable_start, check_unmovable_end, migratetype, isol_flags) {
        struct page *page = pfn_to_page(start_pfn);
        struct zone *zone = page_zone(page);
        unsigned long pfn;

        VM_BUG_ON(pageblock_start_pfn(start_pfn) != pageblock_start_pfn(end_pfn - 1));

        if (is_migrate_cma_page(page)) {
            if (is_migrate_cma(migratetype))
                return NULL;
            return page;
        }

        for (pfn = start_pfn; pfn < end_pfn; pfn++) {
            page = pfn_to_page(pfn);

            if (PageReserved(page))
                return page;

            if (zone_idx(zone) == ZONE_MOVABLE)
                continue;

            if (PageHuge(page) || PageTransCompound(page)) {
                struct folio *folio = page_folio(page);
                unsigned int skip_pages;

                if (PageHuge(page)) {
                    struct hstate *h;
                    h = size_to_hstate(folio_size(folio));
                    if (h && !hugepage_migration_supported(h))
                        return page;
                } else if (!folio_test_lru(folio) && !__folio_test_movable(folio)) {
                    return page;
                }

                skip_pages = folio_nr_pages(folio) - folio_page_idx(folio, page);
                pfn += skip_pages - 1;
                continue;
            }

            if (!page_ref_count(page)) {
                if (PageBuddy(page))
                    pfn += (1 << buddy_order(page)) - 1;
                continue;
            }

            if ((flags & MEMORY_OFFLINE) && PageHWPoison(page))
                continue;

            if ((flags & MEMORY_OFFLINE) && PageOffline(page))
                continue;

            if (__PageMovable(page) || PageLRU(page))
                continue;

            return page;
        }
        return NULL;
    }

    if (!unmovable) {
        ret = move_freepages_block_isolate(zone, page, MIGRATE_ISOLATE) {
            unsigned long start_pfn, pfn;

            if (!prep_move_freepages_block(zone, page, &start_pfn, NULL, NULL))
                return false;

            /* No splits needed if buddies can't span multiple blocks */
            if (pageblock_order == MAX_PAGE_ORDER)
                goto move;

            /* We're a tail block in a larger buddy */
            pfn = find_large_buddy(start_pfn);
            if (pfn != start_pfn) {
                struct page *buddy = pfn_to_page(pfn);
                int order = buddy_order(buddy);

                del_page_from_free_list(buddy, zone, order, get_pfnblock_migratetype(buddy, pfn));
                set_pageblock_migratetype(page, migratetype);
                split_large_buddy(zone, buddy, pfn, order, FPI_NONE);
                return true;
            }

            /* We're the starting block of a larger buddy */
            if (PageBuddy(page) && buddy_order(page) > pageblock_order) {
                int order = buddy_order(page);

                del_page_from_free_list(page, zone, order, get_pfnblock_migratetype(page, pfn));
                set_pageblock_migratetype(page, migratetype);
                split_large_buddy(zone, page, pfn, order, FPI_NONE);
                return true;
            }
        move:
            __move_freepages_block(zone, start_pfn, get_pfnblock_migratetype(page, start_pfn), migratetype) {
                struct page *page;
                unsigned long pfn, end_pfn;
                unsigned int order;
                int pages_moved = 0;

                VM_WARN_ON(start_pfn & (pageblock_nr_pages - 1));
                end_pfn = pageblock_end_pfn(start_pfn);

                for (pfn = start_pfn; pfn < end_pfn;) {
                    page = pfn_to_page(pfn);
                    /* __ClearPageBuddy at __del_page_from_free_list
                     * return data_race(page->page_type >> 24) == PGTY_buddy */
                    if (!PageBuddy(page)) {
                        pfn++;
                        continue;
                    }

                    /* Make sure we are not inadvertently changing nodes */
                    VM_BUG_ON_PAGE(page_to_nid(page) != zone_to_nid(zone), page);
                    VM_BUG_ON_PAGE(page_zone(page) != zone, page);

                    order = buddy_order(page);

                    move_to_free_list(page, zone, order, old_mt, new_mt) {
                        struct free_area *area = &zone->free_area[order];
                        int nr_pages = 1 << order;

                        list_move_tail(&page->buddy_list, &area->free_list[new_mt]);

                        account_freepages(zone, -nr_pages, old_mt);
                        account_freepages(zone, nr_pages, new_mt);

                        if (order >= pageblock_order && is_migrate_isolate(old_mt) != is_migrate_isolate(new_mt)) {
                            if (!is_migrate_isolate(old_mt))
                                nr_pages = -nr_pages;
                            __mod_zone_page_state(zone, NR_FREE_PAGES_BLOCKS, nr_pages);
                        }
                    }

                    pfn += 1 << order;
                    pages_moved += 1 << order;
                }

                set_pageblock_migratetype(pfn_to_page(start_pfn), new_mt);

                return pages_moved;
            }
            return true;
        }
        if (!ret) {
            spin_unlock_irqrestore(&zone->lock, flags);
            return -EBUSY;
        }
        zone->nr_isolate_pageblock++;
        spin_unlock_irqrestore(&zone->lock, flags);
        return 0;
    }

    spin_unlock_irqrestore(&zone->lock, flags);
    if (isol_flags & REPORT_FAILURE) {
        dump_page(unmovable, "  unmovable page");
    }

    return -EBUSY;
}
```

### __alloc_contig_migrate_range

```c
int __alloc_contig_migrate_range(struct compact_control *cc,
        unsigned long start, unsigned long end, int migratetype)
{
    /* This function is based on compact_zone() from compaction.c. */
    unsigned int nr_reclaimed;
    unsigned long pfn = start;
    unsigned int tries = 0;
    int ret = 0;
    struct migration_target_control mtc = {
        .nid = zone_to_nid(cc->zone),
        .gfp_mask = cc->gfp_mask,
        .reason = MR_CONTIG_RANGE,
    };
    struct page *page;
    unsigned long total_mapped = 0;
    unsigned long total_migrated = 0;
    unsigned long total_reclaimed = 0;

    lru_cache_disable();

    while (pfn < end || !list_empty(&cc->migratepages)) {
        if (fatal_signal_pending(current)) {
            ret = -EINTR;
            break;
        }

        if (list_empty(&cc->migratepages)) {
            cc->nr_migratepages = 0;
            ret = isolate_migratepages_range(cc, pfn, end) {
                unsigned long pfn, block_start_pfn, block_end_pfn;
                int ret = 0;

                /* Scan block by block. First and last block may be incomplete */
                pfn = start_pfn;
                block_start_pfn = pageblock_start_pfn(pfn);
                if (block_start_pfn < cc->zone->zone_start_pfn)
                    block_start_pfn = cc->zone->zone_start_pfn;
                block_end_pfn = pageblock_end_pfn(pfn);

                for (; pfn < end_pfn;
                    pfn = block_end_pfn,
                    block_start_pfn = block_end_pfn,
                    block_end_pfn += pageblock_nr_pages) {

                    block_end_pfn = min(block_end_pfn, end_pfn);

                    if (!pageblock_pfn_to_page(block_start_pfn, block_end_pfn, cc->zone))
                        continue;

                    ret = isolate_migratepages_block(cc, pfn, block_end_pfn, ISOLATE_UNEVICTABLE);
                    if (ret)
                        break;

                    if (cc->nr_migratepages >= COMPACT_CLUSTER_MAX)
                        break;
                }

                return ret;
            }
            if (ret && ret != -EAGAIN)
                break;
            pfn = cc->migrate_pfn;
            tries = 0;
        } else if (++tries == 5) {
            ret = -EBUSY;
            break;
        }

        /* Reclaims clean pages from a provided list by attempting
         * to free them back to the buddy allocator. */
        nr_reclaimed = reclaim_clean_pages_from_list(cc->zone, &cc->migratepages) {
            struct scan_control sc = {
                .gfp_mask = GFP_KERNEL,
                .may_unmap = 1,
            };
            struct reclaim_stat stat;
            unsigned int nr_reclaimed;
            struct folio *folio, *next;
            LIST_HEAD(clean_folios);
            unsigned int noreclaim_flag;

            list_for_each_entry_safe(folio, next, folio_list, lru) {
                if (!folio_test_hugetlb(folio) && folio_is_file_lru(folio) &&
                    !folio_test_dirty(folio) && !__folio_test_movable(folio) &&
                    !folio_test_unevictable(folio)) {

                    folio_clear_active(folio);
                    list_move(&folio->lru, &clean_folios);
                }
            }

            noreclaim_flag = memalloc_noreclaim_save();
            nr_reclaimed = shrink_folio_list(&clean_folios, zone->zone_pgdat, &sc, &stat, true);
            memalloc_noreclaim_restore(noreclaim_flag);

            list_splice(&clean_folios, folio_list);
            mod_node_page_state(zone->zone_pgdat, NR_ISOLATED_FILE, -(long)nr_reclaimed);
            mod_node_page_state(zone->zone_pgdat, NR_ISOLATED_ANON, stat.nr_lazyfree_fail);
            mod_node_page_state(zone->zone_pgdat, NR_ISOLATED_FILE, -(long)stat.nr_lazyfree_fail);

            return nr_reclaimed;
        }
        cc->nr_migratepages -= nr_reclaimed;

        ret = migrate_pages(&cc->migratepages, alloc_migration_target,
            NULL, (unsigned long)&mtc, cc->mode, MR_CONTIG_RANGE, NULL);

        /* On -ENOMEM, migrate_pages() bails out right away. It is pointless
         * to retry again over this error, so do the same here. */
        if (ret == -ENOMEM)
            break;
    }

    lru_cache_enable();
    if (ret < 0) {
        if (!(cc->gfp_mask & __GFP_NOWARN) && ret == -EBUSY)
            alloc_contig_dump_pages(&cc->migratepages);
        putback_movable_pages(&cc->migratepages);
    }

    return (ret < 0) ? ret : 0;
}
```

### isolate_freepages_range

```c
unsigned long
isolate_freepages_range(struct compact_control *cc,
            unsigned long start_pfn, unsigned long end_pfn)
{
    unsigned long isolated, pfn, block_start_pfn, block_end_pfn;
    int order;

    for (order = 0; order < NR_PAGE_ORDERS; order++)
        INIT_LIST_HEAD(&cc->freepages[order]);

    pfn = start_pfn;
    block_start_pfn = pageblock_start_pfn(pfn);
    if (block_start_pfn < cc->zone->zone_start_pfn)
        block_start_pfn = cc->zone->zone_start_pfn;
    block_end_pfn = pageblock_end_pfn(pfn);

    for (; pfn < end_pfn;
        pfn += isolated, block_start_pfn = block_end_pfn, block_end_pfn += pageblock_nr_pages) {

        /* Protect pfn from changing by isolate_freepages_block */
        unsigned long isolate_start_pfn = pfn;

        /* pfn could pass the block_end_pfn if isolated freepage
        * is more than pageblock order. In this case, we adjust
        * scanning range to right one. */
        if (pfn >= block_end_pfn) {
            block_start_pfn = pageblock_start_pfn(pfn);
            block_end_pfn = pageblock_end_pfn(pfn);
        }

        block_end_pfn = min(block_end_pfn, end_pfn);

        if (!pageblock_pfn_to_page(block_start_pfn, block_end_pfn, cc->zone))
            break;

        isolated = isolate_freepages_block(cc, &isolate_start_pfn, block_end_pfn, cc->freepages, 0, true);
            --->

        /* In strict mode, isolate_freepages_block() returns 0 if
         * there are any holes in the block (ie. invalid PFNs or
         * non-free pages). */
        if (!isolated)
            break;

        /* If we managed to isolate pages, it is always (1 << n) *
         * pageblock_nr_pages for some non-negative n.  (Max order
         * page may span two pageblocks). */
    }

    if (pfn < end_pfn) {
        /* Loop terminated early, cleanup. */
        release_free_list(cc->freepages);
        return 0;
    }

    /* We don't use freelists for anything. */
    return pfn;
}
```

### undo_isolate_page_range

```c
void undo_isolate_page_range(unsigned long start_pfn, unsigned long end_pfn, int migratetype) {
    unsigned long pfn;
    struct page *page;
    unsigned long isolate_start = pageblock_start_pfn(start_pfn);
    unsigned long isolate_end = pageblock_align(end_pfn);

    for (pfn = isolate_start;
        pfn < isolate_end;
        pfn += pageblock_nr_pages) {
        page = __first_valid_page(pfn, pageblock_nr_pages);
        if (!page || !is_migrate_isolate_page(page))
            continue;

        unset_migratetype_isolate(page, migratetype) {
            struct zone *zone;
            unsigned long flags, nr_pages;
            bool isolated_page = false;
            unsigned int order;
            struct page *buddy;

            zone = page_zone(page);
            spin_lock_irqsave(&zone->lock, flags);
            if (!is_migrate_isolate_page(page))
                goto out;

            if (PageBuddy(page)) {
                order = buddy_order(page);
                if (order >= pageblock_order && order < MAX_PAGE_ORDER) {
                    buddy = find_buddy_page_pfn(page, page_to_pfn(page), order, NULL);
                    if (buddy && !is_migrate_isolate_page(buddy)) {
                        isolated_page = !!__isolate_free_page(page, order);
                            --->
                        VM_WARN_ON(!isolated_page);
                    }
                }
            }


            if (!isolated_page) {
                move_freepages_block_isolate(zone, page, migratetype) {
                    /* ISOATE pages are not PGTY_Buddy, so
                     * __move_freepages_block just reset the migrateype
                     * and doesn't mv pages */
                }
            } else {
                set_pageblock_migratetype(page, migratetype);
                __putback_isolated_page(page, order, migratetype) {
                    __free_one_page(page, page_to_pfn(page), zone, order, mt,
                        FPI_SKIP_REPORT_NOTIFY | FPI_TO_TAIL
                    );
                }
            }
            zone->nr_isolate_pageblock--;
        out:
            spin_unlock_irqrestore(&zone->lock, flags);
        }
    }
}
```

## cma_release

```c
bool cma_release(struct cma *cma, const struct page *pages, unsigned long count)
{
    struct cma_memrange *cmr;
    unsigned long pfn, end_pfn;
    int r;

    if (!cma_pages_valid(cma, pages, count))
        return false;

    pfn = page_to_pfn(pages);
    end_pfn = pfn + count;

    for (r = 0; r < cma->nranges; r++) {
        cmr = &cma->ranges[r];
        if (pfn >= cmr->base_pfn && pfn < (cmr->base_pfn + cmr->count)) {
            VM_BUG_ON(end_pfn > cmr->base_pfn + cmr->count);
            break;
        }
    }

    if (r == cma->nranges)
        return false;

    free_contig_range(pfn, count) {
        unsigned long count = 0;
        struct folio *folio = pfn_folio(pfn);

        if (folio_test_large(folio)) {
            int expected = folio_nr_pages(folio);

            if (nr_pages == expected)
                folio_put(folio);
            else
                WARN(true, "PFN %lu: nr_pages %lu != expected %d\n",
                    pfn, nr_pages, expected);
            return;
        }

        for (; nr_pages--; pfn++) {
            struct page *page = pfn_to_page(pfn);
            count += page_count(page) != 1;
            /* free to MIGRATE_CMA list */
            __free_page(page);
        }
    }

    cma_clear_bitmap(cma, cmr, pfn, count) {
        unsigned long bitmap_no, bitmap_count;
        unsigned long flags;

        bitmap_no = (pfn - cmr->base_pfn) >> cma->order_per_bit;
        bitmap_count = cma_bitmap_pages_to_bits(cma, count);

        spin_lock_irqsave(&cma->lock, flags);
        bitmap_clear(cmr->bitmap, bitmap_no, bitmap_count);
        cma->available_count += count;
        spin_unlock_irqrestore(&cma->lock, flags);
    }

    cma_sysfs_account_release_pages(cma, count) {
        atomic64_add(nr_pages, &cma->nr_pages_released);
    }

    return true;
}
```

# brk

```c
SYSCALL_DEFINE1(brk, unsigned long, brk)
{
    unsigned long retval;
    unsigned long newbrk, oldbrk;
    struct mm_struct *mm = current->mm;
    struct vm_area_struct *next;

    newbrk = PAGE_ALIGN(brk);
    oldbrk = PAGE_ALIGN(mm->brk);
    if (oldbrk == newbrk)
        goto set_brk;

    /* Always allow shrinking brk. */
    if (brk <= mm->brk) {
        if (!do_munmap(mm, newbrk, oldbrk-newbrk, &uf))
            goto set_brk;
        goto out;
    }

    /* Check against existing mmap mappings. */
    next = find_vma(mm, oldbrk);
    if (next && newbrk + PAGE_SIZE > vm_start_gap(next))
        goto out;

    /* Ok, looks good - let it rip. */
    if (do_brk(oldbrk, newbrk-oldbrk, &uf) < 0)
        goto out;

set_brk:
    mm->brk = brk;

    return brk;
out:
    retval = mm->brk;
    return retval
}

static int do_brk(unsigned long addr, unsigned long len, struct list_head *uf)
{
    return do_brk_flags(addr, len, 0, uf);
}

int do_brk_flags(unsigned long addr, unsigned long len, unsigned long flags, struct list_head *uf)
{
    struct mm_struct *mm = current->mm;
    struct vm_area_struct *vma, *prev;
    struct rb_node **rb_link, *rb_parent;
    pgoff_t pgoff = addr >> PAGE_SHIFT;
    int error;

    error = get_unmapped_area(NULL, addr, len, 0, MAP_FIXED);
    if (offset_in_page(error))
        return error;

    /* Clear old maps.  this also does some error checking for us */
    while (find_vma_links(mm, addr, addr + len, &prev, &rb_link, &rb_parent)) {
        if (do_munmap(mm, addr, len, uf))
            return -ENOMEM;
    }

    /* Can we just expand an old private anonymous mapping? */
    vma = vma_merge(mm, prev, addr, addr + len, flags, NULL, NULL, pgoff, NULL, NULL_VM_UFFD_CTX);
    if (vma)
        goto out;

    /* create a vma struct for an anonymous mapping */
    vma = vm_area_alloc(mm);
    if (!vma) {
        vm_unacct_memory(len >> PAGE_SHIFT);
        return -ENOMEM;
    }

    vma_set_anonymous(vma);
    vma->vm_start = addr;
    vma->vm_end = addr + len;
    vma->vm_pgoff = pgoff;
    vma->vm_flags = flags;
    vma->vm_page_prot = vm_get_page_prot(flags);
    vma_link(mm, vma, prev, rb_link, rb_parent);

out:
    perf_event_mmap(vma);
    mm->total_vm += len >> PAGE_SHIFT;
    mm->data_vm += len >> PAGE_SHIFT;
    if (flags & VM_LOCKED)
        mm->locked_vm += (len >> PAGE_SHIFT);
    vma->vm_flags |= VM_SOFTDIRTY;
    return 0;
}

unsigned long get_unmapped_area(
  struct file *file, unsigned long addr, unsigned long len,
  unsigned long pgoff, unsigned long flags)
{
    unsigned long (*get_area)(struct file *, unsigned long,
          unsigned long, unsigned long, unsigned long);

    unsigned long error = arch_mmap_check(addr, len, flags);
    if (error)
        return error;

    /* Careful about overflows, 3G */
    if (len > TASK_SIZE)
        return -ENOMEM;

    get_area = current->mm->get_unmapped_area;
    if (file) {
        if (file->f_op->get_unmapped_area)
        get_area = file->f_op->get_unmapped_area;
    } else if (flags & MAP_SHARED) {
        /* mmap_region() will call shmem_zero_setup() to create a file,
        * so use shmem's get_unmapped_area in case it can be huge.
        * do_mmap_pgoff() will clear pgoff, so match alignment. */
        pgoff = 0;
        get_area = shmem_get_unmapped_area;
    }

    addr = get_area(file, addr, len, pgoff, flags);
    if (IS_ERR_VALUE(addr))
        return addr;

    if (addr > TASK_SIZE - len)
        return -ENOMEM;
    if (offset_in_page(addr))
        return -EINVAL;

    error = security_mmap_addr(addr);
    return error ? error : addr;
}
```

# pgd_mapping

## create_pgd_mapping

```c
__create_pgd_mapping(pgdir, phys, virt, size, prot, pgd_pgtable_alloc, flags) {
    do {
        next = pgd_addr_end(addr, end);
        alloc_init_pud(pgdp, addr, next, phys, prot, pgtable_alloc, flags) {
            if (pgd_none(pgd)) {
                pud_phys = pgtable_alloc(PUD_SHIFT);
                __p4d_populate(p4dp, pud_phys, p4dval) {
                    set_p4d(p4dp, __p4d(__phys_to_p4d_val(pudp) | prot));
                }
                p4d = READ_ONCE(*p4dp);
            }
            BUG_ON(p4d_bad(p4d));

            pudp = pud_set_fixmap_offset(p4dp, addr);
            do {
                next = pud_addr_end(addr, end);
                if (pud_sect_supported() && (flags & NO_BLOCK_MAPPINGS) == 0) {
                    pud_set_huge(pudp, phys, prot);
                } else {
                    alloc_init_cont_pmd(pudp, addr, next, phys, prot, pgtable_alloc, flags) {
                        if (pud_none(pud)) {
                            pmd_phys = pgtable_alloc(PMD_SHIFT);
                            __pud_populate(pudp, pmd_phys, pudval);
                            pud = READ_ONCE(*pudp);
                        }
                        BUG_ON(pud_bad(pud));

                        pmdp = pmd_set_fixmap_offset(pudp, addr);
                        do {
                            next = pmd_cont_addr_end(addr, end);
                            init_pmd(pudp, addr, next, phys, __prot, pgtable_alloc, flags) {
                                do {
                                    next = pmd_addr_end(addr, end);
                                    if ((flags & NO_BLOCK_MAPPINGS) == 0) {
                                        pmd_set_huge(pmdp, phys, prot) {
                                            prot = mk_pmd_sect_prot(prot) {
                                                return __pgprot((pgprot_val(prot) & ~PMD_TABLE_BIT) | PMD_TYPE_SECT);
                                            }
                                            pmd_t new_pmd = pfn_pmd(__phys_to_pfn(phys), prot);

                                            set_pmd(pmdp, new_pmd);
                                            return 1;
                                        }
                                    } else {
                                        alloc_init_cont_pte(pmdp, addr, next, phys, prot, pgtable_alloc, flags) {
                                            if (pmd_none(pmd)) {
                                                pte_phys = pgtable_alloc(PAGE_SHIFT);
                                                __pmd_populate(pmdp, pte_phys, pmdval);
                                                pmd = READ_ONCE(*pmdp);
                                            }
                                            BUG_ON(pmd_bad(pmd));

                                            do {
                                                next = pte_cont_addr_end(addr, end);
                                                init_pte(pmdp, addr, next, phys, __prot) {
                                                    ptep = pte_set_fixmap_offset(pmdp, addr);
                                                    do {
                                                        set_pte(ptep, pfn_pte(__phys_to_pfn(phys), prot)) {
                                                            WRITE_ONCE(*ptep, pte);
                                                            if (pte_valid_not_user(pte)) {
                                                                dsb(ishst);
                                                                isb();
                                                            }
                                                        }
                                                        phys += PAGE_SIZE;
                                                    } while (ptep++, addr += PAGE_SIZE, addr != end);
                                                    pte_clear_fixmap();
                                                }
                                                phys += next - addr;
                                            } while (addr = next, addr != end);
                                        }
                                    }
                                    phys += next - addr;
                                } while (pmdp++, addr = next, addr != end);
                            }
                            phys += next - addr;
                        } while (addr = next, addr != end);
                    }
                }
                phys += next - addr;
            } while (pudp++, addr = next, addr != end);
            pud_clear_fixmap();
        }
        phys += next - addr;
    } while (pgdp++, addr = next, addr != end);
}
```

## remove_pgd_mapping

```c
__remove_pgd_mapping()
    /* free phys mem which virt addr is [start, end] */
    unmap_hotplug_range(asid, pgdir, start, end, 0, tlb) {
        do {
            if (pgd_none(pgd))
                continue;
        /* 1. pgd */
            unmap_hotplug_p4d_range(asid, pgdp, addr, next, free_mapped, tlb) {
                do {
                    if (p4d_none(p4d))
                        continue;
        /* 2. pud */
                    unmap_hotplug_pud_range(asid, p4dp, addr, next, free_mapped, tlb) {
                        do {
                            if (pud_none(pud))
                                continue;
                            if (pud_sect(pud)) {
                                pud_clear(pudp);
                                // tlb_batach_tlb_gather(tlb, addr | ARM64_TLB_FLUSH_PUD);

                                flush_tlb_kernel_range(addr, addr + PAGE_SIZE);
                                    --->
                                if (free_mapped) {
                                    free_hotplug_page_range(pud_page(pud), PUD_SIZE, tlb)
                                        free_pages()
                                }
                                continue;
                            }
        /* 2. pmd */
                            unmap_hotplug_pmd_range(asid, pudp, addr, next, free_mapped, tlb) {
                                do {
                                    if (pmd_none(pmd))
                                        continue;

                                    flush_tlb_kernel_range(addr, addr + PAGE_SIZE);
                                        --->
                                    if (pmd_sect(pmd)) {
                                        pmd_clear(pmdp);
                                        if (free_mapped)
                                            free_hotplug_page_range(pmd_page(pmd), PMD_SIZE, tlb)
                                               free_pages()
                                        continue;
                                    }
        /* 3. pte */
                                    unmap_hotplug_pte_range(asid, pmdp, addr, next, free_mapped, tlb);
                                        do {
                                            if (pte_none(pte))
                                                continue;
                                            pte_clear(NULL, addr, ptep);

                                            flush_tlb_kernel_range(addr, addr + PAGE_SIZE);
                                                --->
                                            if (free_mapped) {
                                                free_hotplug_page_range(pte_page(pte), PAGE_SIZE, tlb)
                                                   free_pages()
                                            }
                                        } while (addr += PAGE_SIZE, addr < end);
                                    }
                                } while (addr = next, addr < end);
                            }
                        } while (addr = next, addr < end);
                    }
                } while (addr = next, addr < end);
            }
        } while (addr = next, addr < end)

    /* free phsy mem of pgtable which is used to map virt addr [start, end] */
    free_empty_tables()
        do {
            pgd = READ_ONCE(*pgdp);
            if (pgd_none(pgd))
                continue;
            free_empty_p4d_table(asid, pgdp, addr, next, floor, ceiling, tlb) {
                do {
                    if (p4d_none(p4d))
                        continue;
                    free_empty_pud_table(asid, p4dp, addr, next, floor, ceiling, tlb) {
                        do {
                            pud = READ_ONCE(*pudp);
                            free_empty_pmd_table(asid, pudp, addr, next, floor, ceiling, tlb) {
                                do {
                                    pmd = READ_ONCE(*pmdp);
                                    if (pmd_none(pmd))
                                        continue;
                                    free_empty_pte_table(asid, pmdp, addr, next, floor, ceiling, tlb) {
                                        do {
                                            WARN_ON(!pte_none(pte));
                                        } while ();

                                        pmd_clear(pmdp);
                                        __flush_tlb_kernel_pgtable(start);
                                        free_hotplug_pgtable_page(virt_to_page(ptep), tlb);
                                            free_pages()
                                    }
                                } while (addr = next, addr < end);

                                pud_clear(pudp);
                                __flush_tlb_kernel_pgtable(start);
                                free_hotplug_pgtable_page(virt_to_page(pmdp), tlb);
                                    free_pages()
                            }
                        } while (addr = next, addr < end);

                        p4d_clear(p4dp);
                        __flush_tlb_kernel_pgtable(start);
                        free_hotplug_pgtable_page(virt_to_page(pudp), tlb);
                            free_pages()
                    }
                } while (addr = next, addr < end);
            }
        } while (addr = next, addr < end);
```

# mmap

* bin 的技术小屋 [原理](https://mp.weixin.qq.com/s/AUsgFOaePwVsPozC3F6Wjw) ⊙ [源码](https://mp.weixin.qq.com/s/BY3OZ6rkYYyQil_webt7Xg)

![](../images/kernel/mem-mmap.svg)

![](../images/kernel/mem-mmap-vma-file-page.svg)

```c
struct mm_struct {
    pgd_t                   *pgd;
    struct maple_tree       mm_mt;
    struct rw_semaphore     mmap_lock;
}

struct vm_area_struct {
    /* For areas with an address space and backing store,
    * linkage into the address_space->i_mmap interval tree. */
    struct {
        struct rb_node rb;
        unsigned long rb_subtree_last;
    } shared;

    /* A file's MAP_PRIVATE vma can be in both i_mmap tree and anon_vma list
    * An anonymous MAP_PRIVATE, stack or brk vma can only be in an anon_vma list.
    * A MAP_SHARED vma can only be in the i_mmap tree. */
    struct list_head anon_vma_chain;
    struct anon_vma *anon_vma;

    const struct vm_operations_struct *vm_ops;

    unsigned long vm_pgoff; /* Offset (within vm_file) in PAGE_SIZE units */
    struct file * vm_file;  /* File we map to (can be NULL). */
    void * vm_private_data; /* was vm_pte (shared mem) */
};

struct anon_vma_chain {
    struct vm_area_struct   *vma;
    struct anon_vma         *anon_vma;
    struct list_head        same_vma; /* locked by mmap_sem & page_table_lock */
    struct rb_node          rb; /* locked by anon_vma->rwsem */
    unsigned long           rb_subtree_last;
};

struct anon_vma {
    struct anon_vma       *root;  /* Root of this anon_vma tree */
    struct rw_semaphore   rwsem;  /* W: modification, R: walking the list */
    atomic_t              refcount;
    unsigned              degree;
    struct anon_vma       *parent;  /* Parent of this anon_vma */
    struct rb_root_cached rb_root;
};

/* page cache in memory */
struct address_space {
    struct inode          *host;
    struct xarray         i_pages; /* cached physical pages */
    struct rw_semaphore   invalidate_lock;
    gfp_t                 gfp_mask;
    atomic_t              i_mmap_writable; /* Number of VM_SHARED mappings. */
    struct rb_root_cached i_mmap; /* Tree of private and shared mappings. vm_area_struct */
    struct rw_semaphore   i_mmap_rwsem;
    unsigned long         nrpages;
    pgoff_t               writeback_index; /* Writeback starts here */
    const struct address_space_operations *a_ops;
    unsigned long         flags; /* enum mapping_flags */
    errseq_t              wb_err;
    spinlock_t            private_lock;
    struct list_head      private_list;
    void*                 private_data;
};
```

```c
mmap() {
    sys_mmap_pgoff() {
        vm_mmap_pgoff() {
            do_mmap(file, addr, len, prot, flag, 0, pgoff, &populate, &uf) {
                get_unmapped_area() {
                    get_area = current->mm->get_unmapped_area;
                    if (file->f_op ->get_unmapped_area) {
                        get_area = file->f_op->get_unmapped_area {
                            __thp_get_unmapped_area() {
                                current->mm->get_unmapped_area();
                            }
                        }
                    } else if (flags & MAP_SHARED) {
                        get_area = shmem_get_unmapped_area;
                    }
                    addr = get_area(file, addr, len, pgoff, flags) { /* arch_get_unmapped_area */
                        vm_unmapped_area() {
                            if (info->flags & VM_UNMAPPED_AREA_TOPDOWN)
                                addr = unmapped_area_topdown(info);
                            else {
                                addr = unmapped_area(info) {

                                }
                            }
                        }
                    }
                }

                mmap_region() {
                    if (!may_expand_vm()) {
                        return -ENOMEM;
                    }

                    /* Unmap any existing mapping in the area */
                    do_vmi_munmap(&vmi, mm);

                    vma_expand();

                    struct vm_area_struct *vma = kmem_cache_zalloc();

                    if (file) {
                        vma->vm_file = get_file(file);
                        /* 2.1. link the file to vma */
                        rc = call_mmap(file, vma) {
                            file->f_op->mmap(file, vma);
                            ext4_file_mmap() {
                                vma->vm_ops = &ext4_file_vm_ops;
                            }
                        }

                        /* 2.2. link the vma to the file */
                        vma_interval_tree_insert(vma, &mapping->i_mmap);

                        if (rc) {
                            unmap_region()
                                --->
                        }
                    } else if (vm_flags & VM_SHARED) {
                        shmem_zero_setup(vma); /* tmpfs under /dev/zero */
                    } else {
                        vma_set_anonymous(vma); /* vma->vm_ops = NULL; */
                    }
                }
            }

            if (populate) {
                mm_populate(ret, populate);
            }
        }
    }
}


setup_new_exec();
    arch_pick_mmap_layout();
        mm->mmap_base = TASK_UNMAPPED_BASE + random_factor;
        mm->get_unmapped_area = arch_get_unmapped_area;

mm->get_unmapped_area();
    arch_get_unmapped_area();
        find_start_end() {
            *begin = get_mmap_base(1);
                return mm->mmap_base;
            *end = task_size_64bit(addr > DEFAULT_MAP_WINDOW);
        }
        vm_unmapped_area();
            unmapped_area();
```

```c
SYSCALL_DEFINE6(mmap, unsigned long, addr, unsigned long, len,
        unsigned long, prot, unsigned long, flags,
        unsigned long, fd, unsigned long, off)
{
    if (offset_in_page(off) != 0)
        return -EINVAL;

    return ksys_mmap_pgoff(addr, len, prot, flags, fd, off >> PAGE_SHIFT) {
        struct file *file = NULL;
        unsigned long retval;

        if (!(flags & MAP_ANONYMOUS)) {
            audit_mmap_fd(fd, flags);
            file = fget(fd);
            if (!file)
                return -EBADF;
            if (is_file_hugepages(file)) {
                len = ALIGN(len, huge_page_size(hstate_file(file)));
            } else if (unlikely(flags & MAP_HUGETLB)) {
                retval = -EINVAL;
                goto out_fput;
            }
        } else if (flags & MAP_HUGETLB) {
            struct hstate *hs;

            hs = hstate_sizelog((flags >> MAP_HUGE_SHIFT) & MAP_HUGE_MASK);
            if (!hs)
                return -EINVAL;

            len = ALIGN(len, huge_page_size(hs));
            file = hugetlb_file_setup(HUGETLB_ANON_FILE, len,
                    VM_NORESERVE,
                    HUGETLB_ANONHUGE_INODE,
                    (flags >> MAP_HUGE_SHIFT) & MAP_HUGE_MASK);
            if (IS_ERR(file))
                return PTR_ERR(file);
        }

        retval = vm_mmap_pgoff(file, addr, len, prot, flags, pgoff) {
            unsigned long ret;
            struct mm_struct *mm = current->mm;
            unsigned long populate;
            LIST_HEAD(uf);

            ret = security_mmap_file(file, prot, flag);
            if (!ret) {
                if (mmap_write_lock_killable(mm))
                    return -EINTR;

                ret = do_mmap(file, addr, len, prot, flag, 0, pgoff, &populate, &uf);
                    --->

                mmap_write_unlock(mm);
                userfaultfd_unmap_complete(mm, &uf);
                if (populate)
                    mm_populate(ret, populate);
            }
            return ret;
        }

    out_fput:
        if (file)
            fput(file);
        return retval;
    }
}
```

## do_mmap

```c
 unsigned long do_mmap(struct file *file, unsigned long addr,
    unsigned long len, unsigned long prot,
    unsigned long flags, vm_flags_t vm_flags,
    unsigned long pgoff, unsigned long *populate,
    struct list_head *uf) {

    struct mm_struct *mm = current->mm;
    int pkey = 0;

    *populate = 0;

    if (!len)
        return -EINVAL;

    if ((prot & PROT_READ) && (current->personality & READ_IMPLIES_EXEC))
        if (!(file && path_noexec(&file->f_path)))
            prot |= PROT_EXEC;

    /* force arch specific MAP_FIXED handling in get_unmapped_area */
    if (flags & MAP_FIXED_NOREPLACE)
        flags |= MAP_FIXED;

    if (!(flags & MAP_FIXED))
        addr = round_hint_to_min(addr);

    /* Careful about overflows.. */
    len = PAGE_ALIGN(len);
    if (!len)
        return -ENOMEM;

    /* offset overflow? */
    if ((pgoff + (len >> PAGE_SHIFT)) < pgoff)
        return -EOVERFLOW;

    /* Too many mappings? */
    if (mm->map_count > sysctl_max_map_count)
        return -ENOMEM;

    addr = get_unmapped_area(file, addr, len, pgoff, flags);
    if (IS_ERR_VALUE(addr))
        return addr;

    if (flags & MAP_FIXED_NOREPLACE) {
        if (find_vma_intersection(mm, addr, addr + len))
            return -EEXIST;
    }

    if (prot == PROT_EXEC) {
        pkey = execute_only_pkey(mm);
        if (pkey < 0)
            pkey = 0;
    }

    /* Do simple checking here so the lower-level routines won't have
    * to. we assume access permissions have been handled by the open
    * of the memory object, so we don't do any here. */
    vm_flags |= calc_vm_prot_bits(prot, pkey) | calc_vm_flag_bits(flags) |
            mm->def_flags | VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC;

    if (flags & MAP_LOCKED)
        if (!can_do_mlock())
            return -EPERM;

    if (!mlock_future_ok(mm, vm_flags, len))
        return -EAGAIN;

    if (file) {
        struct inode *inode = file_inode(file);
        unsigned long flags_mask;

        if (!file_mmap_ok(file, inode, pgoff, len))
            return -EOVERFLOW;

        flags_mask = LEGACY_MAP_MASK | file->f_op->mmap_supported_flags;

        switch (flags & MAP_TYPE) {
        case MAP_SHARED:
            flags &= LEGACY_MAP_MASK;
            fallthrough;
        case MAP_SHARED_VALIDATE:
            if (flags & ~flags_mask)
                return -EOPNOTSUPP;
            if (prot & PROT_WRITE) {
                if (!(file->f_mode & FMODE_WRITE))
                    return -EACCES;
                if (IS_SWAPFILE(file->f_mapping->host))
                    return -ETXTBSY;
            }

            if (IS_APPEND(inode) && (file->f_mode & FMODE_WRITE))
                return -EACCES;

            vm_flags |= VM_SHARED | VM_MAYSHARE;
            if (!(file->f_mode & FMODE_WRITE))
                vm_flags &= ~(VM_MAYWRITE | VM_SHARED);
            fallthrough;
        case MAP_PRIVATE:
            if (!(file->f_mode & FMODE_READ))
                return -EACCES;
            if (path_noexec(&file->f_path)) {
                if (vm_flags & VM_EXEC)
                    return -EPERM;
                vm_flags &= ~VM_MAYEXEC;
            }

            if (!file->f_op->mmap)
                return -ENODEV;
            if (vm_flags & (VM_GROWSDOWN|VM_GROWSUP))
                return -EINVAL;
            break;

        default:
            return -EINVAL;
        }
    } else {
        switch (flags & MAP_TYPE) {
        case MAP_SHARED:
            if (vm_flags & (VM_GROWSDOWN|VM_GROWSUP))
                return -EINVAL;
            pgoff = 0;
            vm_flags |= VM_SHARED | VM_MAYSHARE;
            break;
        case MAP_PRIVATE:
            pgoff = addr >> PAGE_SHIFT;
            break;
        default:
            return -EINVAL;
        }
    }

    if (flags & MAP_NORESERVE) {
        /* We honor MAP_NORESERVE if allowed to overcommit */
        if (sysctl_overcommit_memory != OVERCOMMIT_NEVER)
            vm_flags |= VM_NORESERVE;

        /* hugetlb applies strict overcommit unless MAP_NORESERVE */
        if (file && is_file_hugepages(file))
            vm_flags |= VM_NORESERVE;
    }

    addr = mmap_region(file, addr, len, vm_flags, pgoff, uf);
    if (!IS_ERR_VALUE(addr) &&
        ((vm_flags & VM_LOCKED) ||
        (flags & (MAP_POPULATE | MAP_NONBLOCK)) == MAP_POPULATE))
        *populate = len;
    return addr;
}
```

## get_unmapped_area

```c
unsigned long
get_unmapped_area(struct file *file, unsigned long addr, unsigned long len,
        unsigned long pgoff, unsigned long flags)
{
    unsigned long (*get_area)(struct file *, unsigned long,
                unsigned long, unsigned long, unsigned long);

    unsigned long error = arch_mmap_check(addr, len, flags);
    if (error)
        return error;

    /* Careful about overflows.. */
    if (len > TASK_SIZE)
        return -ENOMEM;

    get_area = current->mm->get_unmapped_area;
    if (file) {
        if (file->f_op->get_unmapped_area) {
            get_area = file->f_op->get_unmapped_area;
        }
    } else if (flags & MAP_SHARED) {
        pgoff = 0;
        get_area = shmem_get_unmapped_area;
    } else if (IS_ENABLED(CONFIG_TRANSPARENT_HUGEPAGE)) {
        get_area = thp_get_unmapped_area;
    }

    addr = get_area(file, addr, len, pgoff, flags) {
        arch_get_unmapped_area() {
            return generic_get_unmapped_area(filp, addr, len, pgoff, flags); {
                struct mm_struct *mm = current->mm;
                struct vm_area_struct *vma, *prev;
                struct vm_unmapped_area_info info;
                const unsigned long mmap_end = arch_get_mmap_end(addr, len, flags) {

                }

                if (len > mmap_end - mmap_min_addr)
                    return -ENOMEM;

                if (flags & MAP_FIXED)
                    return addr;

                if (addr) {
                    addr = PAGE_ALIGN(addr);
                    vma = find_vma_prev(mm, addr, &prev);
                    if (mmap_end - len >= addr && addr >= mmap_min_addr &&
                        (!vma || addr + len <= vm_start_gap(vma)) &&
                        (!prev || addr >= vm_end_gap(prev)))
                        return addr;
                }

                info.flags = 0;
                info.length = len;
                info.low_limit = mm->mmap_base;
                info.high_limit = mmap_end;
                info.align_mask = 0;
                info.align_offset = 0;
                return vm_unmapped_area(&info) {
                    unsigned long addr;

                    if (info->flags & VM_UNMAPPED_AREA_TOPDOWN) {
                        addr = unmapped_area_topdown(info);
                    } else {
                        /* arm64 adopts classical layout, addr grows from low to high */
                        addr = unmapped_area(info) {
                            unsigned long length, gap;
                            unsigned long low_limit, high_limit;
                            struct vm_area_struct *tmp;

                            MA_STATE(mas, &current->mm->mm_mt, 0, 0);

                            /* Adjust search length to account for worst case alignment overhead */
                            length = info->length + info->align_mask;
                            if (length < info->length)
                                return -ENOMEM;

                            low_limit = info->low_limit;
                            if (low_limit < mmap_min_addr)
                                low_limit = mmap_min_addr;
                            high_limit = info->high_limit;
                        retry:
                            if (mas_empty_area(&mas, low_limit, high_limit - 1, length))
                                return -ENOMEM;

                            gap = mas.index;
                            gap += (info->align_offset - gap) & info->align_mask;
                            tmp = mas_next(&mas, ULONG_MAX);
                            if (tmp && (tmp->vm_flags & VM_STARTGAP_FLAGS)) { /* Avoid prev check if possible */
                                if (vm_start_gap(tmp) < gap + length - 1) {
                                    low_limit = tmp->vm_end;
                                    mas_reset(&mas);
                                    goto retry;
                                }
                            } else {
                                tmp = mas_prev(&mas, 0);
                                if (tmp && vm_end_gap(tmp) > gap) {
                                    low_limit = vm_end_gap(tmp);
                                    mas_reset(&mas);
                                    goto retry;
                                }
                            }

                            return gap;
                        }
                    }

                    return addr;
                }
            }
        }
    }
    if (IS_ERR_VALUE(addr))
        return addr;

    if (addr > TASK_SIZE - len)
        return -ENOMEM;
    if (offset_in_page(addr))
        return -EINVAL;

    error = security_mmap_addr(addr);
    return error ? error : addr;
}
```

## mmap_region

![](../images/kernel/mem-mmap_region.png)

```c
unsigned long mmap_region(struct file *file, unsigned long addr,
        unsigned long len, vm_flags_t vm_flags, unsigned long pgoff,
        struct list_head *uf)
{
    struct mm_struct *mm = current->mm;
    struct vm_area_struct *vma = NULL;
    struct vm_area_struct *next, *prev, *merge;
    pgoff_t pglen = len >> PAGE_SHIFT;
    unsigned long charged = 0;
    unsigned long end = addr + len;
    unsigned long merge_start = addr, merge_end = end;
    bool writable_file_mapping = false;
    pgoff_t vm_pgoff;
    int error;
    VMA_ITERATOR(vmi, mm, addr);

    /* Check against address space limit. */
    if (!may_expand_vm(mm, vm_flags, len >> PAGE_SHIFT)) {
        unsigned long nr_pages;

        /* MAP_FIXED may remove pages of mappings that intersects with
         * requested mapping. Account for the pages it would unmap. */
        nr_pages = count_vma_pages_range(mm, addr, end);

        if (!may_expand_vm(mm, vm_flags, (len >> PAGE_SHIFT) - nr_pages))
            return -ENOMEM;
    }

    /* Unmap any existing mapping in the area */
    if (do_vmi_munmap(&vmi, mm, addr, len, uf, false))
        return -ENOMEM;

    /* Private writable mapping: check memory availability */
    if (accountable_mapping(file, vm_flags)) {
        charged = len >> PAGE_SHIFT;
        if (security_vm_enough_memory_mm(mm, charged))
            return -ENOMEM;
        vm_flags |= VM_ACCOUNT;
    }

    next = vma_next(&vmi);
    prev = vma_prev(&vmi);
    if (vm_flags & VM_SPECIAL) {
        if (prev)
            vma_iter_next_range(&vmi);
        goto cannot_expand;
    }

    /* Attempt to expand an old mapping */
    /* Check next */
    ret = can_vma_merge_before(next, vm_flags, NULL, file, pgoff+pglen, NULL_VM_UFFD_CTX, NULL);
    if (next && next->vm_start == end && !vma_policy(next) && ret) {
        merge_end = next->vm_end;
        vma = next;
        vm_pgoff = next->vm_pgoff - pglen;
    }

    /* Check prev */
    ret = vma
        ? can_vma_merge_after(prev, vm_flags, vma->anon_vma, file, pgoff, vma->vm_userfaultfd_ctx, NULL)
        : can_vma_merge_after(prev, vm_flags, NULL, file, pgoff,
                    NULL_VM_UFFD_CTX, NULL);
    if (prev && prev->vm_end == addr && !vma_policy(prev) && (ret)) {
        merge_start = prev->vm_start;
        vma = prev;
        vm_pgoff = prev->vm_pgoff;
    } else if (prev) {
        vma_iter_next_range(&vmi);
    }

    /* Actually expand, if possible */
    if (vma && !vma_expand(&vmi, vma, merge_start, merge_end, vm_pgoff, next)) {
        khugepaged_enter_vma(vma, vm_flags);
        goto expanded;
    }

    if (vma == prev)
        vma_iter_set(&vmi, addr);

cannot_expand:
    vma = vm_area_alloc(mm);
    if (!vma) {
        error = -ENOMEM;
        goto unacct_error;
    }

    vma_iter_config(&vmi, addr, end);
    vma->vm_start = addr;
    vma->vm_end = end;
    vm_flags_init(vma, vm_flags);
    vma->vm_page_prot = vm_get_page_prot(vm_flags);
    vma->vm_pgoff = pgoff;

    if (file) {
        vma->vm_file = get_file(file);
        error = call_mmap(file, vma) {
            return file->f_op->mmap(file, vma) { /* ext4_file_mmap */
                vma->vm_ops = &ext4_file_vm_ops;
            }
        }
        if (error)
            goto unmap_and_free_vma;

        if (vma_is_shared_maywrite(vma)) {
            error = mapping_map_writable(file->f_mapping);
            if (error)
                goto close_and_free_vma;

            writable_file_mapping = true;
        }

        error = -EINVAL;
        if (WARN_ON((addr != vma->vm_start)))
            goto close_and_free_vma;

        vma_iter_config(&vmi, addr, end);

        if (unlikely(vm_flags != vma->vm_flags && prev)) {
            merge = vma_merge_new_vma(&vmi, prev, vma, vma->vm_start, vma->vm_end, vma->vm_pgoff) {

                return vma_merge(vmi, vma->vm_mm, prev, start, end, vma->vm_flags,
                    vma->anon_vma, vma->vm_file, pgoff, vma_policy(vma),
                    vma->vm_userfaultfd_ctx, anon_vma_name(vma));
            }
            if (merge) {
                fput(vma->vm_file);
                vm_area_free(vma);
                vma = merge;
                /* Update vm_flags to pick up the change. */
                vm_flags = vma->vm_flags;
                goto unmap_writable;
            }
        }

        vm_flags = vma->vm_flags;
    } else if (vm_flags & VM_SHARED) {
        error = shmem_zero_setup(vma) { /* tmpfs under /dev/zero */
            struct file *file;
            loff_t size = vma->vm_end - vma->vm_start;

            file = shmem_kernel_file_setup("dev/zero", size, vma->vm_flags);
            if (IS_ERR(file))
                return PTR_ERR(file);

            if (vma->vm_file)
                fput(vma->vm_file);
            vma->vm_file = file;
            vma->vm_ops = &shmem_anon_vm_ops;

            return 0;
        }
        if (error)
            goto free_vma;
    } else {
        vma_set_anonymous(vma) {
            vma->vm_ops = NULL;
        }
    }

    if (map_deny_write_exec(vma, vma->vm_flags)) {
        error = -EACCES;
        goto close_and_free_vma;
    }

    /* Allow architectures to sanity-check the vm_flags */
    error = -EINVAL;
    if (!arch_validate_flags(vma->vm_flags))
        goto close_and_free_vma;

    error = -ENOMEM;
    if (vma_iter_prealloc(&vmi, vma))
        goto close_and_free_vma;

    /* Lock the VMA since it is modified after insertion into VMA tree */
    vma_start_write(vma);
    vma_iter_store(&vmi, vma);
    mm->map_count++;
    if (vma->vm_file) {
        i_mmap_lock_write(vma->vm_file->f_mapping);
        if (vma_is_shared_maywrite(vma))
            mapping_allow_writable(vma->vm_file->f_mapping);

        flush_dcache_mmap_lock(vma->vm_file->f_mapping);

        /* insert vma into address_space i_mmap tree */
        vma_interval_tree_insert(vma, &vma->vm_file->f_mapping->i_mmap);
        flush_dcache_mmap_unlock(vma->vm_file->f_mapping);
        i_mmap_unlock_write(vma->vm_file->f_mapping);
    }

    khugepaged_enter_vma(vma, vma->vm_flags);

    /* Once vma denies write, undo our temporary denial count */
unmap_writable:
    if (writable_file_mapping)
        mapping_unmap_writable(file->f_mapping);
    file = vma->vm_file;
    ksm_add_vma(vma);
expanded:
    perf_event_mmap(vma);

    vm_stat_account(mm, vm_flags, len >> PAGE_SHIFT);
    if (vm_flags & VM_LOCKED) {
        if ((vm_flags & VM_SPECIAL) || vma_is_dax(vma) ||
                    is_vm_hugetlb_page(vma) ||
                    vma == get_gate_vma(current->mm))
            vm_flags_clear(vma, VM_LOCKED_MASK);
        else
            mm->locked_vm += (len >> PAGE_SHIFT);
    }

    if (file)
        uprobe_mmap(vma);

    /* New (or expanded) vma always get soft dirty status.
    * Otherwise user-space soft-dirty page tracker won't
    * be able to distinguish situation when vma area unmapped,
    * then new mapped in-place (which must be aimed as
    * a completely new data area). */
    vm_flags_set(vma, VM_SOFTDIRTY);

    vma_set_page_prot(vma);

    validate_mm(mm);
    return addr;

close_and_free_vma:
    if (file && vma->vm_ops && vma->vm_ops->close)
        vma->vm_ops->close(vma);

    if (file || vma->vm_file) {
unmap_and_free_vma:
        fput(vma->vm_file);
        vma->vm_file = NULL;

        vma_iter_set(&vmi, vma->vm_end);
        /* Undo any partial mapping done by a device driver. */
        unmap_region(mm, &vmi.mas, vma, prev, next, vma->vm_start,
                vma->vm_end, vma->vm_end, true);
    }
    if (writable_file_mapping)
        mapping_unmap_writable(file->f_mapping);
free_vma:
    vm_area_free(vma);
unacct_error:
    if (charged)
        vm_unacct_memory(charged);
    validate_mm(mm);
    return error;
}
```

## mm_populate

```c
int __mm_populate(unsigned long start, unsigned long len, int ignore_errors)
{
    struct mm_struct *mm = current->mm;
    unsigned long end, nstart, nend;
    struct vm_area_struct *vma = NULL;
    int locked = 0;
    long ret = 0;

    end = start + len;

    for (nstart = start; nstart < end; nstart = nend) {
        if (!locked) {
            locked = 1;
            mmap_read_lock(mm);
            vma = find_vma_intersection(mm, nstart, end);
        } else if (nstart >= vma->vm_end)
            vma = find_vma_intersection(mm, vma->vm_end, end);

        if (!vma)
            break;

        nend = min(end, vma->vm_end);
        if (vma->vm_flags & (VM_IO | VM_PFNMAP))
            continue;
        if (nstart < vma->vm_start)
            nstart = vma->vm_start;

        ret = populate_vma_page_range(vma, nstart, nend, &locked) {
            if (vma->vm_flags & VM_LOCKONFAULT)
                return nr_pages;

            gup_flags = FOLL_TOUCH;

            if ((vma->vm_flags & (VM_WRITE | VM_SHARED)) == VM_WRITE)
                gup_flags |= FOLL_WRITE;

            if (vma_is_accessible(vma))
                gup_flags |= FOLL_FORCE;

            if (locked)
                gup_flags |= FOLL_UNLOCKABLE;

            ret = __get_user_pages(mm, start, nr_pages, gup_flags, NULL, locked ? locked : &local_locked) {
                do {
                    struct page *page;
                    unsigned int foll_flags = gup_flags;
                    unsigned int page_increm;

                    /* first iteration or cross vma bound */
                    if (!vma || start >= vma->vm_end) {
                        vma = gup_vma_lookup(mm, start);
                        if (!vma && in_gate_area(mm, start)) {
                            ret = get_gate_page(mm, start & PAGE_MASK,
                                    gup_flags, &vma, pages ? &page : NULL);
                            if (ret)
                                goto out;
                            ctx.page_mask = 0;
                            goto next_page;
                        }

                        if (!vma) {
                            ret = -EFAULT;
                            goto out;
                        }
                        ret = check_vma_flags(vma, gup_flags);
                        if (ret)
                            goto out;
                    }
            retry:
                    if (fatal_signal_pending(current)) {
                        ret = -EINTR;
                        goto out;
                    }
                    cond_resched();

                    page = follow_page_mask(vma, start, foll_flags, &ctx);
                    if (!page || PTR_ERR(page) == -EMLINK) {
                        ret = faultin_page(vma, start, &foll_flags, PTR_ERR(page) == -EMLINK, locked) {
                            handle_mm_fault(vma, address, fault_flags, NULL);
                                --->
                        }
                    } else if (PTR_ERR(page) == -EEXIST) {
                        if (pages) {
                            ret = PTR_ERR(page);
                            goto out;
                        }
                    } else if (IS_ERR(page)) {
                        ret = PTR_ERR(page);
                        goto out;
                    }

            next_page:
                    page_increm = 1 + (~(start >> PAGE_SHIFT) & ctx.page_mask);
                    if (page_increm > nr_pages)
                        page_increm = nr_pages;

                    if (pages) {
                        struct page *subpage;
                        unsigned int j;

                        if (page_increm > 1) {
                            struct folio *folio;

                            folio = try_grab_folio(page, page_increm - 1, foll_flags);
                            if (WARN_ON_ONCE(!folio)) {
                                gup_put_folio(page_folio(page), 1, foll_flags);
                                ret = -EFAULT;
                                goto out;
                            }
                        }

                        for (j = 0; j < page_increm; j++) {
                            subpage = nth_page(page, j);
                            pages[i + j] = subpage;
                            flush_anon_page(vma, subpage, start + j * PAGE_SIZE);
                            flush_dcache_page(subpage);
                        }
                    }

                    i += page_increm;
                    start += page_increm * PAGE_SIZE;
                    nr_pages -= page_increm;
                } while (nr_pages);
            }

            lru_add_drain();
            return ret;
        }
        if (ret < 0) {
            if (ignore_errors) {
                ret = 0;
                continue;    /* continue at next VMA */
            }
            break;
        }
        nend = nstart + ret * PAGE_SIZE;
        ret = 0;
    }
    if (locked)
        mmap_read_unlock(mm);
    return ret;    /* 0 or negative error code */
}
```

# page_fault

![](../images/kernel/intr-arm64.svg)

---

![](../images/kernel/mem-page-fault.png)

![](../images/kernel/mem-fault-0.png)

---

![](../images/kernel/mem-fault-1.png)

---

![](../images/kernel/mem-fault-do_translation_fault.png)

---

![](../images/kernel/mem-fault-do_page_fault.png)

---

![](../images/kernel/mem-fault-handle_pte_fault.png)

---

* [bin 的技术小屋](https://mp.weixin.qq.com/s/zyLSQehjr0zQ5WemjMqluw)


```c
struct file {
    struct file_operations* f_op;
    struct address_space*   f_mapping;
};

/* page cache in memory */
struct address_space {
    struct inode          *host;
    struct xarray         i_pages; /* cached physical pages */
    struct rw_semaphore   invalidate_lock;
    gfp_t                 gfp_mask;
    atomic_t              i_mmap_writable; /* Number of VM_SHARED mappings. */
    struct rb_root_cached i_mmap; /* Tree of private and shared mappings. vm_area_struct */
    struct rw_semaphore   i_mmap_rwsem;
    unsigned long         nrpages;
    pgoff_t               writeback_index; /* Writeback starts here */
    const struct address_space_operations *a_ops;
    unsigned long         flags; /* enum mapping_flags */
    errseq_t              wb_err;
    spinlock_t            private_lock;
    struct list_head      private_list;
    void*                 private_data;
};
```

ARM ESR Layout:
```c
+---------+---------+--------------------------------------------------+
| Bits    | Field   | Description                                      |
+---------+---------+--------------------------------------------------+
| [63:32] | RES0    | Reserved, must be 0                              |
| [31:26] | EC      | Exception Class (6 bits)                         |
|         |         | - 0x00: Unknown reason                           |
|         |         | - 0x01: Trapped WFI/WFE                          |
|         |         | - 0x0E: Illegal Execution State                  |
|         |         | - 0x15: SVC instruction                          |
|         |         | - 0x22: Instruction Abort (lower EL)             |
|         |         | - 0x24: Data Abort (lower EL)                    |
|         |         | - 0x3C: IRQ (from same EL)                       |
|         |         | - 0x3F: Other (see ARM ARM for full list)        |
+---------+---------+--------------------------------------------------+
| [25]    | IL      | Instruction Length                               |
|         |         | - 0: 16-bit instruction                          |
|         |         | - 1: 32-bit instruction                          |
+---------+---------+--------------------------------------------------+
| [24:0]  | ISS     | Instruction-Specific Syndrome (25 bits)          |
|         |         | (Format depends on EC value)                     |
|         |         | e.g., for Data/Instruction Aborts:               |
|         |         | - [24]: DFSC (Data Fault Status Code) present    |
|         |         | - [5:0]: DFSC/IFSC (Fault Status Code)           |
+---------+---------+--------------------------------------------------+
```

```c
/* arm64
 * arch/arm64/mm/fault.c */
static const struct fault_info fault_info[] = {
    { do_translation_fault, SIGSEGV, SEGV_MAPERR,   "level 0 translation fault" },
    { do_translation_fault, SIGSEGV, SEGV_MAPERR,   "level 1 translation fault" },
    { do_translation_fault, SIGSEGV, SEGV_MAPERR,   "level 2 translation fault" },
    { do_translation_fault, SIGSEGV, SEGV_MAPERR,   "level 3 translation fault" },
};

el1h_64_sync_handler() {
    switch (esr) {
        el1_abort(regs, esr) {
            do_mem_abort() {
                const struct fault_info *inf = esr_to_fault_info(esr) {
                    return fault_info + (esr & ESR_ELx_FSC);
                }
                inf->fn(far, esr, regs)->do_translation_fault() {
                    unsigned long addr = untagged_addr(far);

                    if (is_ttbr0_addr(addr)) { /* return addr < TASK_SIZE; */
                        return do_page_fault() {
                            vma = lock_vma_under_rcu(mm, addr);

                            __do_page_fault() {
                                fault = handle_mm_fault()
                                    --->
                                if (fault & VM_FAULT_OOM) {
                                    pagefault_out_of_memory() {

                                    }
                                    return 0;
                                }

                                if (fault & VM_FAULT_SIGBUS) {
                                    arm64_force_sig_fault(SIGBUS, BUS_ADRERR, far, inf->name);
                                } else if (fault & (VM_FAULT_HWPOISON_LARGE | VM_FAULT_HWPOISON)) {
                                    unsigned int lsb;

                                    lsb = PAGE_SHIFT;
                                    if (fault & VM_FAULT_HWPOISON_LARGE)
                                        lsb = hstate_index_to_shift(VM_FAULT_GET_HINDEX(fault));

                                    arm64_force_sig_mceerr(BUS_MCEERR_AR, far, lsb, inf->name);
                                } else {
                                    arm64_force_sig_fault(SIGSEGV,
                                        fault == VM_FAULT_BADACCESS ? SEGV_ACCERR : SEGV_MAPERR,
                                        far, inf->name
                                    );
                                }
                            }
                        }
                    }

                    do_bad_area(addr, fsr, regs) {
                        unsigned long addr = untagged_addr(far);
                        if (user_mode(regs)) {
                            const struct fault_info *inf = esr_to_fault_info(esr);
                            set_thread_esr(addr, esr);
                            arm64_force_sig_fault(inf->sig, inf->code, far, inf->name);
                        } else {
                            __do_kernel_fault(addr, esr, regs);
                        }
                    }
                }
            }
        }
    }
}


/* mm/memory.c */
handle_mm_fault(vma, address, flags, regs);
    hugetlb_fault();

    __handle_mm_fault() {
        pgd = pgd_offset(mm, address)
        p4d = p4d_alloc(pgd)
        if (!p4d)
            return VM_FAULT_OOM;
        pud = pud_alloc(p4d)
        if (!pud)
            return VM_FAULT_OOM;
    retry_pud:
        pmd = pmd_alloc(pud)
        if (!pmd)
            return VM_FAULT_OOM;

        handle_pte_fault() {
            if (!vmf->pte) {
                return do_pte_missing(vmf) {
                    if (vma_is_anonymous(vmf->vma)) {
                        /* 1. anonymous fault */
                        return do_anonymous_page(vmf);
                            --->
                    } else {
                        /* 2. file fault */
                        return do_fault(vmf) {
                            /* 2.1 read fault */
                            if (!(vmf->flags & FAULT_FLAG_WRITE)) {
                                do_read_fault();
                            } else if (!(vma->vm_flags & VM_SHARED)) {
                                /* 2.2 cow fault */
                                do_cow_fault();
                            } else {
                                /* 2.3 shared fault */
                                do_shared_fault();
                            }
                        }
                    }
                }
            }

            /* 3. swap fault */
            if (!pte_present(vmf->orig_pte)) {
                return do_swap_page(vmf)
                    --->
            }

            /* migrate page from remote node to cur node */
            if (pte_protnone(vmf->orig_pte) && vma_is_accessible(vmf->vma)) {
                return do_numa_page(vmf);
                    --->
            }

            if (vmf->flags & (FAULT_FLAG_WRITE | FAULT_FLAG_UNSHARE)) {
                if (!pte_write(entry)) {
                    return do_wp_page(vmf);
                        --->
                } else if (likely(vmf->flags & FAULT_FLAG_WRITE)) {
                    entry = pte_mkdirty(entry);
                }
            }

            update_mmu_tlb(vmf->vma, vmf->address, vmf->pte);

            update_mmu_cache(vmf->vma, vmf->address, vmf->pte);
        }
    }
```

```c
arch\arm64\include\asm\tlbflush.h

static inline void flush_tlb_all(void);

static inline void flush_tlb_mm(struct mm_struct *mm);

static inline void flush_tlb_range(struct vm_area_struct *vma, unsigned long start, unsigned long end);

static inline void flush_tlb_page(struct vm_area_struct *vma, unsigned long uaddr);

static inline void flush_tlb_kernel_range(unsigned long start, unsigned long end);

void tlb_migrate_finish(struct mm_struct *mm);
```

## do_pte_missing

```c
static vm_fault_t do_pte_missing(struct vm_fault *vmf)
{
    if (vma_is_anonymous(vmf->vma)) /* return !vma->vm_ops; */
        return do_anonymous_page(vmf);
    else
        return do_fault(vmf);
}
```

### do_anonymous_page

![](../images/kernel/mem-fault-do_anonymous_fault.png)

```c
do_anonymous_page(vmf) {
    pte = pte_alloc(pmd)
    if (pte)
        return VM_FAULT_OOM;
    anon_vma_prepare(vma)
        --->

    folio = vma_alloc_zeroed_movable_folio(vma, vmf->address);

    mk_pte();

    folio_add_new_anon_rmap(folio, vma, vmf->address);
        __folio_set_anon(folio, &folio->page, vma, address, 1);
            --->
    folio_add_lru_vma(folio, vma);

    set_pte_at(vma->vm_mm, vmf->address, vmf->pte, entry);
        --->
    update_mmu_cache()
}
```

### do_fault

![](../images/kernel/mem-fault-do_fault.png)

```c
static vm_fault_t do_fault(struct vm_fault *vmf) {
    if (!vma->vm_ops->fault) {
        vmf->pte = pte_offset_map_lock(vmf->vma->vm_mm, vmf->pmd, vmf->address, &vmf->ptl);
        if (unlikely(!vmf->pte))
            ret = VM_FAULT_SIGBUS;
        else {
            if (unlikely(pte_none(ptep_get(vmf->pte))))
                ret = VM_FAULT_SIGBUS;
            else
                ret = VM_FAULT_NOPAGE;

            pte_unmap_unlock(vmf->pte, vmf->ptl);
        }
    } else if (!(vmf->flags & FAULT_FLAG_WRITE))
        ret = do_read_fault(vmf);
    else if (!(vma->vm_flags & VM_SHARED))
        ret = do_cow_fault(vmf);
    else
        ret = do_shared_fault(vmf);
}
```

#### do_read_fault

```c
do_read_fault() { /* if (!(vmf->flags & FAULT_FLAG_WRITE)) */
/* 1. allocate phys pages */
    __do_fault() {
        vma->vm_ops->fault() {
            shm_vm_ops.fault() {
                shmem_fault();
                    --->
            }
            ext4_file_vm_ops.fault() {
                filemap_fault() {
                    folio = filemap_get_folio(mapping, index);
                    if (folio) {
                        do_async_mmap_readahead();
                    } else if (!page) {
                        do_sync_mmap_readahead();
                        folio = __filemap_get_folio();
                    }

                    vmf->page = folio_file_page(folio, index);
                }
            }
        }
    }

/* 2. map phys pages with vma */
    finish_fault() {
        set_pte_range(vmf, folio, page, 1, vmf->address) {
            entry = mk_pte(page, vma->vm_page_prot);
            if (write && !(vma->vm_flags & VM_SHARED)) {
                add_mm_counter(vma->vm_mm, MM_ANONPAGES, nr);
                folio_add_new_anon_rmap(folio, vma, addr);
                folio_add_lru_vma(folio, vma);
            } else {
                add_mm_counter(vma->vm_mm, mm_counter_file(page), nr);
                folio_add_file_rmap_ptes(folio, page, nr, vma);
            }
            set_ptes(vma->vm_mm, addr, vmf->pte, entry, nr);

            update_mmu_cache_range(vmf, vma, addr, vmf->pte, nr);
        }
    }
}
```

#### do_cow_fault

```c
do_cow_fault() { /* if (!(vma->vm_flags & VM_SHARED)) */
    anon_vma_prepare(vma)
    vmf->cow_page = alloc_page_vma()
    __do_fault(vmf);

    copy_user_highpage(vmf->cow_page, vmf->page, vmf->address, vma);
    finish_fault(vmf);
}
```

#### do_shared_fault

```c
do_shared_fault() {
    __do_fault(vmf);
    ext4_page_mkwrite();
    finish_fault(vmf);
    fault_dirty_shared_page();
}
```

## do_swap_page

![](../images/kernel/mem-fault-do_swap_fault.png)

```c
vm_fault_t do_swap_page(struct vm_fault *vmf)
{
    struct vm_area_struct *vma = vmf->vma;
    struct folio *swapcache, *folio = NULL;
    struct page *page;
    struct swap_info_struct *si = NULL;
    rmap_t rmap_flags = RMAP_NONE;
    bool exclusive = false;
    swp_entry_t entry;
    pte_t pte;
    vm_fault_t ret = 0;
    void *shadow = NULL;

    if (!pte_unmap_same(vmf))
        goto out;

/* 1. find swp_entry */
    entry = pte_to_swp_entry(vmf->orig_pte);
    if (unlikely(non_swap_entry(entry) { return swp_type(entry) >= MAX_SWAPFILES })) {
        if (is_migration_entry(entry)) {
            migration_entry_wait(vma->vm_mm, vmf->pmd, vmf->address);
        } else if (is_pte_marker_entry(entry)) {
            ret = handle_pte_marker(vmf);
        } else {
            print_bad_pte(vma, vmf->address, vmf->orig_pte, NULL);
            ret = VM_FAULT_SIGBUS;
        }
        goto out;
    }

    si = get_swap_device(entry);
    if (unlikely(!si))
        goto out;

/* 2. get swp cache page */
    folio = swap_cache_get_folio(entry, vma, vmf->address);
    if (folio) {
        page = folio_file_page(folio, swp_offset(entry));
    }
    swapcache = folio;

    if (!folio) {
        /* the 1st time of swp fault: for shared swap cache between processes */
        if (data_race(si->flags & SWP_SYNCHRONOUS_IO) && __swap_count(entry) == 1) {
            folio = vma_alloc_folio(GFP_HIGHUSER_MOVABLE, 0, vma, vmf->address, false);
            page = &folio->page;
            if (folio) {
                __folio_set_locked(folio);
                __folio_set_swapbacked(folio);

                if (mem_cgroup_swapin_charge_folio(folio, vma->vm_mm, GFP_KERNEL, entry)) {
                    ret = VM_FAULT_OOM;
                    goto out_page;
                }
                mem_cgroup_swapin_uncharge_swap(entry);

                shadow = get_shadow_from_swap_cache(entry);
                if (shadow)
                    workingset_refault(folio, shadow);

                folio_add_lru(folio);

                folio->swap = entry;
                swap_read_folio(folio, true, NULL) {
                    if (zswap_load(folio)) {
                        folio_mark_uptodate(folio);
                        folio_unlock(folio);
                    } else if (data_race(sis->flags & SWP_FS_OPS)) {
                        swap_read_folio_fs(folio, plug);
                    } else if (synchronous || (sis->flags & SWP_SYNCHRONOUS_IO)) {
                        swap_read_folio_bdev_sync(folio, sis);
                    } else {
                        swap_read_folio_bdev_async(folio, sis);
                    }
                }
                folio->private = NULL;
            }
        } else {
            /* the 2nd+ times page fault */
            page = swapin_readahead(entry, GFP_HIGHUSER_MOVABLE, vmf);
            if (page)
                folio = page_folio(page);
            swapcache = folio;
        }

        if (!folio) {
            vmf->pte = pte_offset_map_lock(vma->vm_mm, vmf->pmd, vmf->address, &vmf->ptl);
            if (likely(vmf->pte && pte_same(ptep_get(vmf->pte), vmf->orig_pte)))
                ret = VM_FAULT_OOM;
            goto unlock;
        }

        /* Had to read the page from swap area: Major fault */
        ret = VM_FAULT_MAJOR;
        count_vm_event(PGMAJFAULT);
        count_memcg_event_mm(vma->vm_mm, PGMAJFAULT);
    }

    ret |= folio_lock_or_retry(folio, vmf);
    if (ret & VM_FAULT_RETRY)
        goto out_release;

    if (swapcache) {
        if (unlikely(!folio_test_swapcache(folio) || page_swap_entry(page).val != entry.val))
            goto out_page;

        folio = ksm_might_need_to_copy(folio, vma, vmf->address);
        if (unlikely(!folio)) {
            ret = VM_FAULT_OOM;
            folio = swapcache;
            goto out_page;
        } else if (unlikely(folio == ERR_PTR(-EHWPOISON))) {
            ret = VM_FAULT_HWPOISON;
            folio = swapcache;
            goto out_page;
        }
        if (folio != swapcache)
            page = folio_page(folio, 0);

        if ((vmf->flags & FAULT_FLAG_WRITE) && folio == swapcache &&
            !folio_test_ksm(folio) && !folio_test_lru(folio))
            lru_add_drain();
    }

    folio_throttle_swaprate(folio, GFP_KERNEL);

    /* Back out if somebody else already faulted in this pte. */
    vmf->pte = pte_offset_map_lock(vma->vm_mm, vmf->pmd, vmf->address, &vmf->ptl);
    if (unlikely(!vmf->pte || !pte_same(ptep_get(vmf->pte), vmf->orig_pte)))
        goto out_nomap;

    if (unlikely(!folio_test_uptodate(folio))) {
        ret = VM_FAULT_SIGBUS;
        goto out_nomap;
    }

    if (!folio_test_ksm(folio)) {
        exclusive = pte_swp_exclusive(vmf->orig_pte);
        if (folio != swapcache) {
            exclusive = true;
        } else if (exclusive && folio_test_writeback(folio)
            && data_race(si->flags & SWP_STABLE_WRITES)) {

            exclusive = false;
        }
    }

    arch_swap_restore(entry, folio);

/* 3. free the swap entry and swap cache for last reference. */
    swap_free(entry)
        --->
    ret = should_try_to_free_swap(folio, vma, vmf->flags) {
        if (!folio_test_swapcache(folio))
            return false;
        if (mem_cgroup_swap_full(folio) || (vma->vm_flags & VM_LOCKED) ||
            folio_test_mlocked(folio))
            return true;
        return (fault_flags & FAULT_FLAG_WRITE) && !folio_test_ksm(folio) &&
            folio_ref_count(folio) == 2;
    }
    if (ret) {
        folio_free_swap(folio);
            --->
    }

    inc_mm_counter(vma->vm_mm, MM_ANONPAGES);
    dec_mm_counter(vma->vm_mm, MM_SWAPENTS);
    pte = mk_pte(page, vma->vm_page_prot);

    if (!folio_test_ksm(folio) && (exclusive || folio_ref_count(folio) == 1)) {
        if (vmf->flags & FAULT_FLAG_WRITE) {
            pte = maybe_mkwrite(pte_mkdirty(pte), vma);
            vmf->flags &= ~FAULT_FLAG_WRITE;
        }
        rmap_flags |= RMAP_EXCLUSIVE;
    }

    flush_icache_page(vma, page);
    if (pte_swp_soft_dirty(vmf->orig_pte))
        pte = pte_mksoft_dirty(pte);
    if (pte_swp_uffd_wp(vmf->orig_pte))
        pte = pte_mkuffd_wp(pte);
    vmf->orig_pte = pte;

    /* ksm created a completely new copy */
    if (unlikely(folio != swapcache && swapcache)) {
        folio_add_new_anon_rmap(folio, vma, vmf->address);
        folio_add_lru_vma(folio, vma);
    } else {
        folio_add_anon_rmap_pte(folio, page, vma, vmf->address, rmap_flags);
    }

/* 4. set pte, build mapping */
    set_pte_at(vma->vm_mm, vmf->address, vmf->pte, pte);
    arch_do_swap_page(vma->vm_mm, vma, vmf->address, pte, vmf->orig_pte);

    folio_unlock(folio);
    if (folio != swapcache && swapcache) {
        folio_unlock(swapcache);
        folio_put(swapcache);
    }

    if (vmf->flags & FAULT_FLAG_WRITE) {
        ret |= do_wp_page(vmf);
        if (ret & VM_FAULT_ERROR)
            ret &= VM_FAULT_ERROR;
        goto out;
    }

    /* No need to invalidate - it was non-present before */
    update_mmu_cache_range(vmf, vma, vmf->address, vmf->pte, 1);
unlock:
    if (vmf->pte)
        pte_unmap_unlock(vmf->pte, vmf->ptl);
out:
    if (si)
        put_swap_device(si);
    return ret;

out_nomap:
    if (vmf->pte)
        pte_unmap_unlock(vmf->pte, vmf->ptl);
out_page:
    folio_unlock(folio);
out_release:
    folio_put(folio);
    if (folio != swapcache && swapcache) {
        folio_unlock(swapcache);
        folio_put(swapcache);
    }
    if (si)
        put_swap_device(si);
    return ret;
}
```

## do_wp_page

![](../images/kernel/mem-fault-do_wp_page.png)

---

![](../images/kernel/mem-fault-wp_page_copy.png) |

```c
do_wp_page(vmf) {
    const bool unshare = vmf->flags & FAULT_FLAG_UNSHARE;
    struct vm_area_struct *vma = vmf->vma;
    struct folio *folio = NULL;
    pte_t pte;

    if (likely(!unshare)) {
        if (userfaultfd_pte_wp(vma, ptep_get(vmf->pte))) {
            if (!userfaultfd_wp_async(vma)) {
                pte_unmap_unlock(vmf->pte, vmf->ptl);
                return handle_userfault(vmf, VM_UFFD_WP);
            }

            pte = pte_clear_uffd_wp(ptep_get(vmf->pte));

            set_pte_at(vma->vm_mm, vmf->address, vmf->pte, pte);

            vmf->orig_pte = pte;
        }

        /* Userfaultfd write-protect can defer flushes. Ensure the TLB
         * is flushed in this case before copying. */
        if (unlikely(userfaultfd_wp(vmf->vma) && mm_tlb_flush_pending(vmf->vma->vm_mm)))
            flush_tlb_page(vmf->vma, vmf->address);
    }

    /* gets the page associated with a pte. */
    vmf->page = vm_normal_page(vma, vmf->address, vmf->orig_pte);

    if (vmf->page)
        folio = page_folio(vmf->page);

/* 1. Shared mapping */
    if (vma->vm_flags & (VM_SHARED | VM_MAYSHARE)) {
        if (!vmf->page) {
/* 1.1 pfn shared mapping: Page-ranges managed without "struct page", just pure PFN  */
            return wp_pfn_shared(vmf) {
                struct vm_area_struct *vma = vmf->vma;

                if (vma->vm_ops && vma->vm_ops->pfn_mkwrite) {
                    vm_fault_t ret;

                    pte_unmap_unlock(vmf->pte, vmf->ptl);
                    ret = vmf_can_call_fault(vmf);
                    if (ret)
                        return ret;

                    vmf->flags |= FAULT_FLAG_MKWRITE;
                    /* Validate Access: Ensures the write is allowed (e.g., within bounds, permissions).
                     * Prepare Memory: May notify hardware (e.g., GPU) or update metadata.
                     * Update PTE: Sets PTE_WRITABLE (e.g., AP = 01 on ARM64) to allow the write.
                     * Dirty Tracking: Optional, driver-specific; no address_space for PFN mappings, so dirtying is handled differently (e.g., via hardware or driver state). */
                    ret = vma->vm_ops->pfn_mkwrite(vmf);
                    if (ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE))
                        return ret;
                    return finish_mkwrite_fault(vmf, NULL);
                }
                wp_page_reuse(vmf, NULL);
                return 0;
            }
        }
/* 1.2 page shared mapping */
        return wp_page_shared(vmf, folio) {
            do_page_mkwrite();
        }
    }

/* 2. Private mapping
 * 2.1 the last proc can reuse the folio */
    if (folio && folio_test_anon(folio) &&
        (PageAnonExclusive(vmf->page) || wp_can_reuse_anon_folio(folio, vma))) {
        if (!PageAnonExclusive(vmf->page))
            SetPageAnonExclusive(vmf->page);
        if (unlikely(unshare)) {
            pte_unmap_unlock(vmf->pte, vmf->ptl);
            return 0;
        }
        wp_page_reuse(vmf, folio) {
            pte_mkyoung(vmf->orig_pte);
            maybe_mkwrite(pte_mkdirty(entry), vma);
        }
        return 0;
    }

/* 2.2. others need COW */
    return wp_page_copy(vmf) {
        const bool unshare = vmf->flags & FAULT_FLAG_UNSHARE;

        if (vmf->page)
            old_folio = page_folio(vmf->page);
        ret = vmf_anon_prepare(vmf);
        if (unlikely(ret))
            goto out;

/* 2.2.1 alloc new folio */
        pfn_is_zero = is_zero_pfn(pte_pfn(vmf->orig_pte));
        new_folio = folio_prealloc(mm, vma, vmf->address, pfn_is_zero);
        if (!new_folio)
            goto oom;

/* 2.2.2 copy user */
        if (!pfn_is_zero) {
            int err;
            err = __wp_page_copy_user(&new_folio->page, vmf->page, vmf);
            if (err) {
                folio_put(new_folio);
                if (old_folio)
                    folio_put(old_folio);

                delayacct_wpcopy_end();
                return err == -EHWPOISON ? VM_FAULT_HWPOISON : 0;
            }
            kmsan_copy_page_meta(&new_folio->page, vmf->page);
        }

        __folio_mark_uptodate(new_folio);

        mmu_notifier_range_init(&range, MMU_NOTIFY_CLEAR, 0, mm,
                    vmf->address & PAGE_MASK,
                    (vmf->address & PAGE_MASK) + PAGE_SIZE);
        mmu_notifier_invalidate_range_start(&range);

/* 2.2.3 set pte */
        vmf->pte = pte_offset_map_lock(mm, vmf->pmd, vmf->address, &vmf->ptl);
        if (likely(vmf->pte && pte_same(ptep_get(vmf->pte), vmf->orig_pte))) {
            flush_cache_page(vma, vmf->address, pte_pfn(vmf->orig_pte));
            entry = mk_pte(&new_folio->page, vma->vm_page_prot);
            entry = pte_sw_mkyoung(entry);
            if (unlikely(unshare)) {
                if (pte_soft_dirty(vmf->orig_pte))
                    entry = pte_mksoft_dirty(entry);
                if (pte_uffd_wp(vmf->orig_pte))
                    entry = pte_mkuffd_wp(entry);
            } else {
                entry = maybe_mkwrite(pte_mkdirty(entry), vma);
            }

/* 2.2.4 rmap, lru, cgroup, mmu_notifier */
            ptep_clear_flush(vma, vmf->address, vmf->pte);
            folio_add_new_anon_rmap(new_folio, vma, vmf->address);
            folio_add_lru_vma(new_folio, vma);

            BUG_ON(unshare && pte_write(entry));
            set_pte_at_notify(mm, vmf->address, vmf->pte, entry);
            update_mmu_cache_range(vmf, vma, vmf->address, vmf->pte, 1);
            if (old_folio) {
                folio_remove_rmap_pte(old_folio, vmf->page, vma);
            }

            /* Free the old page.. */
            new_folio = old_folio;
            page_copied = 1;
            pte_unmap_unlock(vmf->pte, vmf->ptl);
        } else if (vmf->pte) {
            update_mmu_tlb(vma, vmf->address, vmf->pte);
            pte_unmap_unlock(vmf->pte, vmf->ptl);
        }

        mmu_notifier_invalidate_range_end(&range);

        if (new_folio)
            folio_put(new_folio);
        if (old_folio) {
            if (page_copied)
                free_swap_cache(old_folio);
            folio_put(old_folio);
        }

        return 0;
    oom:
        ret = VM_FAULT_OOM;
    out:
        if (old_folio)
            folio_put(old_folio);

        return ret;
    }
}
```

## do_numa_page

```c

```

## do_kernel_fault

```c
__do_kernel_fault(unsigned long addr, unsigned long esr,
    struct pt_regs *regs)
{
    const char *msg;

    /* Are we prepared to handle this kernel fault?
     * We are almost certainly not prepared to handle instruction faults. */
    if (!is_el1_instruction_abort(esr) && fixup_exception(regs))
        return;

    if (WARN_RATELIMIT(is_spurious_el1_translation_fault(addr, esr, regs),
        "Ignoring spurious kernel translation fault at virtual address %016lx\n", addr))
        return;

    if (is_el1_mte_sync_tag_check_fault(esr)) {
        do_tag_recovery(addr, esr, regs);

        return;
    }

    if (is_el1_permission_fault(addr, esr, regs)) {
        if (esr & ESR_ELx_WNR)
            msg = "write to read-only memory";
        else if (is_el1_instruction_abort(esr))
            msg = "execute from non-executable memory";
        else
            msg = "read from unreadable memory";
    } else if (addr < PAGE_SIZE) {
        msg = "NULL pointer dereference";
    } else {
        if (esr_fsc_is_translation_fault(esr) &&
            kfence_handle_page_fault(addr, esr & ESR_ELx_WNR, regs))
            return;

        msg = "paging request";
    }

    if (efi_runtime_fixup_exception(regs, msg))
        return;

    die_kernel_fault(msg, addr, esr, regs);
}
```

```c
struct exception_table_entry
{
    int insn, fixup;
    short type, data;
};

#define __ASM_EXTABLE_RAW(insn, fixup, type, data)  \
    .pushsection    __ex_table, "a";                \
    .align        2;                                \
    .long        ((insn) - .);                      \
    .long        ((fixup) - .);                     \
    .short        (type);                           \
    .short        (data);                           \
    .popsection;

start_kernel() {
    /* Sort the kernel's built-in exception table */
    void __init sort_main_extable(void)
    {
        if (main_extable_sort_needed &&
            &__stop___ex_table > &__start___ex_table) {
            pr_notice("Sorting __ex_table...\n");
            sort_extable(__start___ex_table, __stop___ex_table);
        }
    }
}
```

```c
bool fixup_exception(struct pt_regs *regs)
{
    const struct exception_table_entry *ex;

    ex = search_exception_tables(instruction_pointer(regs));
    if (!ex)
        return false;

    switch (ex->type) {
    case EX_TYPE_BPF:
        return ex_handler_bpf(ex, regs);
    case EX_TYPE_UACCESS_ERR_ZERO:
    case EX_TYPE_KACCESS_ERR_ZERO:
        return ex_handler_uaccess_err_zero(ex, regs) {
            int reg_err = FIELD_GET(EX_DATA_REG_ERR, ex->data);
            int reg_zero = FIELD_GET(EX_DATA_REG_ZERO, ex->data);

            pt_regs_write_reg(regs, reg_err, -EFAULT);
            pt_regs_write_reg(regs, reg_zero, 0);

            regs->pc = get_ex_fixup(ex);
            return true;
        }
    case EX_TYPE_LOAD_UNALIGNED_ZEROPAD:
        return ex_handler_load_unaligned_zeropad(ex, regs);
    }

    BUG();
}
```

# munmap

```c
munmap()
SYSCALL_DEFINE2(munmap) {
    __vm_munmap(addr, len, true) {
        do_vmi_munmap() {
            vma = vma_find(vmi, end);
            do_vmi_align_munmap() {
                if (start > vma->vm_start) {
                    error = __split_vma(vmi, vma, start, 1);
                }

                unmap_region() {
                    lru_add_drain();
                    tlb_gather_mmu(&tlb, mm);
                    update_hiwater_rss(mm);

                    /* 1. just unmap the phys which is mmaped with vma, vma is free by vm_area_free */
                    unmap_vmas(&tlb, mt, vma, start, end, mm_wr_locked)
                        --->
                    /* 2 free pg table */
                    free_pgtables(&tlb)
                        --->
                    /* 3. finish gather */
                    tlb_finish_mmu(&tlb)
                        --->
                }

                remove_mt(mm, &mas_detach) {
                    mas_for_each(mas, vma, ULONG_MAX) {
                        remove_vma(vma) {
                            vm_area_free(vma) {
                                free_anon_vma_name(vma);
                                kmem_cache_free(vm_area_cachep, vma);
                            }
                        }
                    }
                }
            }
        }
    }
}
```

# mmu_gather

* [ARM64内核源码解读: mmu-gather操作](https://blog.csdn.net/m0_50662680/article/details/128445158)

![](../images/kernel/mem-mmu_gather.png)

```c
struct mmu_gather {
    struct mm_struct        *mm;

    /* gather page table memory */
    struct mmu_table_batch  *batch;

    unsigned long        start;
    unsigned long        end;

    unsigned int        fullmm : 1;
    unsigned int        need_flush_all : 1;
    unsigned int        freed_tables : 1;
    unsigned int        delayed_rmap : 1;
    unsigned int        cleared_ptes : 1;
    unsigned int        cleared_pmds : 1;
    unsigned int        cleared_puds : 1;
    unsigned int        cleared_p4ds : 1;

    unsigned int        vma_exec : 1;
    unsigned int        vma_huge : 1;
    unsigned int        vma_pfn  : 1;

    unsigned int        batch_count;

    /* gather normla memory */
    struct mmu_gather_batch     *active;
    struct mmu_gather_batch     local;
    struct page                 *__pages[MMU_GATHER_BUNDLE];

#ifdef CONFIG_MMU_GATHER_PAGE_SIZE
    unsigned int page_size;
#endif
};

struct mmu_table_batch {
    struct rcu_head     rcu;
    unsigned int        nr;
    void                *tables[];
};

struct mmu_gather_batch {
    struct mmu_gather_batch     *next;
    unsigned int                nr;
    unsigned int                max;
    struct encoded_page         *encoded_pages[];
};
```

## tlb_gather_mmu
```c
tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm, bool fullmm) {
    tlb->mm = mm;
    tlb->fullmm = fullmm;

#ifndef CONFIG_MMU_GATHER_NO_GATHER
    tlb->need_flush_all = 0;
    tlb->local.next = NULL;
    tlb->local.nr   = 0;
    tlb->local.max  = ARRAY_SIZE(tlb->__pages);
    tlb->active     = &tlb->local;
    tlb->batch_count = 0;
#endif
    tlb->delayed_rmap = 0;

    tlb_table_init(tlb) {
        tlb->batch = NULL;
    }

#ifdef CONFIG_MMU_GATHER_PAGE_SIZE
    tlb->page_size = 0;
#endif

    __tlb_reset_range(tlb) {
        if (tlb->fullmm) {
            tlb->start = tlb->end = ~0;
        } else {
            tlb->start = TASK_SIZE;
            tlb->end = 0;
        }
        tlb->freed_tables = 0;
        tlb->cleared_ptes = 0;
        tlb->cleared_pmds = 0;
        tlb->cleared_puds = 0;
        tlb->cleared_p4ds = 0;
    }

    inc_tlb_flush_pending(tlb->mm);
}
```

## unmap_vmas

```c
unmap_vmas(&tlb, mt, vma, start, end, mm_wr_locked) {
    do {
        unmap_single_vma();
            unmap_page_range() {
                tlb_start_vma(tlb, vma);

                pgd = pgd_offset(vma->vm_mm, addr);
                do {
                    next = pgd_addr_end(addr, end);
                    if (pgd_none_or_clear_bad(pgd)) {
                        continue;
                    }
                    next = zap_p4d_range(tlb, vma, pgd, addr, next, details); {
                        p4d = p4d_offset(pgd, addr);
                        do {
                            next = p4d_addr_end(addr, end);
                            if (p4d_none_or_clear_bad(p4d)) {
                                continue;
                            }
                            next = zap_pud_range(tlb, vma, p4d, addr, next, details) {
                                do {
                                    if (pud_none_or_clear_bad(pud)) {
                                        continue;
                                    }
                                    next = zap_pmd_range(tlb, vma, pud, addr, next, details); {
                                        zap_pte_range() {
                                            start_pte = pte = pte_offset_map_lock(mm, pmd, addr, &ptl);
                                            flush_tlb_batched_pending(mm);
                                            arch_enter_lazy_mmu_mode();

                                            do {
                                                pte_t ptent = *pte;
                                                struct page *page;

                                                if (pte_present(ptent)) {
                                                    unsigned int delay_rmap;

                                                    page = vm_normal_page(vma, addr, ptent);
                                                    /* clear pte entry */
                                                    ptent = ptep_get_and_clear_full(mm, addr, pte, tlb->fullmm);

                                                    tlb_remove_tlb_entry(tlb, pte, addr) {
                                                        tlb_flush_pte_range() {
                                                            __tlb_adjust_range(tlb, address, size) {
                                                                tlb->start = min(tlb->start, address);
                                                                tlb->end = max(tlb->end, address + range_size);
                                                            }
                                                            tlb->cleared_ptes = 1;
                                                        }
                                                        __tlb_remove_tlb_entry();
                                                    }
                                                    zap_install_uffd_wp_if_needed(vma, addr, pte, details, ptent);
                                                    if (unlikely(!page))
                                                        continue;

                                                    delay_rmap = 0;
                                                    if (!PageAnon(page)) {
                                                        if (pte_dirty(ptent)) {
                                                            set_page_dirty(page);
                                                            if (tlb_delay_rmap(tlb)) {
                                                                delay_rmap = 1;
                                                                force_flush = 1;
                                                            }
                                                        }
                                                    }

                                                    ret = __tlb_remove_page(tlb, page, delay_rmap) {
                                                        batch = tlb->active;
                                                        batch->encoded_pages[batch->nr++] = page;
                                                        if (batch->nr == batch->max) {
                                                            ret = tlb_next_batch(tlb) {
                                                                batch = tlb->active;
                                                                if (batch->next) {
                                                                    tlb->active = batch->next;
                                                                    return true;
                                                                }

                                                                if (tlb->batch_count == MAX_GATHER_BATCH_COUNT)
                                                                    return false;

                                                                batch = (void *)__get_free_page(GFP_NOWAIT | __GFP_NOWARN);

                                                                tlb->batch_count++;
                                                                batch->next = NULL;
                                                                batch->nr   = 0;
                                                                batch->max  = MAX_GATHER_BATCH;

                                                                tlb->active->next = batch;
                                                                tlb->active = batch;

                                                                return true;
                                                            }
                                                            if (!ret)
                                                                return true;
                                                            batch = tlb->active;
                                                        }
                                                    }
                                                    if (ret) {
                                                        force_flush = 1;
                                                        addr += PAGE_SIZE;
                                                        break;
                                                    }
                                                    continue;
                                                }

                                                pte_clear_not_present_full(mm, addr, pte, tlb->fullmm);
                                                zap_install_uffd_wp_if_needed(vma, addr, pte, details, ptent);
                                            } while (pte++, addr += PAGE_SIZE, addr != end);

                                            if (force_flush) {
                                                tlb_flush_mmu_tlbonly(tlb);
                                                tlb_flush_rmaps(tlb, vma);
                                            }
                                            pte_unmap_unlock(start_pte, ptl);

                                            if (force_flush)
                                                tlb_flush_mmu(tlb);
                                                    --->

                                            return addr;
                                        }
                                    }
                                next:
                                    cond_resched();
                                } while (pud++, addr = next, addr != end);
                            }
                        } while (p4d++, addr = next, addr != end);
                    }
                } while (pgd++, addr = next, addr != end);

                tlb_end_vma(tlb, vma) {
                    tlb_flush_mmu_tlbonly(tlb);
                }
            }
    } while ((vma = mas_find(&mas, end_addr - 1)) != NULL);
}
```

## free_pgtables

```c
/* 2 free pg table */
free_pgtables(&tlb) {
    do {
        unlink_anon_vmas(vma) {
            list_for_each_entry_safe(avc, next, &vma->anon_vma_chain, same_vma) {
                struct anon_vma *anon_vma = avc->anon_vma;
                anon_vma_interval_tree_remove(avc, &anon_vma->rb_root);
                list_del(&avc->same_vma);
                anon_vma_chain_free(avc) {
                    kmem_cache_free(anon_vma_chain_cachep);
                }
            }
        }

        unlink_file_vma(vma) {
            struct file *file = vma->vm_file;
            if (file) {
                struct address_space *mapping = file->f_mapping;
                i_mmap_lock_write(mapping);
                __remove_shared_vm_struct(vma, file, mapping) {
                    vma_interval_tree_remove(vma, &mapping->i_mmap);
                }
                i_mmap_unlock_write(mapping);
            }
        }

        free_pgd_range() {
            addr &= PMD_MASK;
            if (addr < floor) {
                addr += PMD_SIZE;
                if (!addr)
                    return;
            }
            if (ceiling) {
                ceiling &= PMD_MASK;
                if (!ceiling)
                    return;
            }
            if (end - 1 > ceiling - 1)
                end -= PMD_SIZE;
            if (addr > end - 1)
                return;

            do {
                next = pgd_addr_end(addr, end);
                if (pgd_none_or_clear_bad(pgd))
                    continue;
                free_p4d_range(tlb, pgd, addr, next, floor, ceiling) {
                    do {
                        next = p4d_addr_end(addr, end);
                        if (p4d_none_or_clear_bad(p4d))
                            continue;
                        free_pud_range(tlb, p4d, addr, next, floor, ceiling) {
                            do {
                                next = pud_addr_end(addr, end);
                                if (pud_none_or_clear_bad(pud))
                                    continue;
                                free_pmd_range(tlb, pud, addr, next, floor, ceiling); {
                                    do {
                                        next = pmd_addr_end(addr, end);
                                        if (pmd_none_or_clear_bad(pmd))
                                            continue;
                                        free_pte_range(tlb, pmd, addr); {
                                            pgtable_t token = pmd_pgtable(*pmd);
                                            pmd_clear(pmd);
                                            pte_free_tlb(tlb, token, addr) {
                                                tlb_flush_pmd_range(tlb, address, PAGE_SIZE) {
                                                    __tlb_adjust_range(tlb, address, size);
                                                    tlb->cleared_pmds = 1;
                                                }
                                                tlb->freed_tables = 1;
                                                __pte_free_tlb(tlb, ptep, address) {
                                                    pgtable_pte_page_dtor(pte);
                                                    tlb_remove_table(tlb, pte) {
                                                        struct mmu_table_batch **batch = &tlb->batch;

                                                        if (*batch == NULL) {
                                                            *batch = (struct mmu_table_batch *)__get_free_page(GFP_NOWAIT | __GFP_NOWARN);
                                                            if (*batch == NULL) {
                                                                tlb_table_invalidate(tlb);
                                                                tlb_remove_table_one(table);
                                                                return;
                                                            }
                                                            (*batch)->nr = 0;
                                                        }

                                                        (*batch)->tables[(*batch)->nr++] = table;
                                                        if ((*batch)->nr == MAX_TABLE_BATCH) {
                                                            tlb_table_flush(tlb);
                                                                --->
                                                        }
                                                    }
                                                }
                                            }
                                            mm_dec_nr_ptes(tlb->mm);
                                        }
                                    } while (pmd++, addr = next, addr != end);

                                    start &= PUD_MASK;
                                    if (start < floor)
                                        return;
                                    if (ceiling) {
                                        ceiling &= PUD_MASK;
                                        if (!ceiling)
                                            return;
                                    }
                                    if (end - 1 > ceiling - 1)
                                        return;

                                    pud_clear(pud);
                                    pmd_free_tlb(tlb, pmd, start) {
                                        tlb_flush_pud_range(tlb, address, PAGE_SIZE) {
                                            __tlb_adjust_range(tlb, address, size);
                                            tlb->cleared_puds = 1;
                                        }
                                        tlb->freed_tables = 1;
                                        __pmd_free_tlb(tlb, pmdp, address) {
                                            pgtable_pmd_page_dtor(page);
                                            tlb_remove_table(tlb, page);
                                                --->
                                        }
                                    }
                                    mm_dec_nr_pmds(tlb->mm);
                                }
                            } while (pud++, addr = next, addr != end);

                            start &= P4D_MASK;
                            if (start < floor)
                                return;
                            if (ceiling) {
                                ceiling &= P4D_MASK;
                                if (!ceiling)
                                    return;
                            }
                            if (end - 1 > ceiling - 1)
                                return;

                            pud = pud_offset(p4d, start);
                            p4d_clear(p4d);
                            pud_free_tlb(tlb, pud, start) {
                                tlb_remove_table(tlb, pud);
                                    --->
                            }
                            mm_dec_nr_puds(tlb->mm);
                        }

                        p4d = p4d_offset(pgd, start);
                        pgd_clear(pgd);
                        p4d_free_tlb(tlb, p4d, start) {

                        }
                    } while (p4d++, addr = next, addr != end);
                }
            } while (pgd++, addr = next, addr != end)
        }
    } while (vma);
}
```

## tlb_finish_mmu
```c
/* 3. finish gather */
tlb_finish_mmu(&tlb) {
    tlb_flush_mmu(tlb) {
        tlb_flush_mmu_tlbonly(tlb)  {
            if (!(tlb->freed_tables
                || tlb->cleared_ptes || tlb->cleared_pmds
                || tlb->cleared_puds || tlb->cleared_p4ds)) {

                return;
            }
            tlb_flush(tlb) {
                if (tlb->fullmm) {
                    if (!last_level)
                        flush_tlb_mm(tlb->mm);
                    return;
                }

                __flush_tlb_range(&vma, tlb->start, tlb->end, stride,
                        last_level, tlb_level);
            }
            __tlb_reset_range(tlb) {
                if (tlb->fullmm) {
                    tlb->start = tlb->end = ~0;
                } else {
                    tlb->start = TASK_SIZE;
                    tlb->end = 0;
                }
                tlb->freed_tables = 0;
                tlb->cleared_ptes = 0;
                tlb->cleared_pmds = 0;
                tlb->cleared_puds = 0;
                tlb->cleared_p4ds = 0;
            }
        }

        tlb_flush_mmu_free(tlb) {
            tlb_table_flush(tlb)  {
                **batch = &tlb->batch;
                if (*batch) {
                    tlb_table_invalidate(tlb) {
                        if (tlb_needs_table_invalidate()) {
                            tlb_flush_mmu_tlbonly(tlb)
                                --->
                        }
                    }
                    tlb_remove_table_free(*batch) {
                        for (i = 0; i < batch->nr; i++) {
                            __tlb_remove_table(batch->tables[i]) {
                                free_page_and_swap_cache() {
                                    struct folio *folio = page_folio(page);
                                    free_swap_cache(folio);
                                    if (!is_huge_zero_page(page))
                                        folio_put(folio);
                                }
                            }
                        }
                        free_page((unsigned long)batch);
                    }
                    *batch = NULL;
                }
            }

            tlb_batch_pages_flush(tlb) {
                for (batch = &tlb->local; batch && batch->nr; batch = batch->next) {
                    struct encoded_page **pages = batch->encoded_pages;

                    do {
                        unsigned int nr = min(512U, batch->nr);

                        free_pages_and_swap_cache(pages, nr) {
                            lru_add_drain();
                            for (int i = 0; i < nr; i++) {
                                free_swap_cache(encoded_page_ptr(pages[i]));
                            }
                            release_pages(pages, nr) {
                                mem_cgroup_uncharge_list(&pages_to_free);
                                free_unref_page_list(&pages_to_free);
                            }
                        }
                        pages += nr;
                        batch->nr -= nr;

                        cond_resched();
                    } while (batch->nr);
                }
                tlb->active = &tlb->local;
            }
        }
    }

    tlb_batch_list_free() {
        for (batch = tlb->local.next; batch; batch = next) {
            next = batch->next;
            free_pages((unsigned long)batch, 0);
        }
    }
}
```

# mremap

```c
SYSCALL_DEFINE5(mremap) {
    vma = vma_lookup(mm, addr);
    if (flags & (MREMAP_FIXED | MREMAP_DONTUNMAP)) {
        ret = mremap_to(addr, old_len, new_addr, new_len,
                &locked, flags, &uf, &uf_unmap_early,
                &uf_unmap);
        goto out;
    }
    if (old_len >= new_len) {
        VMA_ITERATOR(vmi, mm, addr + new_len);

        if (old_len == new_len) {
            ret = addr;
            goto out;
        }

        ret = do_vmi_munmap(&vmi, mm, addr + new_len, old_len - new_len,
                    &uf_unmap, true);
        if (ret)
            goto out;

        ret = addr;
        goto out_unlocked;
    }

    /* Ok, we need to grow */
    vma = vma_to_resize(addr, old_len, new_len, flags);

    /* old_len exactly to the end of the area.. */
    if (old_len == vma->vm_end - addr) {
        /* can we just expand the current mapping? */
        if (vma_expandable(vma, new_len - old_len)) {
            long pages = (new_len - old_len) >> PAGE_SHIFT;
            unsigned long extension_start = addr + old_len;
            unsigned long extension_end = addr + new_len;
            pgoff_t extension_pgoff = vma->vm_pgoff +
                ((extension_start - vma->vm_start) >> PAGE_SHIFT);
            VMA_ITERATOR(vmi, mm, extension_start);

            vma = vma_merge(&vmi, mm, vma, extension_start,
                extension_end, vma->vm_flags, vma->anon_vma,
                vma->vm_file, extension_pgoff, vma_policy(vma),
                vma->vm_userfaultfd_ctx, anon_vma_name(vma));
            if (!vma) {
                vm_unacct_memory(pages);
                ret = -ENOMEM;
                goto out;
            }

            if (vma->vm_flags & VM_LOCKED) {
                mm->locked_vm += pages;
                locked = true;
                new_addr = addr;
            }
            ret = addr;
            goto out;
        }
    }

    ret = -ENOMEM;
    if (flags & MREMAP_MAYMOVE) {
        unsigned long map_flags = 0;
        if (vma->vm_flags & VM_MAYSHARE)
            map_flags |= MAP_SHARED;

        new_addr = get_unmapped_area(vma->vm_file, 0, new_len,
            vma->vm_pgoff + ((addr - vma->vm_start) >> PAGE_SHIFT),
            map_flags);
        if (IS_ERR_VALUE(new_addr)) {
            ret = new_addr;
            goto out;
        }

        ret = move_vma(vma, addr, old_len, new_len, new_addr,
            &locked, flags, &uf, &uf_unmap);
    }
out:
    return ret;
}
```

# rmap

![](../images/kernel/mem-rmap-arch.svg)

![](../images/kernel/mem-rmap-1.png)

* [wowotech - 逆向映射的演进](http://www.wowotech.net/memory_management/reverse_mapping.html)
* [五花肉 - linux内核反向映射(RMAP)技术分析 - 知乎](https://zhuanlan.zhihu.com/p/564867734)
* [linux 匿名页反向映射 - 知乎](https://zhuanlan.zhihu.com/p/361173109)
* https://blog.csdn.net/u010923083/article/details/116456497
* https://zhuanlan.zhihu.com/p/448713030

```c
struct vm_area_struct {
    union {
        struct {
            /* VMA covers [vm_start; vm_end) addresses within mm */
            unsigned long vm_start;
            unsigned long vm_end;
        };
        freeptr_t vm_freeptr; /* Pointer used by SLAB_TYPESAFE_BY_RCU */
    };

    struct list_head    anon_vma_chain;
    struct anon_vma*    anon_vma;

    /* key used to insert into anon_vma rb_root */
    unsigned long vm_pgoff; /* Offset (within vm_file) in PAGE_SIZE units */
    struct file * vm_file;  /* File we map to (can be NULL). */
}

struct anon_vma {
    struct anon_vma         *root;
    struct rw_semaphore     rwsem;

    unsigned                degree;
    atomic_t                refcount;
    struct anon_vma         *parent;
    struct rb_root_cached   rb_root;
};

struct anon_vma_chain {
    struct vm_area_struct*  vma;
    struct anon_vma*        anon_vma;
    struct list_head        same_vma;
    struct rb_node          rb;
    unsigned long           rb_subtree_last;
};

struct page {
    /* If low bit clear, points to inode address_space, or NULL.
     * If page mapped as anonymous memory, low bit is set,
     * and it points to anon_vma object */
    struct address_space *mapping;

    union {
        /* 1. anon mapping: page offset in user virtual address space
         * 2. file mapping: page offset in file
         * 3. migration type
         * 4. swap_entry_t */
        pgoff_t index;
        union {
            atomic_t _mapcount;
        }
    }
};

INTERVAL_TREE_DEFINE(struct anon_vma_chain, rb, unsigned long, rb_subtree_last,
            avc_start_pgoff, avc_last_pgoff,
            static inline, __anon_vma_interval_tree)
```

## anon_vma_prepare

```c
/* attach an anon_vma to a memory region */
anon_vma_prepare() {
    if (likely(vma->anon_vma))
        return 0;

    avc = anon_vma_chain_alloc(GFP_KERNEL);
    anon_vma = find_mergeable_anon_vma(vma);
    anon_vma = anon_vma_alloc()
    vma->anon_vma = anon_vma;
    anon_vma_chain_link(vma, avc, anon_vma) {
        avc->vma = vma;
        avc->anon_vma = anon_vma;
        list_add(&avc->same_vma, &vma->anon_vma_chain);
        anon_vma_interval_tree_insert(avc, &anon_vma->rb_root);
    }
}

folio_add_new_anon_rmap() {
    __folio_set_anon() {
        struct anon_vma *anon_vma = vma->anon_vma;
        anon_vma = (void *) anon_vma + PAGE_MAPPING_ANON;
        WRITE_ONCE(folio->mapping, (struct address_space *) anon_vma);
        folio->index = linear_page_index(vma, address) {
            pgoff_t pgoff;
            if (unlikely(is_vm_hugetlb_page(vma))) {
                return linear_hugepage_index(vma, address);
            }
            pgoff = (address - vma->vm_start) >> PAGE_SHIFT;
            pgoff += vma->vm_pgoff;
            return pgoff;
        }
    }

    __folio_set_swapbacked(folio);

    if (likely(!folio_test_large(folio))) {
        /* increment count (starts at -1) */
        atomic_set(&folio->_mapcount, 0);
        SetPageAnonExclusive(&folio->page);
    } else if (!folio_test_pmd_mappable(folio)) {
        int i;

        for (i = 0; i < nr; i++) {
            struct page *page = folio_page(folio, i);

            /* increment count (starts at -1) */
            atomic_set(&page->_mapcount, 0);
            SetPageAnonExclusive(page);
        }

        atomic_set(&folio->_nr_pages_mapped, nr);
    } else {
        /* increment count (starts at -1) */
        atomic_set(&folio->_entire_mapcount, 0);
        atomic_set(&folio->_nr_pages_mapped, ENTIRELY_MAPPED);
        SetPageAnonExclusive(&folio->page);
        __lruvec_stat_mod_folio(folio, NR_ANON_THPS, nr);
    }

    __lruvec_stat_mod_folio(folio, NR_ANON_MAPPED, nr);
}
```

## anon_vma_fork

![](../images/kernel/mem-ramp-anon_vma_prepare.png)

```c
dup_mmap(mm, oldmm) {
    for_each_vma(vmi, mpnt) {
        anon_vma_fork();
    }
}

int anon_vma_fork(struct vm_area_struct *vma, struct vm_area_struct *pvma)
{
    struct anon_vma_chain *avc;
    struct anon_vma *anon_vma;
    int error;

    /* Don't bother if the parent process has no anon_vma here. */
    if (!pvma->anon_vma)
        return 0;

    /* Drop inherited anon_vma, we'll reuse existing or allocate new. */
    vma->anon_vma = NULL;

    /* 1. attach the new VMA to the parent VMA's anon_vmas,
     * so rmap can find non-COWed pages in child processes. */
    error = anon_vma_clone(vma/*dst*/, pvma/*src*/) {
        list_for_each_entry_reverse(pavc, &src->anon_vma_chain, same_vma) {
            struct anon_vma *anon_vma;

            avc = anon_vma_chain_alloc(GFP_NOWAIT | __GFP_NOWARN);
            anon_vma = pavc->anon_vma;
            anon_vma_chain_link(dst, avc, anon_vma);
        }
        if (dst->anon_vma)
            dst->anon_vma->num_active_vmas++;
        return 0;
    }

    /* An existing anon_vma has been reused, all done then. */
    if (vma->anon_vma)
        return 0;

    /* 2. Then add our own anon_vma. */
    anon_vma = anon_vma_alloc();
    anon_vma->num_active_vmas++;
    avc = anon_vma_chain_alloc(GFP_KERNEL);

    anon_vma->root = pvma->anon_vma->root;
    anon_vma->parent = pvma->anon_vma;

    get_anon_vma(anon_vma->root);
    /* Mark this anon_vma as the one where our new (COWed) pages go. */
    vma->anon_vma = anon_vma;
    anon_vma_lock_write(anon_vma);
    anon_vma_chain_link(vma, avc, anon_vma);
    anon_vma->parent->num_children++;
    anon_vma_unlock_write(anon_vma);

    return 0;
}
```

### page_vma_mapped_walk

```c
/* check if @pvmw->pfn is mapped in @pvmw->vma at @pvmw->address */
bool page_vma_mapped_walk(struct page_vma_mapped_walk *pvmw)
{
    struct vm_area_struct *vma = pvmw->vma;
    struct mm_struct *mm = vma->vm_mm;
    unsigned long end;
    spinlock_t *ptl;
    pgd_t *pgd;
    p4d_t *p4d;
    pud_t *pud;
    pmd_t pmde;

    /* The only possible pmd mapping has been handled on last iteration */
    if (pvmw->pmd && !pvmw->pte)
        return not_found(pvmw);

    if (unlikely(is_vm_hugetlb_page(vma))) {
        struct hstate *hstate = hstate_vma(vma);
        unsigned long size = huge_page_size(hstate);
        /* The only possible mapping was handled on last iteration */
        if (pvmw->pte)
            return not_found(pvmw);
        /* All callers that get here will already hold the
        * i_mmap_rwsem.  Therefore, no additional locks need to be
        * taken before calling hugetlb_walk(). */
        pvmw->pte = hugetlb_walk(vma, pvmw->address, size);
        if (!pvmw->pte)
            return false;

        pvmw->ptl = huge_pte_lock(hstate, mm, pvmw->pte);
        if (!check_pte(pvmw, pages_per_huge_page(hstate)))
            return not_found(pvmw);
        return true;
    }

    end = vma_address_end(pvmw);
    if (pvmw->pte)
        goto next_pte;
restart:
    do {
        pgd = pgd_offset(mm, pvmw->address);
        if (!pgd_present(*pgd)) {
            step_forward(pvmw, PGDIR_SIZE);
            continue;
        }
        p4d = p4d_offset(pgd, pvmw->address);
        if (!p4d_present(*p4d)) {
            step_forward(pvmw, P4D_SIZE);
            continue;
        }
        pud = pud_offset(p4d, pvmw->address);
        if (!pud_present(*pud)) {
            step_forward(pvmw, PUD_SIZE) {
                pvmw->address = (pvmw->address + size) & ~(size - 1);
                if (!pvmw->address)
                    pvmw->address = ULONG_MAX;
            }
            continue;
        }

        pvmw->pmd = pmd_offset(pud, pvmw->address);
        /* Make sure the pmd value isn't cached in a register by the
        * compiler and used as a stale value after we've observed a
        * subsequent update. */
        pmde = pmdp_get_lockless(pvmw->pmd);

        if (pmd_trans_huge(pmde) || is_pmd_migration_entry(pmde) || (pmd_present(pmde) && pmd_devmap(pmde))) {
            pvmw->ptl = pmd_lock(mm, pvmw->pmd);
            pmde = *pvmw->pmd;
            if (!pmd_present(pmde)) {
                swp_entry_t entry;

                if (!thp_migration_supported() || !(pvmw->flags & PVMW_MIGRATION))
                    return not_found(pvmw);
                entry = pmd_to_swp_entry(pmde);
                if (!is_migration_entry(entry) || !check_pmd(swp_offset_pfn(entry), pvmw))
                    return not_found(pvmw);
                return true;
            }
            if (likely(pmd_trans_huge(pmde) || pmd_devmap(pmde))) {
                if (pvmw->flags & PVMW_MIGRATION)
                    return not_found(pvmw);
                if (!check_pmd(pmd_pfn(pmde), pvmw))
                    return not_found(pvmw);
                return true;
            }
            /* THP pmd was split under us: handle on pte level */
            spin_unlock(pvmw->ptl);
            pvmw->ptl = NULL;
        } else if (!pmd_present(pmde)) {
            /* If PVMW_SYNC, take and drop THP pmd lock so that we
            * cannot return prematurely, while zap_huge_pmd() has
            * cleared *pmd but not decremented compound_mapcount(). */
            if ((pvmw->flags & PVMW_SYNC)
                && thp_vma_suitable_order(vma, pvmw->address, PMD_ORDER)
                && (pvmw->nr_pages >= HPAGE_PMD_NR)) {

                spinlock_t *ptl = pmd_lock(mm, pvmw->pmd);

                spin_unlock(ptl);
            }
            step_forward(pvmw, PMD_SIZE);
            continue;
        }
        if (!map_pte(pvmw, &pmde, &ptl) /* ---> */) {
            if (!pvmw->pte)
                goto restart;
            goto next_pte;
        }
this_pte:
        if (check_pte(pvmw, 1))
            return true;
next_pte:
        do {
            pvmw->address += PAGE_SIZE;
            if (pvmw->address >= end)
                return not_found(pvmw);
            /* Did we cross page table boundary? */
            if ((pvmw->address & (PMD_SIZE - PAGE_SIZE)) == 0) {
                if (pvmw->ptl) {
                    spin_unlock(pvmw->ptl);
                    pvmw->ptl = NULL;
                }
                pte_unmap(pvmw->pte);
                pvmw->pte = NULL;
                goto restart;
            }
            pvmw->pte++;
        } while (pte_none(ptep_get(pvmw->pte)));

        if (!pvmw->ptl) {
            spin_lock(ptl);
            if (unlikely(!pmd_same(pmde, pmdp_get_lockless(pvmw->pmd)))) {
                pte_unmap_unlock(pvmw->pte, ptl);
                pvmw->pte = NULL;
                goto restart;
            }
            pvmw->ptl = ptl;
        }
        goto this_pte;
    } while (pvmw->address < end);

    return false;
}
```

#### map_pte

```c
bool map_pte(struct page_vma_mapped_walk *pvmw, pmd_t *pmdvalp,
            spinlock_t **ptlp)
{
    pte_t ptent;

    if (pvmw->flags & PVMW_SYNC) {
        /* Use the stricter lookup */
        pvmw->pte = pte_offset_map_lock(pvmw->vma->vm_mm, pvmw->pmd,
                        pvmw->address, &pvmw->ptl);
        *ptlp = pvmw->ptl;
        return !!pvmw->pte;
    }

again:
    /* It is important to return the ptl corresponding to pte,
    * in case *pvmw->pmd changes underneath us; so we need to
    * return it even when choosing not to lock, in case caller
    * proceeds to loop over next ptes, and finds a match later.
    * Though, in most cases, page lock already protects this. */
    pvmw->pte = pte_offset_map_rw_nolock(
        pvmw->vma->vm_mm, pvmw->pmd, pvmw->address, pmdvalp, ptlp);
    if (!pvmw->pte)
        return false;

    ptent = ptep_get(pvmw->pte);

    if (pvmw->flags & PVMW_MIGRATION) {
        if (!is_swap_pte(ptent))
            return false;
    } else if (is_swap_pte(ptent)) {
        swp_entry_t entry;
        /* Handle un-addressable ZONE_DEVICE memory.
        *
        * We get here when we are trying to unmap a private
        * device page from the process address space. Such
        * page is not CPU accessible and thus is mapped as
        * a special swap entry, nonetheless it still does
        * count as a valid regular mapping for the page
        * (and is accounted as such in page maps count).
        *
        * So handle this special case as if it was a normal
        * page mapping ie lock CPU page table and return true.
        *
        * For more details on device private memory see HMM
        * (include/linux/hmm.h or mm/hmm.c). */
        entry = pte_to_swp_entry(ptent);
        if (!is_device_private_entry(entry) && !is_device_exclusive_entry(entry))
            return false;
    } else if (!pte_present(ptent)) {
        return false;
    }
    spin_lock(*ptlp);
    if (unlikely(!pmd_same(*pmdvalp, pmdp_get_lockless(pvmw->pmd)))) {
        pte_unmap_unlock(pvmw->pte, *ptlp);
        goto again;
    }
    pvmw->ptl = *ptlp;

    return true;
}
```

## try_to_unmap

![](../images/kernel/mem-rmap-try_to_unmap.png)

```c
try_to_unmap(struct folio *folio, enum ttu_flags flags) {
    struct rmap_walk_control rwc = {
        .rmap_one = try_to_unmap_one,
        .arg = (void *)flags,
        .done = folio_not_mapped,
        .anon_lock = folio_lock_anon_vma_read,
    };

    rmap_walk_locked(folio, &rwc) {
        if (folio_test_anon(folio)) {
            rmap_walk_anon(folio, rwc, true) {
                pgoff_start = folio_pgoff(folio);
                pgoff_end = pgoff_start + folio_nr_pages(folio) - 1;
                anon_vma_interval_tree_foreach(avc, &anon_vma->rb_root, pgoff_start, pgoff_end) {
                    struct vm_area_struct *vma = avc->vma;
                    unsigned long address = vma_address(&folio->page, vma);

                    rwc->invalid_vma(vma, rwc->arg);

                    ret = rwc->rmap_one(folio, vma, address, rwc->arg) {
                        try_to_unmap_one();
                            --->
                    }

                    rwc->done(folio) {
                        folio_not_mapped();
                    }
                }
            }
        } else {
            rmap_walk_file(folio, rwc, true) {
                struct address_space *mapping = folio_mapping(folio);
                vma_interval_tree_foreach(vma, &mapping->i_mmap, pgoff_start, pgoff_end) {
                    unsigned long address = vma_address(&folio->page, vma);

                    if (rwc->invalid_vma && rwc->invalid_vma(vma, rwc->arg))
                        continue;

                    if (!rwc->rmap_one(folio, vma, address, rwc->arg))
                        goto done;
                    if (rwc->done && rwc->done(folio))
                        goto done;
                }
            }
        }
    }
}
```

```c
bool try_to_unmap_one(struct folio *folio, struct vm_area_struct *vma,
            unsigned long address, void *arg)
{
    struct mm_struct *mm = vma->vm_mm;
    DEFINE_FOLIO_VMA_WALK(pvmw, folio, vma, address, 0);
    pte_t pteval;
    struct page *subpage;
    bool anon_exclusive, ret = true;
    struct mmu_notifier_range range;
    enum ttu_flags flags = (enum ttu_flags)(long)arg;
    unsigned long pfn;
    unsigned long hsz = 0;

    if (flags & TTU_SYNC)
        pvmw.flags = PVMW_SYNC;

    range.end = vma_address_end(&pvmw);
    mmu_notifier_range_init(&range, MMU_NOTIFY_CLEAR, 0, vma->vm_mm,
                address, range.end);
    if (folio_test_hugetlb(folio)) {
        adjust_range_if_pmd_sharing_possible(vma, &range.start, &range.end);
        hsz = huge_page_size(hstate_vma(vma));
    }
    mmu_notifier_invalidate_range_start(&range);

    /* check if @pvmw->pfn is mapped in @pvmw->vma at @pvmw->address */
    while (page_vma_mapped_walk(&pvmw)) {
        if (!(flags & TTU_IGNORE_MLOCK) && (vma->vm_flags & VM_LOCKED)) {
            /* Restore the mlock which got missed */
            if (!folio_test_large(folio))
                mlock_vma_folio(folio, vma);
            goto walk_abort;
        }

        if (!pvmw.pte) {
            if (folio_test_anon(folio) && !folio_test_swapbacked(folio)) {
                if (unmap_huge_pmd_locked(vma, pvmw.address, pvmw.pmd, folio))
                    goto walk_done;
                goto walk_abort;
            }

            if (flags & TTU_SPLIT_HUGE_PMD) {
                split_huge_pmd_locked(vma, pvmw.address, pvmw.pmd, false);
                flags &= ~TTU_SPLIT_HUGE_PMD;
                page_vma_mapped_walk_restart(&pvmw);
                continue;
            }
        }

        /* Unexpected PMD-mapped THP? */
        VM_BUG_ON_FOLIO(!pvmw.pte, folio);

        /* Handle PFN swap PTEs, such as device-exclusive ones, that
         * actually map pages. */
        pteval = ptep_get(pvmw.pte);
        if (likely(pte_present(pteval))) {
            pfn = pte_pfn(pteval);
        } else {
            pfn = swp_offset_pfn(pte_to_swp_entry(pteval));
            VM_WARN_ON_FOLIO(folio_test_hugetlb(folio), folio);
        }

        subpage = folio_page(folio, pfn - folio_pfn(folio));
        address = pvmw.address;
        anon_exclusive = folio_test_anon(folio) && PageAnonExclusive(subpage);

        if (folio_test_hugetlb(folio)) {
            bool anon = folio_test_anon(folio);

            flush_cache_range(vma, range.start, range.end);

            if (!anon) {
                VM_BUG_ON(!(flags & TTU_RMAP_LOCKED));
                if (!hugetlb_vma_trylock_write(vma))
                    goto walk_abort;

                /* unmap huge page backed by shared pte. */
                if (huge_pmd_unshare(mm, vma, address, pvmw.pte)) {
                    hugetlb_vma_unlock_write(vma);
                    flush_tlb_range(vma, range.start, range.end);
                    goto walk_done;
                }
                hugetlb_vma_unlock_write(vma);
            }
            pteval = huge_ptep_clear_flush(vma, address, pvmw.pte);
            if (pte_dirty(pteval))
                folio_mark_dirty(folio);
        } else if (likely(pte_present(pteval))) {
            nr_pages = folio_unmap_pte_batch(folio, &pvmw, flags, pteval);
            end_addr = address + nr_pages * PAGE_SIZE;
            flush_cache_range(vma, address, end_addr);

            /* Nuke the page table entry. */
            pteval = get_and_clear_ptes(mm, address, pvmw.pte, nr_pages) {
                return get_and_clear_full_ptes(mm, addr, ptep, nr, 0) {
                    pte_t pte;

                    if (likely(nr == 1)) {
                        contpte_try_unfold(mm, addr, ptep, __ptep_get(ptep));
                        pte = __get_and_clear_full_ptes(mm, addr, ptep, nr, full);
                    } else {
                        pte = contpte_get_and_clear_full_ptes(mm, addr, ptep, nr, full) {
                            contpte_try_unfold_partial(mm, addr, ptep, nr) {
                                if (ptep != contpte_align_down(ptep) || nr < CONT_PTES)
                                    contpte_try_unfold(mm, addr, ptep, __ptep_get(ptep));

                                if (ptep + nr != contpte_align_down(ptep + nr)) {
                                    unsigned long last_addr = addr + PAGE_SIZE * (nr - 1);
                                    pte_t *last_ptep = ptep + nr - 1;

                                    contpte_try_unfold(mm, last_addr, last_ptep, __ptep_get(last_ptep));
                                }
                            }

                            return __get_and_clear_full_ptes(mm, addr, ptep, nr, full) {
                                pte_t pte, tmp_pte;

                                pte = __ptep_get_and_clear(mm, addr, ptep);
                                while (--nr) {
                                    ptep++;
                                    addr += PAGE_SIZE;
                                    tmp_pte = __ptep_get_and_clear(mm, addr, ptep);
                                    if (pte_dirty(tmp_pte))
                                        pte = pte_mkdirty(pte);
                                    if (pte_young(tmp_pte))
                                        pte = pte_mkyoung(pte);
                                }
                                return pte;
                            }
                        }
                    }

                    return pte;
                }
            }
            /* We clear the PTE but do not flush so potentially
             * a remote CPU could still be writing to the folio.
             * If the entry was previously clean then the
             * architecture must guarantee that a clear->dirty
             * transition on a cached TLB entry is written through
             * and traps if the PTE is unmapped. */
            if (should_defer_flush(mm, flags))
                set_tlb_ubc_flush_pending(mm, pteval, address, end_addr);
            else
                flush_tlb_range(vma, address, end_addr);
            if (pte_dirty(pteval))
                folio_mark_dirty(folio);
        } else {
            pte_clear(mm, address, pvmw.pte);
        }

        /* Now the pte is cleared. If this pte was uffd-wp armed,
         * we may want to replace a none pte with a marker pte if
         * it's file-backed, so we don't lose the tracking info. */
        pte_install_uffd_wp_if_needed(vma, address, pvmw.pte, pteval);

        /* Update high watermark before we lower rss */
        update_hiwater_rss(mm);

        if (folio_test_anon(folio)) {
            swp_entry_t entry = page_swap_entry(subpage) {
                struct folio *folio = page_folio(page);
                swp_entry_t entry = folio->swap;

                entry.val += folio_page_idx(folio, page) {
                    return ((p) - &(folio)->page);
                }
                return entry;
            }
            pte_t swp_pte;

            /* MADV_FREE page check */
            if (!folio_test_swapbacked(folio)) {
                int ref_count, map_count;
                smp_mb();

                ref_count = folio_ref_count(folio);
                map_count = folio_mapcount(folio);

                smp_rmb();

                if (ref_count == 1 + map_count && !folio_test_dirty(folio)) {
                    dec_mm_counter(mm, MM_ANONPAGES);
                    goto discard;
                }

                set_pte_at(mm, address, pvmw.pte, pteval);
                folio_set_swapbacked(folio);
                ret = false;
                page_vma_mapped_walk_done(&pvmw);
                break;
            }

            /* Increase reference count of swap entry by 1 */
            if (swap_duplicate(entry) < 0) {
                set_pte_at(mm, address, pvmw.pte, pteval);
                ret = false;
                page_vma_mapped_walk_done(&pvmw);
                break;
            }
            if (arch_unmap_one(mm, vma, address, pteval) < 0) {
                swap_free(entry);
                set_pte_at(mm, address, pvmw.pte, pteval);
                ret = false;
                page_vma_mapped_walk_done(&pvmw);
                break;
            }

            /* See folio_try_share_anon_rmap(): clear PTE first. */
            if (anon_exclusive && folio_try_share_anon_rmap_pte(folio, subpage)) {
                swap_free(entry);
                set_pte_at(mm, address, pvmw.pte, pteval);
                ret = false;
                page_vma_mapped_walk_done(&pvmw);
                break;
            }
            if (list_empty(&mm->mmlist)) {
                spin_lock(&mmlist_lock);
                if (list_empty(&mm->mmlist))
                    list_add(&mm->mmlist, &init_mm.mmlist);
                spin_unlock(&mmlist_lock);
            }
            dec_mm_counter(mm, MM_ANONPAGES);
            inc_mm_counter(mm, MM_SWAPENTS);

            swp_pte = swp_entry_to_pte(entry);
            if (anon_exclusive)
                swp_pte = pte_swp_mkexclusive(swp_pte);
            if (pte_soft_dirty(pteval))
                swp_pte = pte_swp_mksoft_dirty(swp_pte);
            if (pte_uffd_wp(pteval))
                swp_pte = pte_swp_mkuffd_wp(swp_pte);
            set_pte_at(mm, address, pvmw.pte, swp_pte);
        } else {
            dec_mm_counter(mm, mm_counter_file(&folio->page));
        }
discard:
        if (unlikely(folio_test_hugetlb(folio)))
            hugetlb_remove_rmap(folio);
        else {
            folio_remove_rmap_pte(folio, subpage, vma) {
                atomic_add_negative(-1, &page->_mapcount);
            }
        }
        if (vma->vm_flags & VM_LOCKED)
            mlock_drain_local();
        folio_put(folio);
    }

    mmu_notifier_invalidate_range_end(&range);

    return ret;
}
```

### folio_unmap_pte_batch

```c
static inline unsigned int folio_unmap_pte_batch(struct folio *folio,
            struct page_vma_mapped_walk *pvmw,
            enum ttu_flags flags, pte_t pte)
{
    unsigned long end_addr, addr = pvmw->address;
    struct vm_area_struct *vma = pvmw->vma;
    unsigned int max_nr;

    if (flags & TTU_HWPOISON)
        return 1;
    if (!folio_test_large(folio))
        return 1;

    /* We may only batch within a single VMA and a single page table. */
    end_addr = pmd_addr_end(addr, vma->vm_end);
    max_nr = (end_addr - addr) >> PAGE_SHIFT;

    /* We only support lazyfree batching for now ... */
    if (!folio_test_anon(folio) || folio_test_swapbacked(folio))
        return 1;
    if (pte_unused(pte))
        return 1;

    return folio_pte_batch(folio, pvmw->pte, pte, max_nr) {
       /* Detect a PTE batch: consecutive (present) PTEs that map consecutive
        * pages of the same large folio in a single VMA and a single page table. */
        return folio_pte_batch_flags(folio, NULL, ptep, &pte, max_nr, 0) {
            bool any_writable = false, any_young = false, any_dirty = false;
            pte_t expected_pte, pte = *ptentp;
            unsigned int nr, cur_nr;

            /* calc the number of pages remaining in the folio from the current PTE’s PFN to the end of the folio. */
            max_nr = min_t(unsigned long, max_nr,
               folio_pfn(folio) + folio_nr_pages(folio) - pte_pfn(pte));

            nr = pte_batch_hint(ptep, pte) {
                if (!pte_valid_cont(pte))
                    return 1;
                /* ptep >> 3: Divides the address by 8 (since PTEs are typically 8 bytes each).
                 *      This gives the index of the PTE within the page table.
                 * &(CONT_PTES - 1): Masks the index to find the offset within a contiguous PTE group. */
                return CONT_PTES - (((unsigned long)ptep >> 3) & (CONT_PTES - 1));
            }

            expected_pte = __pte_batch_clear_ignored(pte_advance_pfn(pte, nr), flags) {
                if (!(flags & FPB_RESPECT_DIRTY))
                    pte = pte_mkclean(pte);
                if (likely(!(flags & FPB_RESPECT_SOFT_DIRTY)))
                    pte = pte_clear_soft_dirty(pte);
                if (likely(!(flags & FPB_RESPECT_WRITE)))
                    pte = pte_wrprotect(pte);
                return pte_mkold(pte);
            }
            ptep = ptep + nr;

            while (nr < max_nr) {
                pte = ptep_get(ptep);

                if (!pte_same(__pte_batch_clear_ignored(pte, flags), expected_pte))
                    break;

                if (flags & FPB_MERGE_WRITE)
                    any_writable |= pte_write(pte);
                if (flags & FPB_MERGE_YOUNG_DIRTY) {
                    any_young |= pte_young(pte);
                    any_dirty |= pte_dirty(pte);
                }

                cur_nr = pte_batch_hint(ptep, pte);
                expected_pte = pte_advance_pfn(expected_pte, cur_nr) {
                    return pfn_pte(pte_pfn(pte) + nr, pte_pgprot(pte));
                }
                ptep += cur_nr;
                nr += cur_nr;
            }

            if (any_writable)
                *ptentp = pte_mkwrite(*ptentp, vma);
            if (any_young)
                *ptentp = pte_mkyoung(*ptentp);
            if (any_dirty)
                *ptentp = pte_mkdirty(*ptentp);

            return min(nr, max_nr);
        }
    }
}
```

### unmap_huge_pmd_locked

```c
bool unmap_huge_pmd_locked(struct vm_area_struct *vma, unsigned long addr,
            pmd_t *pmdp, struct folio *folio)
{
    VM_WARN_ON_FOLIO(!folio_test_pmd_mappable(folio), folio);
    VM_WARN_ON_FOLIO(!folio_test_locked(folio), folio);
    VM_WARN_ON_FOLIO(!folio_test_anon(folio), folio);
    VM_WARN_ON_FOLIO(folio_test_swapbacked(folio), folio);
    VM_WARN_ON_ONCE(!IS_ALIGNED(addr, HPAGE_PMD_SIZE));

    return __discard_anon_folio_pmd_locked(vma, addr, pmdp, folio) {
        struct mm_struct *mm = vma->vm_mm;
        int ref_count, map_count;
        pmd_t orig_pmd = *pmdp;

        if (pmd_dirty(orig_pmd))
            folio_set_dirty(folio);
        if (folio_test_dirty(folio) && !(vma->vm_flags & VM_DROPPABLE)) {
            folio_set_swapbacked(folio);
            return false;
        }

        orig_pmd = pmdp_huge_clear_flush(vma, addr, pmdp);

        /* Syncing against concurrent GUP-fast:
        * - clear PMD; barrier; read refcount
        * - inc refcount; barrier; read PMD */
        smp_mb();

        ref_count = folio_ref_count(folio);
        map_count = folio_mapcount(folio);

        /* Order reads for folio refcount and dirty flag
        * (see comments in __remove_mapping()). */
        smp_rmb();

        /* If the folio or its PMD is redirtied at this point, or if there
        * are unexpected references, we will give up to discard this folio
        * and remap it.
        *
        * The only folio refs must be one from isolation plus the rmap(s). */
        if (pmd_dirty(orig_pmd))
            folio_set_dirty(folio);
        if (folio_test_dirty(folio) && !(vma->vm_flags & VM_DROPPABLE)) {
            folio_set_swapbacked(folio);
            set_pmd_at(mm, addr, pmdp, orig_pmd);
            return false;
        }

        if (ref_count != map_count + 1) {
            set_pmd_at(mm, addr, pmdp, orig_pmd);
            return false;
        }

        folio_remove_rmap_pmd(folio, pmd_page(orig_pmd), vma);
        zap_deposited_table(mm, pmdp) {
            pgtable_t pgtable;

            pgtable = pgtable_trans_huge_withdraw(mm, pmd) {
                pgtable_t pgtable;
                /* FIFO */
                pgtable = pmd_huge_pte(mm, pmdp);
                pmd_huge_pte(mm, pmdp) = list_first_entry_or_null(&pgtable->lru, struct page, lru);
                ret = pmd_huge_pte(mm, pmdp) {
                    return (pmd_ptdesc(pmd) {
                        ret = pmd_pgtable_page(pmd) {
                            unsigned long mask = ~(PTRS_PER_PMD * sizeof(pmd_t) - 1);
                            return virt_to_page((void *)((unsigned long) pmd & mask));
                        }
                        return page_ptdesc(ret) {
                            #define page_ptdesc(p) (_Generic((p), \
                            const struct page *: (const struct ptdesc *)(p), \
                            struct page *: (struct ptdesc *)(p)))
                        };
                    } ->pmd_huge_pte)
                }
                if (ret)
                    list_del(&pgtable->lru);
                return pgtable;
            }
            pte_free(mm, pgtable) {
                struct ptdesc *ptdesc = page_ptdesc(pte_page);
                pagetable_dtor_free(ptdesc);
            }
            mm_dec_nr_ptes(mm);
        }
        add_mm_counter(mm, MM_ANONPAGES, -HPAGE_PMD_NR);
        if (vma->vm_flags & VM_LOCKED)
            mlock_drain_local();
        folio_put(folio);

        return true;
    }
}
```

### split_huge_pmd_locked

```c
void split_huge_pmd_locked(struct vm_area_struct *vma, unsigned long address,
               pmd_t *pmd, bool freeze)
{
    VM_WARN_ON_ONCE(!IS_ALIGNED(address, HPAGE_PMD_SIZE));
    if (pmd_trans_huge(*pmd) || is_pmd_migration_entry(*pmd))
        __split_huge_pmd_locked(vma, pmd, address, freeze);
}

static void __split_huge_pmd_locked(struct vm_area_struct *vma, pmd_t *pmd,
        unsigned long haddr, bool freeze)
{
    struct mm_struct *mm = vma->vm_mm;
    struct folio *folio;
    struct page *page;
    pgtable_t pgtable;
    pmd_t old_pmd, _pmd;
    bool young, write, soft_dirty, pmd_migration = false, uffd_wp = false;
    bool anon_exclusive = false, dirty = false;
    unsigned long addr;
    pte_t *pte;
    int i;

    VM_BUG_ON(haddr & ~HPAGE_PMD_MASK);
    VM_BUG_ON_VMA(vma->vm_start > haddr, vma);
    VM_BUG_ON_VMA(vma->vm_end < haddr + HPAGE_PMD_SIZE, vma);
    VM_BUG_ON(!is_pmd_migration_entry(*pmd) && !pmd_trans_huge(*pmd));

    count_vm_event(THP_SPLIT_PMD);

    if (!vma_is_anonymous(vma)) {
        old_pmd = pmdp_huge_clear_flush(vma, haddr, pmd);
        /* We are going to unmap this huge page. So
         * just go ahead and zap it */
        if (arch_needs_pgtable_deposit())
            zap_deposited_table(mm, pmd);
        if (!vma_is_dax(vma) && vma_is_special_huge(vma))
            return;
        if (unlikely(is_pmd_migration_entry(old_pmd))) {
            swp_entry_t entry;

            entry = pmd_to_swp_entry(old_pmd);
            folio = pfn_swap_entry_folio(entry);
        } else if (is_huge_zero_pmd(old_pmd)) {
            return;
        } else {
            page = pmd_page(old_pmd);
            folio = page_folio(page);
            if (!folio_test_dirty(folio) && pmd_dirty(old_pmd))
                folio_mark_dirty(folio);
            if (!folio_test_referenced(folio) && pmd_young(old_pmd))
                folio_set_referenced(folio);
            folio_remove_rmap_pmd(folio, page, vma);
            folio_put(folio);
        }
        add_mm_counter(mm, mm_counter_file(folio), -HPAGE_PMD_NR);
        return;
    }

    if (is_huge_zero_pmd(*pmd)) {
        /* FIXME: Do we want to invalidate secondary mmu by calling
         * mmu_notifier_arch_invalidate_secondary_tlbs() see comments below
         * inside __split_huge_pmd() ?
         *
         * We are going from a zero huge page write protected to zero
         * small page also write protected so it does not seems useful
         * to invalidate secondary mmu at this time. */
        return __split_huge_zero_page_pmd(vma, haddr, pmd);
    }

    pmd_migration = is_pmd_migration_entry(*pmd);
    if (unlikely(pmd_migration)) {
        swp_entry_t entry;

        old_pmd = *pmd;
        entry = pmd_to_swp_entry(old_pmd);
        page = pfn_swap_entry_to_page(entry);
        write = is_writable_migration_entry(entry);
        if (PageAnon(page))
            anon_exclusive = is_readable_exclusive_migration_entry(entry);
        young = is_migration_entry_young(entry);
        dirty = is_migration_entry_dirty(entry);
        soft_dirty = pmd_swp_soft_dirty(old_pmd);
        uffd_wp = pmd_swp_uffd_wp(old_pmd);
    } else {
        old_pmd = pmdp_invalidate(vma, haddr, pmd);
        page = pmd_page(old_pmd);
        folio = page_folio(page);
        if (pmd_dirty(old_pmd)) {
            dirty = true;
            folio_set_dirty(folio);
        }
        write = pmd_write(old_pmd);
        young = pmd_young(old_pmd);
        soft_dirty = pmd_soft_dirty(old_pmd);
        uffd_wp = pmd_uffd_wp(old_pmd);

        VM_WARN_ON_FOLIO(!folio_ref_count(folio), folio);
        VM_WARN_ON_FOLIO(!folio_test_anon(folio), folio);

        /* Without "freeze", we'll simply split the PMD, propagating the
         * PageAnonExclusive() flag for each PTE by setting it for
         * each subpage -- no need to (temporarily) clear.
         *
         * With "freeze" we want to replace mapped pages by
         * migration entries right away. This is only possible if we
         * managed to clear PageAnonExclusive() -- see
         * set_pmd_migration_entry().
         *
         * In case we cannot clear PageAnonExclusive(), split the PMD
         * only and let try_to_migrate_one() fail later.
         *
         * See folio_try_share_anon_rmap_pmd(): invalidate PMD first. */
        anon_exclusive = PageAnonExclusive(page);
        if (freeze && anon_exclusive &&
            folio_try_share_anon_rmap_pmd(folio, page))
            freeze = false;
        if (!freeze) {
            rmap_t rmap_flags = RMAP_NONE;

            folio_ref_add(folio, HPAGE_PMD_NR - 1);
            if (anon_exclusive)
                rmap_flags |= RMAP_EXCLUSIVE;
            folio_add_anon_rmap_ptes(folio, page, HPAGE_PMD_NR,
                         vma, haddr, rmap_flags);
        }
    }

    /* Withdraw the table only after we mark the pmd entry invalid.
     * This's critical for some architectures (Power). */
    pgtable = pgtable_trans_huge_withdraw(mm, pmd);
    pmd_populate(mm, &_pmd, pgtable);

    pte = pte_offset_map(&_pmd, haddr);
    VM_BUG_ON(!pte);

    /* Note that NUMA hinting access restrictions are not transferred to
     * avoid any possibility of altering permissions across VMAs. */
    if (freeze || pmd_migration) {
        for (i = 0, addr = haddr; i < HPAGE_PMD_NR; i++, addr += PAGE_SIZE) {
            pte_t entry;
            swp_entry_t swp_entry;

            if (write)
                swp_entry = make_writable_migration_entry( page_to_pfn(page + i));
            else if (anon_exclusive)
                swp_entry = make_readable_exclusive_migration_entry( page_to_pfn(page + i));
            else
                swp_entry = make_readable_migration_entry(page_to_pfn(page + i));

            if (young)
                swp_entry = make_migration_entry_young(swp_entry);
            if (dirty)
                swp_entry = make_migration_entry_dirty(swp_entry);
            entry = swp_entry_to_pte(swp_entry);
            if (soft_dirty)
                entry = pte_swp_mksoft_dirty(entry);
            if (uffd_wp)
                entry = pte_swp_mkuffd_wp(entry);

            VM_WARN_ON(!pte_none(ptep_get(pte + i)));
            set_pte_at(mm, addr, pte + i, entry);
        }
    } else {
        pte_t entry;

        entry = mk_pte(page, READ_ONCE(vma->vm_page_prot));
        if (write)
            entry = pte_mkwrite(entry, vma);
        if (!young)
            entry = pte_mkold(entry);
        /* NOTE: this may set soft-dirty too on some archs */
        if (dirty)
            entry = pte_mkdirty(entry);
        if (soft_dirty)
            entry = pte_mksoft_dirty(entry);
        if (uffd_wp)
            entry = pte_mkuffd_wp(entry);

        for (i = 0; i < HPAGE_PMD_NR; i++)
            VM_WARN_ON(!pte_none(ptep_get(pte + i)));

        set_ptes(mm, haddr, pte, entry, HPAGE_PMD_NR);
    }
    pte_unmap(pte);

    if (!pmd_migration)
        folio_remove_rmap_pmd(folio, page, vma);
    if (freeze)
        put_page(page);

    smp_wmb(); /* make pte visible before pmd */
    pmd_populate(mm, pmd, pgtable);
}
```


# kernel mapping
```c
/* arch/x86/include/asm/pgtable_64.h */
extern pud_t level3_kernel_pgt[512];
extern pud_t level3_ident_pgt[512];

extern pmd_t level2_kernel_pgt[512];
extern pmd_t level2_fixmap_pgt[512];
extern pmd_t level2_ident_pgt[512];

extern pte_t level1_fixmap_pgt[512];
extern pgd_t init_top_pgt[];

#define swapper_pg_dir init_top_pgt

/* arch\x86\kernel\head_64.S */
__INITDATA
NEXT_PAGE(init_top_pgt)
  .quad   level3_ident_pgt - __START_KERNEL_map + _KERNPG_TABLE
  .org    init_top_pgt + PGD_PAGE_OFFSET*8, 0
  .quad   level3_ident_pgt - __START_KERNEL_map + _KERNPG_TABLE
  .org    init_top_pgt + PGD_START_KERNEL*8, 0
  /* (2^48-(2*1024*1024*1024))/(2^39) = 511 */
  .quad   level3_kernel_pgt - __START_KERNEL_map + _PAGE_TABLE

NEXT_PAGE(level3_ident_pgt)
  .quad  level2_ident_pgt - __START_KERNEL_map + _KERNPG_TABLE
  .fill  511, 8, 0
NEXT_PAGE(level2_ident_pgt)
  /* Since I easily can, map the first 1G.
   * Don't set NX because code runs from these pages. */
  PMDS(0, __PAGE_KERNEL_IDENT_LARGE_EXEC, PTRS_PER_PMD)


NEXT_PAGE(level3_kernel_pgt)
  .fill  L3_START_KERNEL,8,0
  /* (2^48-(2*1024*1024*1024)-((2^39)*511))/(2^30) = 510 */
  .quad  level2_kernel_pgt - __START_KERNEL_map + _KERNPG_TABLE
  .quad  level2_fixmap_pgt - __START_KERNEL_map + _PAGE_TABLE


NEXT_PAGE(level2_kernel_pgt)
  /* 512 MB kernel mapping. We spend a full page on this pagetable
   * anyway.
   *
   * The kernel code+data+bss must not be bigger than that.
   *
   * (NOTE: at +512MB starts the module area, see MODULES_VADDR.
   *  If you want to increase this then increase MODULES_VADDR
   *  too.) */
  PMDS(0, __PAGE_KERNEL_LARGE_EXEC,
    KERNEL_IMAGE_SIZE/PMD_SIZE)


NEXT_PAGE(level2_fixmap_pgt)
  .fill  506,8,0
  .quad  level1_fixmap_pgt - __START_KERNEL_map + _PAGE_TABLE
  /* 8MB reserved for vsyscalls + a 2MB hole = 4 + 1 entries */
  .fill  5,8,0


NEXT_PAGE(level1_fixmap_pgt)
  .fill  51


PGD_PAGE_OFFSET = pgd_index(__PAGE_OFFSET_BASE)
PGD_START_KERNEL = pgd_index(__START_KERNEL_map)
L3_START_KERNEL = pud_index(__START_KERNEL_map)
```
![](../images/kernel/mem-kernel-page-table.png)

```c
/* kernel mm_struct */
struct mm_struct init_mm = {
    .mm_rb      = RB_ROOT,
    .pgd        = swapper_pg_dir,
    .mm_users   = ATOMIC_INIT(2),
    .mm_count   = ATOMIC_INIT(1),
    .mmap_sem   = __RWSEM_INITIALIZER(init_mm.mmap_sem),
    .page_table_lock =  __SPIN_LOCK_UNLOCKED(init_mm.page_table_lock),
    .mmlist     = LIST_HEAD_INIT(init_mm.mmlist),
    .user_ns    = &init_user_ns,
    INIT_MM_CONTEXT(init_mm)
};

/* init kernel mm_struct */
void __init setup_arch(char **cmdline_p)
{
  clone_pgd_range(swapper_pg_dir + KERNEL_PGD_BOUNDARY,
      initial_page_table + KERNEL_PGD_BOUNDARY,
      KERNEL_PGD_PTRS);

  load_cr3(swapper_pg_dir);
  __flush_tlb_all();

  init_mm.start_code = (unsigned long) _text;
  init_mm.end_code = (unsigned long) _etext;
  init_mm.end_data = (unsigned long) _edata;
  init_mm.brk = _brk_end;
  init_mem_mapping();
}

/* init_mem_mapping -> */
unsigned long kernel_physical_mapping_init(
  unsigned long paddr_start,
  unsigned long paddr_end,
  unsigned long page_size_mask)
{
  unsigned long vaddr, vaddr_start, vaddr_end, vaddr_next, paddr_last;

  paddr_last = paddr_end;
  vaddr = (unsigned long)__va(paddr_start);
  vaddr_end = (unsigned long)__va(paddr_end);
  vaddr_start = vaddr;

  for (; vaddr < vaddr_end; vaddr = vaddr_next) {
    pgd_t *pgd = pgd_offset_k(vaddr);
    p4d_t *p4d;

    vaddr_next = (vaddr & PGDIR_MASK) + PGDIR_SIZE;

    if (pgd_val(*pgd)) {
      p4d = (p4d_t *)pgd_page_vaddr(*pgd);
      paddr_last = phys_p4d_init(p4d, __pa(vaddr),
               __pa(vaddr_end),
               page_size_mask);
      continue;
    }

    p4d = alloc_low_page();
    paddr_last = phys_p4d_init(p4d, __pa(vaddr), __pa(vaddr_end),
             page_size_mask);

    p4d_populate(&init_mm, p4d_offset(pgd, vaddr), (pud_t *) p4d);
  }
  __flush_tlb_all();

  return paddr_last;
}
```

# fork

```c
kernel_clone()
    copy_mm() {
        if (clone_flags & CLONE_VM) {
            mmget(oldmm);
            mm = oldmm;
        } else {
            mm = dup_mm(tsk, current->mm) {
                mm = allocate_mm() {
                    kmem_cache_alloc(mm_cachep, GFP_KERNEL);
                }
                memcpy(mm, oldmm, sizeof(*mm));

                mm_init(mm, tsk, mm->user_ns)
                dup_mmap(mm, oldmm) {
                    dup_mm_exe_file(mm, oldmm) {
                        struct file *exe_file = get_mm_exe_file(oldmm);
                        RCU_INIT_POINTER(mm->exe_file, exe_file);
                        if (exe_file) {
                            deny_write_access(exe_file) {
                                struct inode *inode = file_inode(file);
                                return atomic_dec_unless_positive(&inode->i_writecount) ? 0 : -ETXTBSY;
                            }
                        }
                    }

                    mm->total_vm = oldmm->total_vm;
                    mm->data_vm = oldmm->data_vm;
                    mm->exec_vm = oldmm->exec_vm;
                    mm->stack_vm = oldmm->stack_vm;

                    __mt_dup(&oldmm->mm_mt, &mm->mm_mt, GFP_KERNEL);

                    for_each_vma(vmi, mpnt) {
                        struct file *file;

                        if (mpnt->vm_flags & VM_DONTCOPY) {
                            vm_stat_account(mm, mpnt->vm_flags, -vma_pages(mpnt));
                            continue;
                        }
                        charge = 0;
                        /* Don't duplicate many vmas if we've been oom-killed (for example) */
                        if (fatal_signal_pending(current)) {
                            retval = -EINTR;
                            goto loop_out;
                        }
                        if (mpnt->vm_flags & VM_ACCOUNT) {
                            unsigned long len = vma_pages(mpnt);
                            charge = len;
                        }

                        tmp = vm_area_dup(mpnt) {
                            new = kmem_cache_alloc(vm_area_cachep, GFP_KERNEL);
                            memcpy(new, orig, sizeof(*new));
                        }

                        retval = vma_dup_policy(mpnt, tmp);

                        tmp->vm_mm = mm;
                        retval = dup_userfaultfd(tmp, &uf);

                        if (tmp->vm_flags & VM_WIPEONFORK) {
                            /* VM_WIPEONFORK gets a clean slate in the child.
                             * Don't prepare anon_vma until fault since we don't
                             * copy page for current vma. */
                            tmp->anon_vma = NULL;
                        } else {
                            anon_vma_fork(tmp, mpnt) {
                                --->
                            }
                        }

                        vm_flags_clear(tmp, VM_LOCKED_MASK);
                        file = tmp->vm_file;
                        if (file) {
                            struct address_space *mapping = file->f_mapping;

                            if (vma_is_shared_maywrite(tmp)) {
                                mapping_allow_writable(mapping) {
                                    atomic_inc(&mapping->i_mmap_writable);
                                }
                            }

                            flush_dcache_mmap_lock(mapping);
                            /* insert tmp into the share list, just after mpnt */
                            vma_interval_tree_insert_after(tmp, mpnt, &mapping->i_mmap);
                            flush_dcache_mmap_unlock(mapping);
                        }

                        /* Link the vma into the MT */
                        if (vma_iter_bulk_store(&vmi, tmp))
                            goto fail_nomem_vmi_store;

                        mm->map_count++;
                        if (!(tmp->vm_flags & VM_WIPEONFORK))
                            /* set cow at copy_present_pte */
                            retval = copy_page_range(tmp, mpnt);
                                --->

                        if (tmp->vm_ops && tmp->vm_ops->open)
                            tmp->vm_ops->open(tmp);

                        if (retval)
                            goto loop_out;
                    }
                    /* a new mm has just been created */
                    retval = arch_dup_mmap(oldmm, mm);

                    flush_tlb_mm(oldmm);
                }
            }
        }

        tsk->mm = mm;
        tsk->active_mm = mm;
    }
```

## copy_page_range

```c
int copy_page_range(struct vm_area_struct *dst_vma, struct vm_area_struct *src_vma)
{
    pgd_t *src_pgd, *dst_pgd;
    unsigned long next;
    unsigned long addr = src_vma->vm_start;
    unsigned long end = src_vma->vm_end;
    struct mm_struct *dst_mm = dst_vma->vm_mm;
    struct mm_struct *src_mm = src_vma->vm_mm;
    struct mmu_notifier_range range;
    bool is_cow;
    int ret;

    ret = vma_needs_copy(dst_vma, src_vma) {
        if (userfaultfd_wp(dst_vma))
            return true;
        if (src_vma->vm_flags & (VM_PFNMAP | VM_MIXEDMAP))
            return true;
        if (src_vma->anon_vma)
            return true;
        return false;
    }
    if (!ret)
        return 0;

    if (is_vm_hugetlb_page(src_vma))
        return copy_hugetlb_page_range(dst_mm, src_mm, dst_vma, src_vma);

    if (unlikely(src_vma->vm_flags & VM_PFNMAP)) {
        ret = track_pfn_copy(src_vma);
        if (ret)
            return ret;
    }

    is_cow = is_cow_mapping(src_vma->vm_flags) {
        return (flags & (VM_SHARED | VM_MAYWRITE)) == VM_MAYWRITE;
    }

    if (is_cow) {
        mmu_notifier_range_init(&range, MMU_NOTIFY_PROTECTION_PAGE,
                    0, src_mm, addr, end);
        mmu_notifier_invalidate_range_start(&range);
        /* Disabling preemption is not needed for the write side, as
        * the read side doesn't spin, but goes to the mmap_lock.
        *
        * Use the raw variant of the seqcount_t write API to avoid
        * lockdep complaining about preemptibility. */
        vma_assert_write_locked(src_vma);
        raw_write_seqcount_begin(&src_mm->write_protect_seq);
    }

    ret = 0;
    dst_pgd = pgd_offset(dst_mm, addr);
    src_pgd = pgd_offset(src_mm, addr);
    do {
        next = pgd_addr_end(addr, end);
        if (pgd_none_or_clear_bad(src_pgd))
            continue;
        ret = copy_p4d_range(dst_vma, src_vma, dst_pgd, src_pgd, addr, next) {
            struct mm_struct *dst_mm = dst_vma->vm_mm;
            p4d_t *src_p4d, *dst_p4d;
            unsigned long next;

            dst_p4d = p4d_alloc(dst_mm, dst_pgd, addr);
            if (!dst_p4d)
                return -ENOMEM;

            src_p4d = p4d_offset(src_pgd, addr);
            do {
                next = p4d_addr_end(addr, end);
                if (p4d_none_or_clear_bad(src_p4d))
                    continue;
                ret = copy_pud_range(dst_vma, src_vma, dst_p4d, src_p4d, addr, next) {
                    dst_pud = pud_alloc(dst_mm, dst_p4d, addr);
                    if (!dst_pud)
                        return -ENOMEM;
                    src_pud = pud_offset(src_p4d, addr);
                    do {
                        next = pud_addr_end(addr, end);
                        if (pud_none_or_clear_bad(src_pud))
                            continue;

                        ret = copy_pmd_range(dst_vma, src_vma, dst_pud, src_pud, addr, next) {
                            dst_pmd = pmd_alloc(dst_mm, dst_pud, addr);
                            if (!dst_pmd)
                                return -ENOMEM;
                            src_pmd = pmd_offset(src_pud, addr);
                            do {
                                next = pmd_addr_end(addr, end);
                                if (pmd_none_or_clear_bad(src_pmd))
                                    continue;
                                ret = copy_pte_range(dst_vma, src_vma, dst_pmd, src_pmd, addr, next);
                                    --->
                                if (ret)
                                    return -ENOMEM;
                            } while (dst_pmd++, src_pmd++, addr = next, addr != end);
                            return 0;
                        }
                        if (ret)
                            return -ENOMEM;
                    } while (dst_pud++, src_pud++, addr = next, addr != end);
                    return 0;
                }
                if (ret)
                    return -ENOMEM;
            } while (dst_p4d++, src_p4d++, addr = next, addr != end);
            return 0;
        }
        if (unlikely(ret)) {
            untrack_pfn_clear(dst_vma);
            ret = -ENOMEM;
            break;
        }
    } while (dst_pgd++, src_pgd++, addr = next, addr != end);

    if (is_cow) {
        raw_write_seqcount_end(&src_mm->write_protect_seq);
        mmu_notifier_invalidate_range_end(&range);
    }
    return ret;
}
```

```c
int copy_pte_range(struct vm_area_struct *dst_vma, struct vm_area_struct *src_vma,
        pmd_t *dst_pmd, pmd_t *src_pmd, unsigned long addr,
        unsigned long end)
{
    struct mm_struct *dst_mm = dst_vma->vm_mm;
    struct mm_struct *src_mm = src_vma->vm_mm;
    pte_t *orig_src_pte, *orig_dst_pte;
    pte_t *src_pte, *dst_pte;
    pte_t ptent;
    spinlock_t *src_ptl, *dst_ptl;
    int progress, ret = 0;
    int rss[NR_MM_COUNTERS];
    swp_entry_t entry = (swp_entry_t){0};
    struct folio *prealloc = NULL;

again:
    progress = 0;
    init_rss_vec(rss);

    dst_pte = pte_alloc_map_lock(dst_mm, dst_pmd, addr, &dst_ptl);
    if (!dst_pte) {
        ret = -ENOMEM;
        goto out;
    }

    src_pte = pte_offset_map_nolock(src_mm, src_pmd, addr, &src_ptl);
    if (!src_pte) {
        pte_unmap_unlock(dst_pte, dst_ptl);
        /* ret == 0 */
        goto out;
    }
    spin_lock_nested(src_ptl, SINGLE_DEPTH_NESTING);
    orig_src_pte = src_pte;
    orig_dst_pte = dst_pte;
    arch_enter_lazy_mmu_mode();

    do {
        if (progress >= 32) {
            progress = 0;
            if (need_resched() || spin_needbreak(src_ptl) || spin_needbreak(dst_ptl))
                break;
        }
        ptent = ptep_get(src_pte);
        if (pte_none(ptent)) {
            progress++;
            continue;
        }
        if (unlikely(!pte_present(ptent))) {
            ret = copy_nonpresent_pte(dst_mm, src_mm,
                dst_pte, src_pte, dst_vma, src_vma, addr, rss) {

                swp_entry_t entry = pte_to_swp_entry(orig_pte);

                if (likely(!non_swap_entry(entry))) { /* swp_type(entry) >= MAX_SWAPFILES; */
                    /* inc swap refcnt */
                    if (swap_duplicate(entry) < 0)
                        return -EIO;

                    /* make sure dst_mm is on swapoff's mmlist. */
                    if (unlikely(list_empty(&dst_mm->mmlist))) {
                        spin_lock(&mmlist_lock);
                        if (list_empty(&dst_mm->mmlist))
                            list_add(&dst_mm->mmlist, &src_mm->mmlist);
                        spin_unlock(&mmlist_lock);
                    }
                    /* Mark the swap entry as shared. */
                    if (pte_swp_exclusive(orig_pte)) {
                        pte = pte_swp_clear_exclusive(orig_pte);
                        set_pte_at(src_mm, addr, src_pte, pte);
                    }
                    rss[MM_SWAPENTS]++;
                } else if (is_migration_entry(entry)) { /* swp_type(entry) == SWP_MIGRATION_* */
                    folio = pfn_swap_entry_folio(entry);

                    rss[mm_counter(folio)]++;

                    if (!is_readable_migration_entry(entry) && is_cow_mapping(vm_flags)) {
                        entry = make_readable_migration_entry( swp_offset(entry));
                        pte = swp_entry_to_pte(entry);
                        if (pte_swp_soft_dirty(orig_pte))
                            pte = pte_swp_mksoft_dirty(pte);
                        if (pte_swp_uffd_wp(orig_pte))
                            pte = pte_swp_mkuffd_wp(pte);
                        set_pte_at(src_mm, addr, src_pte, pte);
                    }
                } else if (is_device_private_entry(entry)) {

                } else if (is_device_exclusive_entry(entry)) {

                } else if (is_pte_marker_entry(entry)) {
                    pte_marker marker = copy_pte_marker(entry, dst_vma);

                    if (marker)
                        set_pte_at(dst_mm, addr, dst_pte, make_pte_marker(marker));
                    return 0;
                }
                if (!userfaultfd_wp(dst_vma))
                    pte = pte_swp_clear_uffd_wp(pte);
                set_pte_at(dst_mm, addr, dst_pte, pte);
                return 0;
            }

            if (ret == -EIO) {
                entry = pte_to_swp_entry(ptep_get(src_pte));
                break;
            } else if (ret == -EBUSY) {
                break;
            } else if (!ret) {
                progress += 8;
                continue;
            }

            WARN_ON_ONCE(ret != -ENOENT);
        }

        /* copy_present_pte() will clear `*prealloc' if consumed */
        ret = copy_present_pte(dst_vma, src_vma, dst_pte, src_pte,
            addr, rss, &prealloc) {

            page = vm_normal_page(src_vma, addr, pte);
            if (page)
                folio = page_folio(page);
            if (page && folio_test_anon(folio)) {
                folio_get(folio);
                if (unlikely(folio_try_dup_anon_rmap_pte(folio, page, src_vma))) {
                    /* Page may be pinned, we have to copy. */
                    folio_put(folio);
                    return copy_present_page(dst_vma, src_vma, dst_pte, src_pte,
                        addr, rss, prealloc, page) {

                        copy_user_highpage(&new_folio->page, page, addr, src_vma);
                        __folio_mark_uptodate(new_folio);
                        folio_add_new_anon_rmap(new_folio, dst_vma, addr);
                        folio_add_lru_vma(new_folio, dst_vma);
                        rss[MM_ANONPAGES]++;

                        /* All done, just insert the new page copy in the child */
                        pte = mk_pte(&new_folio->page, dst_vma->vm_page_prot);
                        pte = maybe_mkwrite(pte_mkdirty(pte), dst_vma);
                        if (userfaultfd_pte_wp(dst_vma, ptep_get(src_pte)))
                            pte = pte_mkuffd_wp(pte);
                        set_pte_at(dst_vma->vm_mm, addr, dst_pte, pte);
                        return 0;
                    }
                }
                rss[MM_ANONPAGES]++;
            } else if (page) {
                folio_get(folio);
                folio_dup_file_rmap_pte(folio, page);
                rss[mm_counter_file(page)]++;
            }

            /* If it's a COW mapping, write protect it both
             * in the parent and the child */
            if (is_cow_mapping(vm_flags) && pte_write(pte)) {
                ptep_set_wrprotect(src_mm, addr, src_pte);
                pte = pte_wrprotect(pte) {
                    if (pte_hw_dirty(pte))
                        pte = set_pte_bit(pte, __pgprot(PTE_DIRTY));

                    pte = clear_pte_bit(pte, __pgprot(PTE_WRITE));
                    pte = set_pte_bit(pte, __pgprot(PTE_RDONLY));
                    return pte;
                }
            }
            VM_BUG_ON(page && folio_test_anon(folio) && PageAnonExclusive(page));

            /* If it's a shared mapping, mark it clean in the child */
            if (vm_flags & VM_SHARED)
                pte = pte_mkclean(pte);
            pte = pte_mkold(pte);

            if (!userfaultfd_wp(dst_vma))
                pte = pte_clear_uffd_wp(pte);

            set_pte_at(dst_vma->vm_mm, addr, dst_pte, pte);
            return 0;
        }

        /* If we need a pre-allocated page for this pte, drop the
         * locks, allocate, and try again. */
        if (unlikely(ret == -EAGAIN))
            break;
        if (unlikely(prealloc)) {
            folio_put(prealloc);
            prealloc = NULL;
        }
        progress += 8;
    } while (dst_pte++, src_pte++, addr += PAGE_SIZE, addr != end);

    arch_leave_lazy_mmu_mode();
    pte_unmap_unlock(orig_src_pte, src_ptl);
    add_mm_rss_vec(dst_mm, rss);
    pte_unmap_unlock(orig_dst_pte, dst_ptl);
    cond_resched();

    if (ret == -EIO) {
        VM_WARN_ON_ONCE(!entry.val);
        if (add_swap_count_continuation(entry, GFP_KERNEL) < 0) {
            ret = -ENOMEM;
            goto out;
        }
        entry.val = 0;
    } else if (ret == -EBUSY) {
        goto out;
    } else if (ret ==  -EAGAIN) {
        prealloc = folio_prealloc(src_mm, src_vma, addr, false);
        if (!prealloc)
            return -ENOMEM;
    } else if (ret) {
        VM_WARN_ON_ONCE(1);
    }

    /* We've captured and resolved the error. Reset, try again. */
    ret = 0;

    if (addr != end)
        goto again;
out:
    if (unlikely(prealloc))
        folio_put(prealloc);
    return ret;
}
```

# out_of_memory

* [LWN Index - Out-of-memory handling](https://lwn.net/Kernel/Index/#Memory_management-Out-of-memory_handling)
    * [User-space out-of-memory handling :one:](https://lwn.net/Articles/590960/) ⊙ [:two:](https://lwn.net/Articles/591990/)

* Tuning and Debugging
    * View Scores: Check `/proc/<pid>/oom_score` for each process.
    * Adjust Priority: Write to `/proc/<pid>/oom_score_adj` (e.g., echo -500 > /proc/1234/oom_score_adj to protect PID 1234).
    * Logs: Use `dmesg` or `journalctl` to see OOM events.
    * Disable: Set `vm.oom_kill_allocating_task = 1` to kill the allocating process instead of scoring, or increase memory/swap to avoid OOM entirely.

```c
struct oom_control {
    /* Used to determine cpuset */
    struct zonelist *zonelist;

    /* Used to determine mempolicy */
    nodemask_t *nodemask;

    /* Memory cgroup in which oom is invoked, or NULL for global oom */
    struct mem_cgroup *memcg;

    /* Used to determine cpuset and node locality requirement */
    const gfp_t gfp_mask;

    /* order == -1 means the oom kill is required by sysrq, otherwise only
     * for display purposes. */
    const int order;

    /* Used by oom implementation, do not set */
    unsigned long totalpages;
    struct task_struct *chosen;
    long chosen_points;

    /* Used to print the constraint info. */
    enum oom_constraint constraint;
};

bool out_of_memory(struct oom_control *oc)
{
    unsigned long freed = 0;

    if (oom_killer_disabled)
        return false;

    if (!is_memcg_oom(oc)) {
        blocking_notifier_call_chain(&oom_notify_list, 0, &freed);
        if (freed > 0 && !is_sysrq_oom(oc)) {
            /* Got some memory back in the last second. */
            return true;
        }
    }

    ret = task_will_free_mem(current) {
        struct mm_struct *mm = task->mm;
        struct task_struct *p;
        bool ret = true;

        if (!mm)
            return false;

        ret = __task_will_free_mem(task) {
            struct signal_struct *sig = task->signal;

            if (sig->core_state)
                return false;

            if (sig->flags & SIGNAL_GROUP_EXIT)
                return true;

            if (thread_group_empty(task) && (task->flags & PF_EXITING))
                return true;

            return false;
        }
        if (!ret)
            return false;

        /* This task has already been drained by the oom reaper so there are
        * only small chances it will free some more */
        if (test_bit(MMF_OOM_SKIP, &mm->flags))
            return false;

        if (atomic_read(&mm->mm_users) <= 1)
            return true;

        /* Make sure that all tasks which share the mm with the given tasks
        * are dying as well to make sure that a) nobody pins its mm and
        * b) the task is also reapable by the oom reaper. */
        rcu_read_lock();
        for_each_process(p) {
            if (!process_shares_mm(p, mm))
                continue;
            if (same_thread_group(task, p))
                continue;
            ret = __task_will_free_mem(p);
            if (!ret)
                break;
        }
        rcu_read_unlock();

        return ret;
    }
    if (ret) {
        mark_oom_victim(current) {
            struct mm_struct *mm = tsk->mm;

            if (test_and_set_tsk_thread_flag(tsk, TIF_MEMDIE))
                return;

            /* oom_mm is bound to the signal struct life time. */
            if (!cmpxchg(&tsk->signal->oom_mm, NULL, mm)) {
                mmgrab(tsk->signal->oom_mm);
            }

            __thaw_task(tsk);
            atomic_inc(&oom_victims);
        }

        /* Give the OOM victim time to exit naturally before invoking the oom_reaping. */
        queue_oom_reaper(current) {
            if (test_and_set_bit(MMF_OOM_REAP_QUEUED, &tsk->signal->oom_mm->flags))
                return;

            get_task_struct(tsk);
            timer_setup(&tsk->oom_reaper_timer, wake_oom_reaper, 0);
            tsk->oom_reaper_timer.expires = jiffies + OOM_REAPER_DELAY;
            add_timer(&tsk->oom_reaper_timer);
        }
        return true;
    }

    if (!(oc->gfp_mask & __GFP_FS) && !is_memcg_oom(oc))
        return true;

    oc->constraint = constrained_alloc(oc);
    if (oc->constraint != CONSTRAINT_MEMORY_POLICY)
        oc->nodemask = NULL;
    check_panic_on_oom(oc);

    if (!is_memcg_oom(oc)
        && sysctl_oom_kill_allocating_task
        && current->mm && !oom_unkillable_task(current)
        && oom_cpuset_eligible(current, oc)
        && current->signal->oom_score_adj != OOM_SCORE_ADJ_MIN) {

        get_task_struct(current);
        oc->chosen = current;
        oom_kill_process(oc, "Out of memory (oom_kill_allocating_task)");
        return true;
    }

    select_bad_process(oc);
        --->
    if (!oc->chosen) {
        dump_header(oc);
    }
    if (oc->chosen && oc->chosen != (void *)-1UL) {
        oom_kill_process(oc, !is_memcg_oom(oc)
            ? "Out of memory" : "Memory cgroup out of memory"
        );
    }
    return !!oc->chosen;
}
```

## oom_reaper

```c
void wake_oom_reaper(struct timer_list *timer)
{
    struct task_struct *tsk = container_of(timer, struct task_struct, oom_reaper_timer);
    struct mm_struct *mm = tsk->signal->oom_mm;
    unsigned long flags;

    /* The victim managed to terminate on its own - see exit_mmap */
    if (test_bit(MMF_OOM_SKIP, &mm->flags)) {
        put_task_struct(tsk);
        return;
    }

    spin_lock_irqsave(&oom_reaper_lock, flags);
    tsk->oom_reaper_list = oom_reaper_list;
    oom_reaper_list = tsk;
    spin_unlock_irqrestore(&oom_reaper_lock, flags);
    wake_up(&oom_reaper_wait);
}

int oom_reaper(void *unused)
{
    set_freezable();

    while (true) {
        struct task_struct *tsk = NULL;

        wait_event_freezable(oom_reaper_wait, oom_reaper_list != NULL);

        spin_lock_irq(&oom_reaper_lock);
        if (oom_reaper_list != NULL) {
            tsk = oom_reaper_list;
            oom_reaper_list = tsk->oom_reaper_list;
        }
        spin_unlock_irq(&oom_reaper_lock);

        if (tsk) {
            oom_reap_task(tsk) {
                int attempts = 0;
                struct mm_struct *mm = tsk->signal->oom_mm;

                oom_reap_task_mm = [](tsk, mm) {
                    bool ret = true;

                    if (test_bit(MMF_OOM_SKIP, &mm->flags)) {
                        goto out_unlock;
                    }

                    ret = __oom_reap_task_mm(mm) {
                        struct vm_area_struct *vma;
                        bool ret = true;
                        VMA_ITERATOR(vmi, mm, 0);

                        set_bit(MMF_UNSTABLE, &mm->flags);

                        for_each_vma(vmi, vma) {
                            if (vma->vm_flags & (VM_HUGETLB|VM_PFNMAP))
                                continue;

                            if (vma_is_anonymous(vma) || !(vma->vm_flags & VM_SHARED)) {
                                struct mmu_notifier_range range;
                                struct mmu_gather tlb;

                                mmu_notifier_range_init(&range, MMU_NOTIFY_UNMAP, 0, mm, vma->vm_start, vma->vm_end);
                                tlb_gather_mmu(&tlb, mm);
                                if (mmu_notifier_invalidate_range_start_nonblock(&range)) {
                                    tlb_finish_mmu(&tlb);
                                    ret = false;
                                    continue;
                                }
                                unmap_page_range(&tlb, vma, range.start, range.end, NULL);
                                    --->
                                mmu_notifier_invalidate_range_end(&range);
                                tlb_finish_mmu(&tlb);
                            }
                        }

                        return ret;
                    }
                }

                while (attempts++ < MAX_OOM_REAP_RETRIES && !oom_reap_task_mm()) {
                    schedule_timeout_idle(HZ/10);
                }

                if (attempts <= MAX_OOM_REAP_RETRIES || test_bit(MMF_OOM_SKIP, &mm->flags))
                    goto done;

                sched_show_task(tsk);
                debug_show_all_locks();

            done:
                tsk->oom_reaper_list = NULL;

                set_bit(MMF_OOM_SKIP, &mm->flags);

                put_task_struct(tsk);
            }
        }
    }

    return 0;
}
```

## select_bad_process

```c
void select_bad_process(struct oom_control *oc)
{
    oc->chosen_points = LONG_MIN;

    if (is_memcg_oom(oc))
        mem_cgroup_scan_tasks(oc->memcg, oom_evaluate_task, oc);
    else {
        struct task_struct *p;

        rcu_read_lock();
        for_each_process(p) {
            oom_evaluate_task = [](p, oc) {
                struct oom_control *oc = arg;
                long points;

                if (oom_unkillable_task(task))
                    goto next;

                /* p may not have freeable memory in nodemask */
                if (!is_memcg_oom(oc) && !oom_cpuset_eligible(task, oc))
                    goto next;

                /* This task already has access to memory reserves and is being killed.
                * Don't allow any other task to have access to the reserves unless
                * the task has MMF_OOM_SKIP because chances that it would release
                * any memory is quite low. */
                if (!is_sysrq_oom(oc) && tsk_is_oom_victim(task)) {
                    if (test_bit(MMF_OOM_SKIP, &task->signal->oom_mm->flags))
                        goto next;
                    goto abort;
                }

                /* If task is allocating a lot of memory and has been marked to be
                 * killed first if it triggers an oom, then select it. */
                if (oom_task_origin(task)) { /* return p->signal->oom_flag_origin; */
                    points = LONG_MAX;
                    goto select;
                }

                points = oom_badness(task, oc->totalpages) {
                    long points;
                    long adj;

                    if (oom_unkillable_task(p))
                        return LONG_MIN;

                    p = find_lock_task_mm(p);
                    if (!p)
                        return LONG_MIN;

                    adj = (long)p->signal->oom_score_adj;
                    if (adj == OOM_SCORE_ADJ_MIN || test_bit(MMF_OOM_SKIP, &p->mm->flags) || in_vfork(p)) {
                        task_unlock(p);
                        return LONG_MIN;
                    }

                    /* The baseline for the badness score is the proportion of RAM that each
                     * task's rss, pagetable and swap space use */
                    points = get_mm_rss(p->mm) {
                        return get_mm_counter(mm, MM_FILEPAGES) +
                            get_mm_counter(mm, MM_ANONPAGES) +
                            get_mm_counter(mm, MM_SHMEMPAGES);
                    }
                    + get_mm_counter(p->mm, MM_SWAPENTS)
                    + mm_pgtables_bytes(p->mm) / PAGE_SIZE;
                    task_unlock(p);

                    /* Normalize to oom_score_adj units */
                    adj *= totalpages / 1000;
                    points += adj;

                    return points;
                }
                if (points == LONG_MIN || points < oc->chosen_points)
                    goto next;

            select:
                if (oc->chosen)
                    put_task_struct(oc->chosen);
                get_task_struct(task);
                oc->chosen = task;
                oc->chosen_points = points;
            next:
                return 0;
            abort:
                if (oc->chosen)
                    put_task_struct(oc->chosen);
                oc->chosen = (void *)-1UL;
                return 1;
            }

            if (oom_evaluate_task())
                break;
        }
        rcu_read_unlock();
    }
}
```

### mem_cgroup_scan_tasks

```c
void mem_cgroup_scan_tasks(struct mem_cgroup *memcg,
    int (*fn)(struct task_struct *, void *), void *arg)
{
    struct mem_cgroup *iter;
    int ret = 0;
    int i = 0;

    BUG_ON(mem_cgroup_is_root(memcg));

    for_each_mem_cgroup_tree(iter, memcg) {
        struct css_task_iter it;
        struct task_struct *task;

        css_task_iter_start(&iter->css, CSS_TASK_ITER_PROCS, &it);
        while (!ret && (task = css_task_iter_next(&it))) {
            /* Avoid potential softlockup warning */
            if ((++i & 1023) == 0)
                cond_resched();
            ret = fn(task, arg);
        }
        css_task_iter_end(&it);
        if (ret) {
            mem_cgroup_iter_break(memcg, iter);
            break;
        }
    }
}
```

## oom_kill_process

```c
void oom_kill_process(struct oom_control *oc, const char *message)
{
    struct task_struct *victim = oc->chosen;
    struct mem_cgroup *oom_group;
    static DEFINE_RATELIMIT_STATE(oom_rs, DEFAULT_RATELIMIT_INTERVAL,
                        DEFAULT_RATELIMIT_BURST);

    /* If the task is already exiting, don't alarm the sysadmin or kill
     * its children or threads, just give it access to memory reserves
     * so it can die quickly */
    task_lock(victim);
    if (task_will_free_mem(victim)) {
        mark_oom_victim(victim);
        queue_oom_reaper(victim);
        task_unlock(victim);
        put_task_struct(victim);
        return;
    }
    task_unlock(victim);

    if (__ratelimit(&oom_rs)) {
        dump_header(oc);
        dump_oom_victim(oc, victim);
    }

    oom_group = mem_cgroup_get_oom_group(victim, oc->memcg);

    __oom_kill_process(victim, message) {
        struct task_struct *p;
        struct mm_struct *mm;
        bool can_oom_reap = true;

        p = find_lock_task_mm(victim);
        if (!p) {
            put_task_struct(victim);
            return;
        } else if (victim != p) {
            get_task_struct(p);
            put_task_struct(victim);
            victim = p;
        }

        /* Get a reference to safely compare mm after task_unlock(victim) */
        mm = victim->mm;
        mmgrab(mm);

        /* Raise event before sending signal: task reaper must see this */
        count_vm_event(OOM_KILL);
        memcg_memory_event_mm(mm, MEMCG_OOM_KILL);

        do_send_sig_info(SIGKILL, SEND_SIG_PRIV, victim, PIDTYPE_TGID);
        mark_oom_victim(victim);
        task_unlock(victim);

        rcu_read_lock();
        for_each_process(p) {
            if (!process_shares_mm(p, mm))
                continue;
            if (same_thread_group(p, victim))
                continue;
            if (is_global_init(p)) {
                can_oom_reap = false;
                set_bit(MMF_OOM_SKIP, &mm->flags);
                continue;
            }

            if (unlikely(p->flags & PF_KTHREAD))
                continue;
            do_send_sig_info(SIGKILL, SEND_SIG_PRIV, p, PIDTYPE_TGID);
        }
        rcu_read_unlock();

        if (can_oom_reap)
            queue_oom_reaper(victim);

        mmdrop(mm);
        put_task_struct(victim);
    }

    /* If necessary, kill all tasks in the selected memory cgroup. */
    if (oom_group) {
        memcg_memory_event(oom_group, MEMCG_OOM_GROUP_KILL);
        mem_cgroup_print_oom_group(oom_group);
        mem_cgroup_scan_tasks(
            oom_group,
            oom_kill_memcg_member() {
                if (task->signal->oom_score_adj != OOM_SCORE_ADJ_MIN && !is_global_init(task)) {
                    get_task_struct(task);
                    __oom_kill_process(task, message);
                }
            }
            (void *)message
        );
        mem_cgroup_put(oom_group);
    }
}
```

# exit_mm

```c
/* exit -> do_exit -> exit_mm */
void exit_mm(void)
{
    struct mm_struct *mm = current->mm;

    exit_mm_release(current, mm) {
        futex_exit_release(tsk) {
            if (unlikely(tsk->robust_list)) {
                exit_robust_list(tsk) {

                }
                tsk->robust_list = NULL;
            }

            if (unlikely(!list_empty(&tsk->pi_state_list))) {
                exit_pi_state_list(tsk) {

                }
            }
        }

        mm_release(tsk, mm) {
            uprobe_free_utask(tsk);

            /* Get rid of any cached register state */
            deactivate_mm(tsk, mm);

            if (tsk->clear_child_tid) {
                if (atomic_read(&mm->mm_users) > 1) {
                    put_user(0, tsk->clear_child_tid);
                    do_futex(tsk->clear_child_tid, FUTEX_WAKE,
                            1, NULL, NULL, 0, 0);
                }
                tsk->clear_child_tid = NULL;
            }

            if (tsk->vfork_done)
                complete_vfork_done(tsk);
        }
    }

    if (!mm)
        return;

    mmap_read_lock(mm);
    mmgrab_lazy_tlb(mm);
    BUG_ON(mm != current->active_mm);
    /* more a memory barrier than a real lock */
    task_lock(current);
    smp_mb__after_spinlock();
    local_irq_disable();
    current->mm = NULL;
    membarrier_update_current_mm(NULL);
    enter_lazy_tlb(mm, current);
    local_irq_enable();
    task_unlock(current);
    mmap_read_unlock(mm);
    mm_update_next_owner(mm);

    mmput(mm) {
        if (atomic_dec_and_test(&mm->mm_users)) {
            __mmput(mm) {
                uprobe_clear_state(mm);
                exit_aio(mm);
                ksm_exit(mm);
                khugepaged_exit(mm); /* must run before exit_mmap */

                exit_mmap(mm) {
                    struct mmu_gather tlb;
                    struct vm_area_struct *vma;
                    unsigned long nr_accounted = 0;
                    MA_STATE(mas, &mm->mm_mt, 0, 0);
                    int count = 0;

                    /* mm's last user has gone, and its about to be pulled down */
                    mmu_notifier_release(mm);

                    mmap_read_lock(mm);
                    arch_exit_mmap(mm);

                    vma = mas_find(&mas, ULONG_MAX);
                    if (!vma || unlikely(xa_is_zero(vma))) {
                        /* Can happen if dup_mmap() received an OOM */
                        mmap_read_unlock(mm);
                        mmap_write_lock(mm);
                        goto destroy;
                    }

                    lru_add_drain();
                    flush_cache_mm(mm);
                    tlb_gather_mmu_fullmm(&tlb, mm);

                    /* update_hiwater_rss(mm) here? but nobody should be looking */
                    /* Use ULONG_MAX here to ensure all VMAs in the mm are unmapped */
                    unmap_vmas(&tlb, &mas, vma, 0, ULONG_MAX, ULONG_MAX, false) {
                        struct mmu_notifier_range range;
                        struct zap_details details = {
                            .zap_flags = ZAP_FLAG_DROP_MARKER | ZAP_FLAG_UNMAP,
                            /* Careful - we need to zap private pages too! */
                            .even_cows = true,
                        };

                        mmu_notifier_range_init(&range, MMU_NOTIFY_UNMAP, 0, vma->vm_mm,
                                    start_addr, end_addr);
                        mmu_notifier_invalidate_range_start(&range);
                        do {
                            unsigned long start = start_addr;
                            unsigned long end = end_addr;
                            hugetlb_zap_begin(vma, &start, &end);
                            unmap_single_vma(tlb, vma, start, end, &details, mm_wr_locked)
                                --->
                            hugetlb_zap_end(vma, &details);
                            vma = mas_find(mas, tree_end - 1);
                        } while (vma && likely(!xa_is_zero(vma)));
                        mmu_notifier_invalidate_range_end(&range);
                    }

                    mmap_read_unlock(mm);

                    /* Set MMF_OOM_SKIP to hide this task from the oom killer/reaper
                     * because the memory has been already freed. */
                    set_bit(MMF_OOM_SKIP, &mm->flags);
                    mmap_write_lock(mm);
                    mt_clear_in_rcu(&mm->mm_mt);
                    mas_set(&mas, vma->vm_end);
                    free_pgtables(&tlb, &mas, vma, FIRST_USER_ADDRESS, USER_PGTABLES_CEILING, true);
                        --->
                    tlb_finish_mmu(&tlb);

                    /* Walk the list again, actually closing and freeing it, with preemption
                     * enabled, without holding any MM locks besides the unreachable
                     * mmap_write_lock. */
                    mas_set(&mas, vma->vm_end);
                    do {
                        if (vma->vm_flags & VM_ACCOUNT) {
                            nr_accounted += vma_pages(vma);
                        }

                        remove_vma(vma, true) {
                            might_sleep();
                            if (vma->vm_ops && vma->vm_ops->close)
                                vma->vm_ops->close(vma);
                            if (vma->vm_file)
                                fput(vma->vm_file);
                            mpol_put(vma_policy(vma));
                            if (unreachable)
                                __vm_area_free(vma);
                            else {
                                vm_area_free(vma) {
                                    kmem_cache_free(vm_area_cachep, vma);
                                }
                            }
                        }

                        count++;
                        cond_resched();
                        vma = mas_find(&mas, ULONG_MAX);
                    } while (vma && likely(!xa_is_zero(vma)));

                    BUG_ON(count != mm->map_count);

                    trace_exit_mmap(mm);
                destroy:
                    __mt_destroy(&mm->mm_mt);
                    mmap_write_unlock(mm);
                    vm_unacct_memory(nr_accounted);
                }
                mm_put_huge_zero_page(mm);
                set_mm_exe_file(mm, NULL);
                if (!list_empty(&mm->mmlist)) {
                    spin_lock(&mmlist_lock);
                    list_del(&mm->mmlist);
                    spin_unlock(&mmlist_lock);
                }
                if (mm->binfmt)
                    module_put(mm->binfmt->module);
                lru_gen_del_mm(mm);

                mmdrop(mm) {
                    if (unlikely(atomic_dec_and_test(&mm->mm_count))) {
                        __mmdrop(mm) {
                            cleanup_lazy_tlbs(mm);

                            WARN_ON_ONCE(mm == current->active_mm);
                            mm_free_pgd(mm) {
                                pgd_free(mm, mm->pgd) {
                                    if (PGD_SIZE == PAGE_SIZE)
                                        free_page((unsigned long)pgd);
                                    else
                                        kmem_cache_free(pgd_cache, pgd);
                                }
                            }
                            destroy_context(mm);
                            mmu_notifier_subscriptions_destroy(mm);
                            check_mm(mm);
                            put_user_ns(mm->user_ns);
                            mm_pasid_drop(mm);
                            mm_destroy_cid(mm);

                            for (i = 0; i < NR_MM_COUNTERS; i++) {
                                percpu_counter_destroy(&mm->rss_stat[i]);
                            }
                            free_mm(mm) {
                                kmem_cache_free(mm_cachep, (mm));
                            }
                        }
                    }
                }
            }
        }
    }

    if (test_thread_flag(TIF_MEMDIE)) {
        exit_oom_victim() {
            clear_thread_flag(TIF_MEMDIE);
            if (!atomic_dec_return(&oom_victims)) {
                wake_up_all(&oom_victims_wait);
            }
        }
    }
}
```

# dma

## dma_alloc_coherent

```c
static inline void *dma_alloc_coherent(struct device *dev, size_t size,
        dma_addr_t *dma_handle, gfp_t gfp)
{
    return dma_alloc_attrs(dev, size, dma_handle, gfp, (gfp & __GFP_NOWARN) ? DMA_ATTR_NO_WARN : 0) {
        const struct dma_map_ops *ops = get_dma_ops(dev);
        void *cpu_addr;

        WARN_ON_ONCE(!dev->coherent_dma_mask);

        /* DMA allocations can never be turned back into a page pointer, so
        * requesting compound pages doesn't make sense (and can't even be
        * supported at all by various backends). */
        if (WARN_ON_ONCE(flag & __GFP_COMP))
            return NULL;

        if (dma_alloc_from_dev_coherent(dev, size, dma_handle, &cpu_addr)) {
            trace_dma_alloc(dev, cpu_addr, *dma_handle, size,
                    DMA_BIDIRECTIONAL, flag, attrs);
            return cpu_addr;
        }

        /* let the implementation decide on the zone to allocate from: */
        flag &= ~(__GFP_DMA | __GFP_DMA32 | __GFP_HIGHMEM);

        if (dma_alloc_direct(dev, ops)) {
            cpu_addr = dma_direct_alloc(dev, size, dma_handle, flag, attrs);
        } else if (use_dma_iommu(dev)) {
            cpu_addr = iommu_dma_alloc(dev, size, dma_handle, flag, attrs);
        } else if (ops->alloc) {
            cpu_addr = ops->alloc(dev, size, dma_handle, flag, attrs);
        } else {
            trace_dma_alloc(dev, NULL, 0, size, DMA_BIDIRECTIONAL, flag,
                    attrs);
            return NULL;
        }

        trace_dma_alloc(dev, cpu_addr, *dma_handle, size, DMA_BIDIRECTIONAL,
                flag, attrs);
        debug_dma_alloc_coherent(dev, size, *dma_handle, cpu_addr, attrs);
        return cpu_addr;
    }
}
```

### dma_direct_alloc

```c
void *dma_direct_alloc(struct device *dev, size_t size,
        dma_addr_t *dma_handle, gfp_t gfp, unsigned long attrs)
{
    bool remap = false, set_uncached = false;
    struct page *page;
    void *ret;

    size = PAGE_ALIGN(size);
    if (attrs & DMA_ATTR_NO_WARN)
        gfp |= __GFP_NOWARN;

    if ((attrs & DMA_ATTR_NO_KERNEL_MAPPING) &&
        !force_dma_unencrypted(dev) && !is_swiotlb_for_alloc(dev))
        return dma_direct_alloc_no_mapping(dev, size, dma_handle, gfp);

    if (!dev_is_dma_coherent(dev)) {
        if (IS_ENABLED(CONFIG_ARCH_HAS_DMA_ALLOC) &&
            !is_swiotlb_for_alloc(dev))
            return arch_dma_alloc(dev, size, dma_handle, gfp,
                        attrs);

        /* If there is a global pool, always allocate from it for
        * non-coherent devices. */
        if (IS_ENABLED(CONFIG_DMA_GLOBAL_POOL))
            return dma_alloc_from_global_coherent(dev, size,
                    dma_handle);

        /* Otherwise we require the architecture to either be able to
        * mark arbitrary parts of the kernel direct mapping uncached,
        * or remapped it uncached. */
        set_uncached = IS_ENABLED(CONFIG_ARCH_HAS_DMA_SET_UNCACHED);
        remap = IS_ENABLED(CONFIG_DMA_DIRECT_REMAP);
        if (!set_uncached && !remap) {
            pr_warn_once("coherent DMA allocations not supported on this platform.\n");
            return NULL;
        }
    }

    /* Remapping or decrypting memory may block, allocate the memory from
    * the atomic pools instead if we aren't allowed block. */
    if ((remap || force_dma_unencrypted(dev)) &&
        dma_direct_use_pool(dev, gfp))
        return dma_direct_alloc_from_pool(dev, size, dma_handle, gfp);

    /* we always manually zero the memory once we are done */
    page = __dma_direct_alloc_pages(dev, size, gfp & ~__GFP_ZERO, true);
    if (!page)
        return NULL;

    /* dma_alloc_contiguous can return highmem pages depending on a
    * combination the cma= arguments and per-arch setup.  These need to be
    * remapped to return a kernel virtual address. */
    if (PageHighMem(page)) {
        remap = true;
        set_uncached = false;
    }

    if (remap) {
        pgprot_t prot = dma_pgprot(dev, PAGE_KERNEL, attrs);

        if (force_dma_unencrypted(dev))
            prot = pgprot_decrypted(prot);

        /* remove any dirty cache lines on the kernel alias */
        arch_dma_prep_coherent(page, size);

        /* create a coherent mapping */
        ret = dma_common_contiguous_remap(page, size, prot,
                __builtin_return_address(0));
        if (!ret)
            goto out_free_pages;
    } else {
        ret = page_address(page);
        if (dma_set_decrypted(dev, ret, size))
            goto out_leak_pages;
    }

    memset(ret, 0, size);

    if (set_uncached) {
        arch_dma_prep_coherent(page, size);
        ret = arch_dma_set_uncached(ret, size);
        if (IS_ERR(ret))
            goto out_encrypt_pages;
    }

    *dma_handle = phys_to_dma_direct(dev, page_to_phys(page));
    return ret;

out_encrypt_pages:
    if (dma_set_encrypted(dev, page_address(page), size))
        return NULL;
out_free_pages:
    __dma_direct_free_pages(dev, page, size);
    return NULL;
out_leak_pages:
    return NULL;
}
```

### iommu_dma_alloc

# hugetlb

![](../images/kernel/mem-hugetlb.svg)

* [kernel doc](https://docs.kernel.org/mm/index.html)
    * [HugeTLB Pages](https://docs.kernel.org/admin-guide/mm/hugetlbpage.html)
    * [Transparent Hugepage Support](https://docs.kernel.org/admin-guide/mm/transhuge.html)
    * [Hugetlbfs Reservation](https://docs.kernel.org/mm/hugetlbfs_reserv.html)
* [[PATCH v23 0/9] Free some vmemmap pages of HugeTLB page](https://lore.kernel.org/all/20210510030027.56044-1-songmuchun@bytedance.com/)
* [[PATCH v10 0/8] Buddy allocator like (or non-uniform) folio split](https://lore.kernel.org/all/20250307174001.242794-1-ziy@nvidia.com/)

* [Linux mTHP 动态大页](https://mp.weixin.qq.com/s/B76XlGP7efmsfZR-ABAM1A)
* [Leo - Linux内存管理之透明大页(THP)](https://mp.weixin.qq.com/s/xRxP4beYJ-5URA7MUW96eA)
* [为什么HugePage能让Oracle数据库如虎添翼?](https://mp.weixin.qq.com/s/3Lb7-KuAlN6NnfFPL5RDdQ)

Segment | hugetlb   | mTHP
:-:     | :-:       | :-:
Text    | Limited   | Limited
Data    | Partial   | Partial
Heap    | :white_check_mark: | :white_check_mark:
SHM     | :white_check_mark: | only mmap(MAP_ANONYMOUS | MAP_SHARED)
Stack   | :negative_squared_cross_mark: | Y

* **Limited**. The text segment is typically not backed by huge pages unless the application is specifically designed to load its executable code into a huge-page-backed region (e.g., via mmap with MAP_HUGETLB).
* **Partial**. Static data can be backed by THP if the data segment is part of an anonymous mapping and meets the kernel’s criteria for promotion to huge pages (e.g., sufficient contiguous memory).
* **File-backed segments** (e.g., text, libraries, or memory-mapped files) are less likely to use huge pages unless explicitly configured (HugeTLB) or supported by the file system (THP in newer kernels). This limits support for text and library segments.

**overview of the current limitations of huge pages:**
1. Inability to Swap to Disk. cannot be swapped out to disk under memory pressure
2. Memory Fragmentation Challenges. Allocating huge page at runtime can fail if memory is fragmented
3. Limited Application Support
    * applications must explicitly support huge pages via system calls like `mmap() with MAP_HUGETLB` or `shmget() with SHM_HUGETLB`.
    * Many applications are not designed to use these interfaces, limiting the applicability of HugeTLB pages.
4. Transparent Huge Pages (THP) Limitations
    * Latency Spikes: Promoting smaller pages to huge pages or demoting them can cause latency spikes due to memory compaction and defragmentation
    * Suboptimal Memory Usage: THP may promote pages to huge pages unnecessarily, leading to wasted memory for applications with irregular access patterns.
    * Limited Control: Developers have less control over THP behavior compared to HugeTLB
    * Limited Size Support: THP only supports 2MB pages, not 1GB pages, restricting its scalability for extremely large datasets.
5. Complex Configuration
    * setting kernel parameters (hugepages, hugepagesz, default_hugepagesz)
    * modifying `/etc/sysctl.conf`, creating mount points for hugetlbfs
    * adjusting memory lock limits in `/etc/security/limits.conf`.

6. NUMA System Challenges
    * huge page allocation can be unevenly distributed across nodes unless explicitly configured
    * `/sys/devices/system/node/node_id/hugepages/hugepages-size/nr_hugepages` to allocate huge pages per node
7. Limited Monitoring and Debugging

8. Overcommit and Surplus Pages

    * The nr_overcommit_hugepages parameter allows the system to allocate surplus huge pages from the normal page pool when the persistent pool is exhausted.
9. Architecture-Specific Constraints

```sh
/proc/                       # Virtual filesystem with HugeTLB info
├── meminfo                  # Memory info, including HugePages stats
│                            # - HugePages_Total: Total huge pages allocated
│                            # - HugePages_Free: Free huge pages
│                            # - HugePages_Rsvd: Reserved huge pages
│                            # - HugePages_Surp: Surplus huge pages
│                            # - Hugepagesize: Default huge page size (e.g., 2MB)
│
├── [pid]/                   # Process-specific directories
│   ├── smaps                # Memory mappings, including huge page usage
│   └── numa_maps            # NUMA memory mappings, showing huge page details

/sys/kernel/mm/hugepages/    # HugeTLB configuration directory
├── hugepages-2048kB/        # Directory for 2MB huge pages (example size)
│   ├── nr_hugepages         # Number of huge pages allocated
│   ├── nr_hugepages_mempolicy  # Number of huge pages for NUMA policy
│   ├── nr_overcommit_hugepages # Overcommit huge pages
│   ├── free_hugepages       # Number of free huge pages
│   ├── resv_hugepages       # Number of reserved huge pages
│   ├── surplus_hugepages    # Number of surplus huge pages
├── hugepages-1048576kB/     # Directory for 1GB huge pages (if supported)
│   ├── nr_hugepages         # Same as above, for 1GB pages
│   ├── nr_hugepages_mempolicy
│   ├── nr_overcommit_hugepages
│   ├── free_hugepages
│   ├── resv_hugepages
│   ├── surplus_hugepages

/sys/devices/system/node/    # NUMA node-specific huge page info
├── node[0-N]/               # Per-NUMA node directories
│   ├── hugepages/           # HugeTLB info for the node
│   │   ├── hugepages-2048kB/ # 2MB huge pages for this node
│   │   │   ├── nr_hugepages  # Number of huge pages on this node
│   │   │   ├── free_hugepages      # Free huge pages on this node
│   │   │   ├── surplus_hugepages   # Surplus huge pages on this node
│   │   ├── hugepages-1048576kB/    # 1GB huge pages (if supported)
```

**How to Enable:**
```sh
# 1. kernel config
CONFIG_HUGETLBFS=y
CONFIG_HUGETLB_PAGE=y
CONFIG_TRANSPARENT_HUGEPAGE=y


# staic config
# boot time paramters: /etc/default/grub
hugepages=1024
hugepagesz=1G hugepages=2
default_hugepagesz=2M hugepages=512 hugepagesz=1G hugepages=1


# dynamic config
sudo sh -c "echo 1024 > /sys/kernel/mm/hugepages/hugepages-<size>kB/nr_hugepages"

# Overcommit Surplus Pages:
echo 100 > /proc/sys/vm/nr_overcommit_hugepages

```

---

```c
int hugetlb_max_hstate __read_mostly;

unsigned int default_hstate_idx;

struct hstate hstates[HUGE_MAX_HSTATE];
```

```c
struct hstate {
    struct mutex        resize_lock;
    struct lock_class_key resize_key;
    int                 next_nid_to_alloc;
    int                 next_nid_to_free;
    unsigned int        order;
    unsigned int        demote_order;
    unsigned long       mask;
    unsigned long       max_huge_pages;
    unsigned long       nr_huge_pages;
    unsigned long       free_huge_pages;
    unsigned long       resv_huge_pages;
    unsigned long       surplus_huge_pages;
    unsigned long       nr_overcommit_huge_pages;
    struct list_head    hugepage_activelist;
    struct list_head    hugepage_freelists[MAX_NUMNODES];
    unsigned int        max_huge_pages_node[MAX_NUMNODES];
    unsigned int        nr_huge_pages_node[MAX_NUMNODES];
    unsigned int        free_huge_pages_node[MAX_NUMNODES];
    unsigned int        surplus_huge_pages_node[MAX_NUMNODES];
    char name[HSTATE_NAME_LEN];
};

 /* HugeTLB Support Matrix
  *
  * ---------------------------------------------------
  * | Page Size | CONT PTE |  PMD  | CONT PMD |  PUD  |
  * ---------------------------------------------------
  * |     4K    |   64K    |   2M  |    32M   |   1G  |
  * |    16K    |    2M    |  32M  |     1G   |       |
  * |    64K    |    2M    | 512M  |    16G   |       |
  * --------------------------------------------------- */
```

## arm64_hugetlb_cma_reserve

```c
void __init arm64_hugetlb_cma_reserve(void)
{
    int order;

    if (pud_sect_supported())
        order = PUD_SHIFT - PAGE_SHIFT;
    else
        order = CONT_PMD_SHIFT - PAGE_SHIFT;

    hugetlb_cma_reserve(order);
}

void __init hugetlb_cma_reserve(int order)
{
    unsigned long size, reserved, per_node;
    bool node_specific_cma_alloc = false;
    int nid;

    /* HugeTLB CMA reservation is required for gigantic
     * huge pages which could not be allocated via the
     * page allocator. Just warn if there is any change
     * breaking this assumption. */
    VM_WARN_ON(order <= MAX_PAGE_ORDER);
    cma_reserve_called = true;

    if (!hugetlb_cma_size)
        return;

    hugetlb_bootmem_set_nodes() {
        int i, nid;
        unsigned long start_pfn, end_pfn;

        if (!nodes_empty(hugetlb_bootmem_nodes))
            return;

        for_each_mem_pfn_range(i, MAX_NUMNODES, &start_pfn, &end_pfn, &nid) {
            if (end_pfn > start_pfn)
                node_set(nid, hugetlb_bootmem_nodes);
        }
    }

    for (nid = 0; nid < MAX_NUMNODES; nid++) {
        if (hugetlb_cma_size_in_node[nid] == 0)
            continue;

        if (!node_isset(nid, hugetlb_bootmem_nodes)) {
            pr_warn("hugetlb_cma: invalid node %d specified\n", nid);
            hugetlb_cma_size -= hugetlb_cma_size_in_node[nid];
            hugetlb_cma_size_in_node[nid] = 0;
            continue;
        }

        if (hugetlb_cma_size_in_node[nid] < (PAGE_SIZE << order)) {
            pr_warn("hugetlb_cma: cma area of node %d should be at least %lu MiB\n",
                nid, (PAGE_SIZE << order) / SZ_1M);
            hugetlb_cma_size -= hugetlb_cma_size_in_node[nid];
            hugetlb_cma_size_in_node[nid] = 0;
        } else {
            node_specific_cma_alloc = true;
        }
    }

    /* Validate the CMA size again in case some invalid nodes specified. */
    if (!hugetlb_cma_size)
        return;

    if (hugetlb_cma_size < (PAGE_SIZE << order)) {
        pr_warn("hugetlb_cma: cma area should be at least %lu MiB\n",
            (PAGE_SIZE << order) / SZ_1M);
        hugetlb_cma_size = 0;
        return;
    }

    if (!node_specific_cma_alloc) {
        /* If 3 GB area is requested on a machine with 4 numa nodes,
         * let's allocate 1 GB on first three nodes and ignore the last one. */
        per_node = DIV_ROUND_UP(hugetlb_cma_size,
                    nodes_weight(hugetlb_bootmem_nodes));
        pr_info("hugetlb_cma: reserve %lu MiB, up to %lu MiB per node\n",
            hugetlb_cma_size / SZ_1M, per_node / SZ_1M);
    }

    reserved = 0;
    for_each_node_mask(nid, hugetlb_bootmem_nodes) {
        int res;
        char name[CMA_MAX_NAME];

        if (node_specific_cma_alloc) {
            if (hugetlb_cma_size_in_node[nid] == 0)
                continue;

            size = hugetlb_cma_size_in_node[nid];
        } else {
            size = min(per_node, hugetlb_cma_size - reserved);
        }

        size = round_up(size, PAGE_SIZE << order);

        snprintf(name, sizeof(name), "hugetlb%d", nid);
        /* Note that 'order per bit' is based on smallest size that
         * may be returned to CMA allocator in the case of
         * huge page demotion. */
        res = cma_declare_contiguous_multi(size, PAGE_SIZE << order,
                    HUGETLB_PAGE_ORDER, name,
                    &hugetlb_cma[nid], nid);
        if (res) {
            pr_warn("hugetlb_cma: reservation failed: err %d, node %d",
                res, nid);
            continue;
        }

        reserved += size;
        pr_info("hugetlb_cma: reserved %lu MiB on node %d\n",
            size / SZ_1M, nid);

        if (reserved >= hugetlb_cma_size)
            break;
    }

    if (!reserved)
        /* hugetlb_cma_size is used to determine if allocations from
         * cma are possible.  Set to zero if no cma regions are set up. */
        hugetlb_cma_size = 0;
}

```

## hugetlb_init

```c
kernel_init() {
    do_basic_setup() {
        do_initcalls() {
            subsys_initcall(hugetlb_init);
        }
    }
}

hugetlb_early_param("hugepages", hugepages_setup);
hugetlb_early_param("hugepagesz", hugepagesz_setup);
hugetlb_early_param("default_hugepagesz", default_hugepagesz_setup);
early_param("hugetlb_cma", cmdline_parse_hugetlb_cma);
early_param("hugetlb_cma_only", cmdline_parse_hugetlb_cma_only);
```

```c
static int __init hugetlb_init(void)
{
    int i;

    if (!hugepages_supported()) {
        if (hugetlb_max_hstate || default_hstate_max_huge_pages)
            pr_warn("HugeTLB: huge pages not supported, ignoring associated command-line parameters\n");
        return 0;
    }

    hugetlb_add_hstate(HUGETLB_PAGE_ORDER) {
        struct hstate *h;
        unsigned long i;

        if (size_to_hstate(PAGE_SIZE << order)) {
            return;
        }
        BUG_ON(hugetlb_max_hstate >= HUGE_MAX_HSTATE);
        BUG_ON(order < order_base_2(__NR_USED_SUBPAGE));

        h = &hstates[hugetlb_max_hstate++];
        __mutex_init(&h->resize_lock, "resize mutex", &h->resize_key);
        h->order = order;
        h->mask = ~(huge_page_size(h) - 1);
        for (i = 0; i < MAX_NUMNODES; ++i)
            INIT_LIST_HEAD(&h->hugepage_freelists[i]);
        INIT_LIST_HEAD(&h->hugepage_activelist);
        snprintf(h->name, HSTATE_NAME_LEN, "hugepages-%lukB", huge_page_size(h)/SZ_1K);

        parsed_hstate = h;
    }

    if (!parsed_default_hugepagesz) {
        default_hstate_idx = hstate_index(size_to_hstate(HPAGE_SIZE));
        if (default_hstate_max_huge_pages) {
            if (default_hstate.max_huge_pages) {
                char buf[32];

                string_get_size(huge_page_size(&default_hstate),
                    1, STRING_UNITS_2, buf, 32);
            }
            default_hstate.max_huge_pages = default_hstate_max_huge_pages;

            for_each_online_node(i)
                default_hstate.max_huge_pages_node[i] =
                    default_hugepages_in_node[i];
        }
    }

    hugetlb_cma_check() {
        if (!hugetlb_cma_size || cma_reserve_called)
            return;

        pr_warn("hugetlb_cma: the option isn't supported by current arch\n");
    }

    hugetlb_init_hstates() {
        struct hstate *h, *h2;

        for_each_hstate(h) {
            h->next_nid_to_alloc = first_memory_node;
            h->next_nid_to_free = first_memory_node;

            /* oversize hugepages were init'ed in early boot */
            if (!hstate_is_gigantic(h))
                hugetlb_hstate_alloc_pages(h);
                --->

            /* Set demote order for each hstate.  Note that
            * h->demote_order is initially 0.
            * - We can not demote gigantic pages if runtime freeing
            *   is not supported, so skip this.
            * - If CMA allocation is possible, we can not demote
            *   HUGETLB_PAGE_ORDER or smaller size pages. */
            if (hstate_is_gigantic(h) && !gigantic_page_runtime_supported())
                continue;

            if (hugetlb_cma_total_size() && h->order <= HUGETLB_PAGE_ORDER)
                continue;

            for_each_hstate(h2) {
                if (h2 == h)
                    continue;
                if (h2->order < h->order && h2->order > h->demote_order)
                    h->demote_order = h2->order;
            }
        }
    }

    gather_bootmem_prealloc() {
        struct padata_mt_job job = {
            .thread_fn  = gather_bootmem_prealloc_parallel,
            .fn_arg     = NULL,
            .start      = 0,
            .size       = nr_node_ids,
            .align      = 1,
            .min_chunk  = 1,
            .max_threads    = num_node_state(N_MEMORY),
            .numa_aware     = true,
        };

        padata_do_multithreaded(&job);
    }

    report_hugepages();

    hugetlb_sysfs_init();
    hugetlb_cgroup_file_init();

    num_fault_mutexes = roundup_pow_of_two(8 * num_possible_cpus());

    hugetlb_fault_mutex_table =
        kmalloc_array(num_fault_mutexes, sizeof(struct mutex), GFP_KERNEL);
    BUG_ON(!hugetlb_fault_mutex_table);

    for (i = 0; i < num_fault_mutexes; i++)
        mutex_init(&hugetlb_fault_mutex_table[i]);
    return 0;
}
```

### gather_bootmem_prealloc_parallel

```c
static void __init gather_bootmem_prealloc_parallel(unsigned long start,
                            unsigned long end, void *arg)
{
    int nid;

    for (nid = start; nid < end; nid++) {
        gather_bootmem_prealloc_node(nid) {
            LIST_HEAD(folio_list);
            struct huge_bootmem_page *m, *tm;
            struct hstate *h = NULL, *prev_h = NULL;

            list_for_each_entry_safe(m, tm, &huge_boot_pages[nid], list) {
                struct page *page = virt_to_page(m);
                struct folio *folio = (void *)page;

                h = m->hstate;
                if (!hugetlb_bootmem_page_zones_valid(nid, m)) {
                    /* Can't use this page. Initialize the
                    * page structures if that hasn't already
                    * been done, and give them to the page
                    * allocator. */
                    hugetlb_bootmem_free_invalid_page(nid, page, h);
                    continue;
                }

                /* It is possible to have multiple huge page sizes (hstates)
                * in this list.  If so, process each size separately. */
                if (h != prev_h && prev_h != NULL)
                    prep_and_add_bootmem_folios(prev_h, &folio_list);
                prev_h = h;

                VM_BUG_ON(!hstate_is_gigantic(h));
                WARN_ON(folio_ref_count(folio) != 1);

                hugetlb_folio_init_vmemmap(folio, h, HUGETLB_VMEMMAP_RESERVE_PAGES) {

                }
                init_new_hugetlb_folio(h, folio) {

                }

                if (hugetlb_bootmem_page_prehvo(m))
                    /* If pre-HVO was done, just set the
                    * flag, the HVO code will then skip
                    * this folio. */
                    folio_set_hugetlb_vmemmap_optimized(folio);

                if (hugetlb_bootmem_page_earlycma(m))
                    folio_set_hugetlb_cma(folio);

                list_add(&folio->lru, &folio_list);

                /* We need to restore the 'stolen' pages to totalram_pages
                * in order to fix confusing memory reports from free(1) and
                * other side-effects, like CommitLimit going negative.
                *
                * For CMA pages, this is done in init_cma_pageblock
                * (via hugetlb_bootmem_init_migratetype), so skip it here. */
                if (!folio_test_hugetlb_cma(folio))
                    adjust_managed_page_count(page, pages_per_huge_page(h));
                cond_resched();
            }

            prep_and_add_bootmem_folios(h, &folio_list) {
                unsigned long flags;
                struct folio *folio, *tmp_f;

                /* Send list for bulk vmemmap optimization processing */
                hugetlb_vmemmap_optimize_bootmem_folios(h, folio_list);

                list_for_each_entry_safe(folio, tmp_f, folio_list, lru) {
                    if (!folio_test_hugetlb_vmemmap_optimized(folio)) {
                        /* If HVO fails, initialize all tail struct pages
                        * We do not worry about potential long lock hold
                        * time as this is early in boot and there should
                        * be no contention. */
                        hugetlb_folio_init_tail_vmemmap(folio,
                                HUGETLB_VMEMMAP_RESERVE_PAGES,
                                pages_per_huge_page(h));
                    }
                    hugetlb_bootmem_init_migratetype(folio, h);
                    /* Subdivide locks to achieve better parallel performance */
                    spin_lock_irqsave(&hugetlb_lock, flags);
                    __prep_account_new_huge_page(h, folio_nid(folio));
                    enqueue_hugetlb_folio(h, folio);
                    spin_unlock_irqrestore(&hugetlb_lock, flags);
                }
            }
        }
    }
}
```

## hugetlb_hstate_alloc_pages

```c
static void __init hugetlb_hstate_alloc_pages(struct hstate *h)
{
    unsigned long allocated;

    /* Skip gigantic hugepages allocation if early CMA
    * reservations are not available. */
    if (hstate_is_gigantic(h) && hugetlb_cma_total_size() && !hugetlb_early_cma(h)) {
        pr_warn_once("HugeTLB: hugetlb_cma is enabled, skip boot time allocation\n");
        return;
    }

    /* do node specific alloc */
    ret = hugetlb_hstate_alloc_pages_specific_nodes(h) {
        int i;
        bool node_specific_alloc = false;

        for_each_online_node(i) {
            if (h->max_huge_pages_node[i] > 0) {
                hugetlb_hstate_alloc_pages_onenode(h, i) {
                    unsigned long i;
                    char buf[32];
                    LIST_HEAD(folio_list);

                    for (i = 0; i < h->max_huge_pages_node[nid]; ++i) {
                        if (hstate_is_gigantic(h)) {
                            if (!alloc_bootmem_huge_page(h, nid)) {
                                --->
                                break;
                            }
                        } else {
                            struct folio *folio;
                            gfp_t gfp_mask = htlb_alloc_mask(h) | __GFP_THISNODE;

                            folio = only_alloc_fresh_hugetlb_folio(h, gfp_mask, nid, &node_states[N_MEMORY], NULL);
                                --->
                            if (!folio)
                                break;
                            list_add(&folio->lru, &folio_list);
                        }
                        cond_resched();
                    }

                    if (!list_empty(&folio_list)) {
                        prep_and_add_allocated_folios(h, &folio_list) {
                            unsigned long flags;
                            struct folio *folio, *tmp_f;

                            /* Send list for bulk vmemmap optimization processing */
                            hugetlb_vmemmap_optimize_folios(h, folio_list);

                            /* Add all new pool pages to free lists in one lock cycle */
                            spin_lock_irqsave(&hugetlb_lock, flags);
                            list_for_each_entry_safe(folio, tmp_f, folio_list, lru) {
                                __prep_account_new_huge_page(h, folio_nid(folio)) {
                                    h->nr_huge_pages++;
                                    h->nr_huge_pages_node[nid]++;
                                }
                                enqueue_hugetlb_folio(h, folio) {
                                    int nid = folio_nid(folio);

                                    lockdep_assert_held(&hugetlb_lock);
                                    VM_BUG_ON_FOLIO(folio_ref_count(folio), folio);

                                    list_move(&folio->lru, &h->hugepage_freelists[nid]);
                                    h->free_huge_pages++;
                                    h->free_huge_pages_node[nid]++;
                                    folio_set_hugetlb_freed(folio);
                                }
                            }
                            spin_unlock_irqrestore(&hugetlb_lock, flags);
                        }
                    }

                    if (i == h->max_huge_pages_node[nid])
                        return;

                    string_get_size(huge_page_size(h), 1, STRING_UNITS_2, buf, 32);
                    pr_warn("HugeTLB: allocating %u of page size %s failed node%d.  Only allocated %lu hugepages.\n",
                        h->max_huge_pages_node[nid], buf, nid, i);
                    h->max_huge_pages -= (h->max_huge_pages_node[nid] - i);
                    h->max_huge_pages_node[nid] = i;
                }
                node_specific_alloc = true;
            }
        }

        return node_specific_alloc;
    }
    if (ret)
        return;

    /* below will do all node balanced alloc */
    if (hstate_is_gigantic(h))
        allocated = hugetlb_gigantic_pages_alloc_boot(h);
    else
        allocated = hugetlb_pages_alloc_boot(h);

    hugetlb_hstate_alloc_pages_errcheck(allocated, h) {
        if (allocated < h->max_huge_pages) {
            h->max_huge_pages = allocated;
        }
    }
}
```

### hugetlb_gigantic_pages_alloc_boot

```c
static unsigned long __init hugetlb_gigantic_pages_alloc_boot(struct hstate *h)
{
    unsigned long i;

    for (i = 0; i < h->max_huge_pages; ++i) {
        if (!alloc_bootmem_huge_page(h, NUMA_NO_NODE))
            break;
        cond_resched();
    }

    return i;
}

int __init alloc_bootmem_huge_page(struct hstate *h, int nid)
{

#ifdef CONFIG_PPC_BOOK3S_64
    if (firmware_has_feature(FW_FEATURE_LPAR) && !radix_enabled())
        return pseries_alloc_bootmem_huge_page(h);
#endif
    return __alloc_bootmem_huge_page(h, nid) {
        struct huge_bootmem_page *m = NULL; /* initialize for clang */
        int nr_nodes, node = nid;

        /* do node specific alloc */
        if (nid != NUMA_NO_NODE) {
            m = alloc_bootmem(h, node, true);
            if (!m)
                return 0;
            goto found;
        }

        /* allocate from next node when distributing huge pages */
        for_each_node_mask_to_alloc(&h->next_nid_to_alloc, nr_nodes, node, &hugetlb_bootmem_nodes) {
            m = alloc_bootmem(h, node, false);
            if (!m)
                return 0;
            goto found;
        }

    found:

        /* Only initialize the head struct page in memmap_init_reserved_pages,
        * rest of the struct pages will be initialized by the HugeTLB
        * subsystem itself.
        * The head struct page is used to get folio information by the HugeTLB
        * subsystem like zone id and node id. */
        memblock_reserved_mark_noinit(virt_to_phys((void *)m + PAGE_SIZE),
            huge_page_size(h) - PAGE_SIZE) {

            return memblock_setclr_flag(&memblock.reserved, base, size, 1,
                MEMBLOCK_RSRV_NOINIT);
        }

        return 1;
    }
}

struct huge_bootmem_page {
    struct list_head list;
    struct hstate *hstate;
    unsigned long flags;
    struct cma *cma;
};

static struct cma *hugetlb_cma[MAX_NUMNODES];

static __init void *alloc_bootmem(struct hstate *h, int nid, bool node_exact)
{
    struct huge_bootmem_page *m;
    int listnode = nid;

    if (hugetlb_early_cma(h)) {
        m = hugetlb_cma_alloc_bootmem(h, &listnode, node_exact) {
            struct cma *cma;
            struct huge_bootmem_page *m;
            int node = *nid;

            cma = hugetlb_cma[*nid];
            m = cma_reserve_early(cma, huge_page_size(h));
            if (!m) {
                if (node_exact)
                    return NULL;

                for_each_node_mask(node, hugetlb_bootmem_nodes) {
                    cma = hugetlb_cma[node];
                    if (!cma || node == *nid)
                        continue;
                    m = cma_reserve_early(cma, huge_page_size(h)) {
                        int r;
                        struct cma_memrange *cmr;
                        unsigned long available;
                        void *ret = NULL;

                        if (!cma || !cma->count)
                            return NULL;

                        /* Can only be called early in init. */
                        if (test_bit(CMA_ACTIVATED, &cma->flags))
                            return NULL;

                        if (!IS_ALIGNED(size, CMA_MIN_ALIGNMENT_BYTES))
                            return NULL;

                        if (!IS_ALIGNED(size, (PAGE_SIZE << cma->order_per_bit)))
                            return NULL;

                        size >>= PAGE_SHIFT;

                        if (size > cma->available_count)
                            return NULL;

                        for (r = 0; r < cma->nranges; r++) {
                            cmr = &cma->ranges[r];
                            available = cmr->count - (cmr->early_pfn - cmr->base_pfn);
                            if (size <= available) {
                                ret = phys_to_virt(PFN_PHYS(cmr->early_pfn));
                                cmr->early_pfn += size;
                                cma->available_count -= size;
                                return ret;
                            }
                        }

                        return ret;
                    }
                    if (m) {
                        *nid = node;
                        break;
                    }
                }
            }

            if (m) {
                m->flags = HUGE_BOOTMEM_CMA;
                m->cma = cma;
            }

            return m;
        }
    } else {
        if (node_exact)
            m = memblock_alloc_exact_nid_raw(huge_page_size(h),
                huge_page_size(h), 0, MEMBLOCK_ALLOC_ACCESSIBLE, nid) {

                return memblock_alloc_internal(size, align, min_addr, max_addr, nid, true);
            }
        else {
            m = memblock_alloc_try_nid_raw(huge_page_size(h),
                huge_page_size(h), 0, MEMBLOCK_ALLOC_ACCESSIBLE, nid) {

                return memblock_alloc_internal(size, align, min_addr, max_addr, nid, false);
            }
            /* For pre-HVO to work correctly, pages need to be on
            * the list for the node they were actually allocated
            * from. That node may be different in the case of
            * fallback by memblock_alloc_try_nid_raw. So,
            * extract the actual node first. */
            if (m)
                listnode = early_pfn_to_nid(PHYS_PFN(virt_to_phys(m)));
        }

        if (m) {
            m->flags = 0;
            m->cma = NULL;
        }
    }

    if (m) {
        /* Use the beginning of the huge page to store the
        * huge_bootmem_page struct (until gather_bootmem
        * puts them into the mem_map).
        *
        * Put them into a private list first because mem_map
        * is not up yet. */
        INIT_LIST_HEAD(&m->list);
        list_add(&m->list, &huge_boot_pages[listnode]);
        m->hstate = h;
    }

    return m;
}
```

### hugetlb_pages_alloc_boot

```c
static unsigned long __init hugetlb_pages_alloc_boot(struct hstate *h)
{
    struct padata_mt_job job = {
        .fn_arg     = h,
        .align      = 1,
        .numa_aware = true
    };

    unsigned long jiffies_start;
    unsigned long jiffies_end;

    job.thread_fn   = hugetlb_pages_alloc_boot_node;
    job.start       = 0;
    job.size        = h->max_huge_pages;

    /* job.max_threads is 25% of the available cpu threads by default.
    *
    * On large servers with terabytes of memory, huge page allocation
    * can consume a considerably amount of time.
    *
    * Tests below show how long it takes to allocate 1 TiB of memory with 2MiB huge pages.
    * 2MiB huge pages. Using more threads can significantly improve allocation time.
    *
    * +-----------------------+-------+-------+-------+-------+-------+
    * | threads               |   8   |   16  |   32  |   64  |   128 |
    * +-----------------------+-------+-------+-------+-------+-------+
    * | skylake      144 cpus |   44s |   22s |   16s |   19s |   20s |
    * | cascade lake 192 cpus |   39s |   20s |   11s |   10s |    9s |
    * +-----------------------+-------+-------+-------+-------+-------+ */
    if (hugepage_allocation_threads == 0) {
        hugepage_allocation_threads = num_online_cpus() / 4;
        hugepage_allocation_threads = max(hugepage_allocation_threads, 1);
    }

    job.max_threads = hugepage_allocation_threads;
    job.min_chunk   = h->max_huge_pages / hugepage_allocation_threads;

    jiffies_start = jiffies;
    padata_do_multithreaded(&job);
    jiffies_end = jiffies;

    return h->nr_huge_pages;
}

static void __init hugetlb_pages_alloc_boot_node(unsigned long start, unsigned long end, void *arg)
{
    struct hstate *h = (struct hstate *)arg;
    int i, num = end - start;
    nodemask_t node_alloc_noretry;
    LIST_HEAD(folio_list);
    int next_node = first_online_node;

    /* Bit mask controlling how hard we retry per-node allocations.*/
    nodes_clear(node_alloc_noretry);

    for (i = 0; i < num; ++i) {
        struct folio *folio = alloc_pool_huge_folio(h, &node_states[N_MEMORY],
                        &node_alloc_noretry, &next_node) {
            gfp_t gfp_mask = htlb_alloc_mask(h) | __GFP_THISNODE;
            int nr_nodes, node;

            for_each_node_mask_to_alloc(next_node, nr_nodes, node, nodes_allowed) {
                struct folio *folio;

                folio = only_alloc_fresh_hugetlb_folio(h, gfp_mask, node,
                            nodes_allowed, node_alloc_noretry);
                    --->
                if (folio)
                    return folio;
            }

            return NULL;
        }
        if (!folio)
            break;

        list_move(&folio->lru, &folio_list);
        cond_resched();
    }

    prep_and_add_allocated_folios(h, &folio_list);
        --->
}

static struct folio *only_alloc_fresh_hugetlb_folio(struct hstate *h,
        gfp_t gfp_mask, int nid, nodemask_t *nmask,
        nodemask_t *node_alloc_noretry)
{
    struct folio *folio;

    if (hstate_is_gigantic(h))
        folio = alloc_gigantic_folio(h, gfp_mask, nid, nmask);
    else
        folio = alloc_buddy_hugetlb_folio(h, gfp_mask, nid, nmask, node_alloc_noretry);
    if (folio)
        init_new_hugetlb_folio(h, folio);
    return folio;
}
```

#### alloc_gigantic_folio

```c
static struct folio *alloc_gigantic_folio(struct hstate *h, gfp_t gfp_mask,
        int nid, nodemask_t *nodemask)
{
    struct folio *folio;
    int order = huge_page_order(h);
    bool retried = false;

    if (nid == NUMA_NO_NODE)
        nid = numa_mem_id();

retry:
    folio = hugetlb_cma_alloc_folio(h, gfp_mask, nid, nodemask) {
        int node;
        int order = huge_page_order(h);
        struct folio *folio = NULL;

        if (hugetlb_cma[nid])
            folio = cma_alloc_folio(hugetlb_cma[nid], order, gfp_mask);
        if (!folio && !(gfp_mask & __GFP_THISNODE)) {
            for_each_node_mask(node, *nodemask) {
                if (node == nid || !hugetlb_cma[node])
                    continue;

                folio = cma_alloc_folio(hugetlb_cma[node], order, gfp_mask);
                if (folio)
                    break;
            }
        }

        if (folio)
            folio_set_hugetlb_cma(folio);

        return folio;
    }
    if (!folio) {
        if (hugetlb_cma_exclusive_alloc()) /* return hugetlb_cma_only; */
            return NULL;

        folio = folio_alloc_gigantic(order, gfp_mask, nid, nodemask) {
            struct page *page;

            if (WARN_ON(!order || !(gfp & __GFP_COMP)))
                return NULL;

            page = alloc_contig_pages_noprof(1 << order, gfp, nid, node) {
                unsigned long ret, pfn, flags;
                struct zonelist *zonelist;
                struct zone *zone;
                struct zoneref *z;

                zonelist = node_zonelist(nid, gfp_mask);
                for_each_zone_zonelist_nodemask(zone, z, zonelist, gfp_zone(gfp_mask), nodemask) {
                    spin_lock_irqsave(&zone->lock, flags);

                    pfn = ALIGN(zone->zone_start_pfn, nr_pages);
                    while (zone_spans_last_pfn(zone, pfn, nr_pages)) {
                        if (pfn_range_valid_contig(zone, pfn, nr_pages)) {
                            /* We release the zone lock here because
                            * alloc_contig_range() will also lock the zone
                            * at some point. If there's an allocation
                            * spinning on this lock, it may win the race
                            * and cause alloc_contig_range() to fail... */
                            spin_unlock_irqrestore(&zone->lock, flags);
                            ret = __alloc_contig_pages(pfn, nr_pages, gfp_mask) ;
                                --->
                            if (!ret)
                                return pfn_to_page(pfn);
                            spin_lock_irqsave(&zone->lock, flags);
                        }
                        pfn += nr_pages;
                    }
                    spin_unlock_irqrestore(&zone->lock, flags);
                }
                return NULL;
            }

            return page ? page_folio(page) : NULL;
        }
        if (!folio)
            return NULL;
    }

    /* return page_ref_freeze(&folio->page, count) */
    if (folio_ref_freeze(folio, 1))
        return folio;

    pr_warn("HugeTLB: unexpected refcount on PFN %lu\n", folio_pfn(folio));
    hugetlb_free_folio(folio);

    if (!retried) {
        retried = true;
        goto retry;
    }
    return NULL;
}
```

##### __alloc_contig_pages

```c
ret = __alloc_contig_pages(pfn, nr_pages, gfp_mask) {
    unsigned long end_pfn = start_pfn + nr_pages;

    return alloc_contig_range_noprof(start_pfn, end_pfn, ACR_FLAGS_NONE, gfp_mask);
}

int alloc_contig_range_noprof(unsigned long start, unsigned long end,
                acr_flags_t alloc_flags, gfp_t gfp_mask)
{
    unsigned long outer_start, outer_end;
    int ret = 0;

    struct compact_control cc = {
        .nr_migratepages = 0,
        .order = -1,
        .zone = page_zone(pfn_to_page(start)),
        .mode = MIGRATE_SYNC,
        .ignore_skip_hint = true,
        .no_set_skip_hint = true,
        .alloc_contig = true,
    };
    INIT_LIST_HEAD(&cc.migratepages);
    enum pb_isolate_mode mode = (alloc_flags & ACR_FLAGS_CMA)
        ? PB_ISOLATE_MODE_CMA_ALLOC
        : PB_ISOLATE_MODE_OTHER;

    gfp_mask = current_gfp_context(gfp_mask);
    if (__alloc_contig_verify_gfp_mask(gfp_mask, (gfp_t *)&cc.gfp_mask))
        return -EINVAL;

    ret = start_isolate_page_range(start, end, mode);
        --->
    if (ret)
        goto done;

    drain_all_pages(cc.zone);

    ret = __alloc_contig_migrate_range(&cc, start, end);
        --->
    if (ret && ret != -EBUSY)
        goto done;

   /* https://lore.kernel.org/all/1734503588-16254-1-git-send-email-yangge1116@126.com/ */
    ret = replace_free_hugepage_folios(start, end) {
        struct folio *folio;
        int ret = 0;

        LIST_HEAD(isolate_list);

        while (start_pfn < end_pfn) {
            folio = pfn_folio(start_pfn);

            /* Not to disrupt normal path by vainly holding hugetlb_lock */
            if (folio_test_hugetlb(folio) && !folio_ref_count(folio)) {
                ret = alloc_and_dissolve_hugetlb_folio(folio, &isolate_list);
                    --->
                if (ret)
                    break;

                putback_movable_pages(&isolate_list) {
                    struct folio *folio;
                    struct folio *folio2;

                    list_for_each_entry_safe(folio, folio2, l, lru) {
                        if (unlikely(folio_test_hugetlb(folio))) {
                            folio_putback_hugetlb(folio) {
                                spin_lock_irq(&hugetlb_lock);
                                folio_set_hugetlb_migratable(folio);
                                list_move_tail(&folio->lru, &(folio_hstate(folio))->hugepage_activelist);
                                spin_unlock_irq(&hugetlb_lock);
                                folio_put(folio);
                            }
                            continue;
                        }
                        list_del(&folio->lru);
                        if (unlikely(page_has_movable_ops(&folio->page))) {
                            putback_movable_ops_page(&folio->page);
                        } else {
                            node_stat_mod_folio(folio, NR_ISOLATED_ANON +
                                    folio_is_file_lru(folio), -folio_nr_pages(folio));
                            folio_putback_lru(folio);
                        }
                    }
                }
            }
            start_pfn++;
        }

        return ret;
    }
    if (ret)
        goto done;

   /* align start to page order size
    * by looking for a buddy that straddles start_pfn */
    outer_start = find_large_buddy(start);

    /* Make sure the range is really isolated. */
    ret = test_pages_isolated(outer_start, end, mode) {
        unsigned long pfn, flags;
        struct page *page;
        struct zone *zone;
        int ret;

        /* wait for deferred freeing of hugetlb folios to be freed to buddy */
        wait_for_freed_hugetlb_folios();

       /* Note: pageblock_nr_pages != MAX_PAGE_ORDER. Then, chunks of free
        * pages are not aligned to pageblock_nr_pages.
        * Then we just check migratetype first. */
        for (pfn = start_pfn; pfn < end_pfn; pfn += pageblock_nr_pages) {
            page = __first_valid_page(pfn, pageblock_nr_pages);
            if (page && !is_migrate_isolate_page(page))
                break;
        }
        page = __first_valid_page(start_pfn, end_pfn - start_pfn);
        if ((pfn < end_pfn) || !page) {
            ret = -EBUSY;
            goto out;
        }

        /* Check all pages are free or marked as ISOLATED */
        zone = page_zone(page);
        spin_lock_irqsave(&zone->lock, flags);
        pfn = __test_page_isolated_in_pageblock(start_pfn, end_pfn, mode) {
            struct page *page;

            while (pfn < end_pfn) {
                page = pfn_to_page(pfn);
                if (PageBuddy(page))
                    pfn += 1 << buddy_order(page);
                else if ((mode == PB_ISOLATE_MODE_MEM_OFFLINE) && PageHWPoison(page))
                    pfn++;
                else if ((mode == PB_ISOLATE_MODE_MEM_OFFLINE) && PageOffline(page) && !page_count(page))
                    pfn++;
                else
                    break;
            }

            return pfn;
        }
        spin_unlock_irqrestore(&zone->lock, flags);

        ret = pfn < end_pfn ? -EBUSY : 0;

    out:
        trace_test_pages_isolated(start_pfn, end_pfn, pfn);

        return ret;
    }
    if (ret) {
        ret = -EBUSY;
        goto done;
    }

    /* Grab isolated pages from freelists. */
    outer_end = isolate_freepages_range(&cc, outer_start, end);
    if (!outer_end) {
        ret = -EBUSY;
        goto done;
    }

    if (!(gfp_mask & __GFP_COMP)) {
        split_free_pages(cc.freepages, gfp_mask);

        /* Free head and tail (if any) */
        if (start != outer_start)
            free_contig_range(outer_start, start - outer_start);
        if (end != outer_end)
            free_contig_range(end, outer_end - end);
    } else if (start == outer_start && end == outer_end && is_power_of_2(end - start)) {
        struct page *head = pfn_to_page(start);
        int order = ilog2(end - start);

        check_new_pages(head, order);
        prep_new_page(head, order, gfp_mask, 0);
        set_page_refcounted(head);
    } else {
        ret = -EINVAL;
        WARN(true, "PFN range: requested [%lu, %lu), allocated [%lu, %lu)\n",
            start, end, outer_start, outer_end);
    }
done:
    undo_isolate_page_range(start, end);
    return ret;
}
```

##### alloc_and_dissolve_hugetlb_folio

```c
int alloc_and_dissolve_hugetlb_folio(struct folio *old_folio,
            struct list_head *list)
{
    gfp_t gfp_mask;
    struct hstate *h;
    int nid = folio_nid(old_folio);
    struct folio *new_folio = NULL;
    int ret = 0;

retry:
    /* The old_folio might have been dissolved from under our feet, so make sure
     * to carefully check the state under the lock. */
    spin_lock_irq(&hugetlb_lock);
    if (!folio_test_hugetlb(old_folio)) {
        /* Freed from under us. Drop new_folio too. */
        goto free_new;
    } else if (folio_ref_count(old_folio)) { /* in-use hugetlb */
        bool isolated;

        /* Someone has grabbed the folio, try to isolate it here.
         * Fail with -EBUSY if not possible. */
        spin_unlock_irq(&hugetlb_lock);
        isolated = folio_isolate_hugetlb(old_folio, list) {
            bool ret = true;

            spin_lock_irq(&hugetlb_lock);
            if (!folio_test_hugetlb(folio) ||
                !folio_test_hugetlb_migratable(folio) ||
                !folio_try_get(folio)) {
                ret = false;
                goto unlock;
            }
            folio_clear_hugetlb_migratable(folio);
            list_move_tail(&folio->lru, list);
        unlock:
            spin_unlock_irq(&hugetlb_lock);
            return ret;
        }
        ret = isolated ? 0 : -EBUSY;
        spin_lock_irq(&hugetlb_lock);
        goto free_new;

    } else if (!folio_test_hugetlb_freed(old_folio)) {
        /* Folio's refcount is 0 but it has not been enqueued in the
         * freelist yet. Race window is small, so we can succeed here if
         * we retry. */
        spin_unlock_irq(&hugetlb_lock);
        cond_resched();
        goto retry;
    } else {
        h = folio_hstate(old_folio);
        if (!new_folio) {
            spin_unlock_irq(&hugetlb_lock);
            gfp_mask = htlb_alloc_mask(h) | __GFP_THISNODE;
            new_folio = alloc_buddy_hugetlb_folio(h, gfp_mask, nid, NULL, NULL);
                --->
            if (!new_folio)
                return -ENOMEM;
            __prep_new_hugetlb_folio(h, new_folio);
            goto retry;
        }

        /* Ok, old_folio is still a genuine free hugepage. Remove it from
         * the freelist and decrease the counters. These will be
         * incremented again when calling __prep_account_new_huge_page()
         * and enqueue_hugetlb_folio() for new_folio. The counters will
         * remain stable since this happens under the lock. */
        remove_hugetlb_folio(h, old_folio, false) {
            int nid = folio_nid(folio);

            VM_BUG_ON_FOLIO(hugetlb_cgroup_from_folio(folio), folio);
            VM_BUG_ON_FOLIO(hugetlb_cgroup_from_folio_rsvd(folio), folio);

            lockdep_assert_held(&hugetlb_lock);
            if (hstate_is_gigantic(h) && !gigantic_page_runtime_supported())
                return;

            list_del(&folio->lru);

            if (folio_test_hugetlb_freed(folio)) {
                folio_clear_hugetlb_freed(folio);
                h->free_huge_pages--;
                h->free_huge_pages_node[nid]--;
            }
            if (adjust_surplus) {
                h->surplus_huge_pages--;
                h->surplus_huge_pages_node[nid]--;
            }

            /* We can only clear the hugetlb flag after allocating vmemmap
            * pages.  Otherwise, someone (memory error handling) may try to write
            * to tail struct pages. */
            if (!folio_test_hugetlb_vmemmap_optimized(folio))
                __folio_clear_hugetlb(folio);

            h->nr_huge_pages--;
            h->nr_huge_pages_node[nid]--;
        }

        /* Ref count on new_folio is already zero as it was dropped
         * earlier.  It can be directly added to the pool free list. */
        __prep_account_new_huge_page(h, nid);
        enqueue_hugetlb_folio(h, new_folio);

        /* Folio has been replaced, we can safely free the old one. */
        spin_unlock_irq(&hugetlb_lock);
        update_and_free_hugetlb_folio(h, old_folio, false);
    }

    return ret;

free_new:
    spin_unlock_irq(&hugetlb_lock);
    if (new_folio)
        update_and_free_hugetlb_folio(h, new_folio, false);

    return ret;
}
```

#### alloc_buddy_hugetlb_folio

```c
static struct folio *alloc_buddy_hugetlb_folio(struct hstate *h,
        gfp_t gfp_mask, int nid, nodemask_t *nmask,
        nodemask_t *node_alloc_noretry)
{
    int order = huge_page_order(h);
    struct folio *folio;
    bool alloc_try_hard = true;

    /* By default we always try hard to allocate the folio with
    * __GFP_RETRY_MAYFAIL flag.  However, if we are allocating folios in
    * a loop (to adjust global huge page counts) and previous allocation
    * failed, do not continue to try hard on the same node.  Use the
    * node_alloc_noretry bitmap to manage this state information. */
    if (node_alloc_noretry && node_isset(nid, *node_alloc_noretry))
        alloc_try_hard = false;
    if (alloc_try_hard)
        gfp_mask |= __GFP_RETRY_MAYFAIL;
    if (nid == NUMA_NO_NODE)
        nid = numa_mem_id();

    folio = (struct folio *)__alloc_frozen_pages(gfp_mask, order, nid, nmask);

    /* If we did not specify __GFP_RETRY_MAYFAIL, but still got a
    * folio this indicates an overall state change.  Clear bit so
    * that we resume normal 'try hard' allocations. */
    if (node_alloc_noretry && folio && !alloc_try_hard)
        node_clear(nid, *node_alloc_noretry);

    /* If we tried hard to get a folio but failed, set bit so that
    * subsequent attempts will not try as hard until there is an
    * overall state change. */
    if (node_alloc_noretry && !folio && alloc_try_hard)
        node_set(nid, *node_alloc_noretry);

    if (!folio) {
        __count_vm_event(HTLB_BUDDY_PGALLOC_FAIL);
        return NULL;
    }

    __count_vm_event(HTLB_BUDDY_PGALLOC);
    return folio;
}
```

## alloc_hugetlb_folio

```c
struct folio *alloc_hugetlb_folio(struct vm_area_struct *vma,
                    unsigned long addr, bool cow_from_owner)
{
    struct hugepage_subpool *spool = subpool_vma(vma);
    struct hstate *h = hstate_vma(vma);
    struct folio *folio;
    long retval, gbl_chg, gbl_reserve;
    map_chg_state map_chg;
    int ret, idx;
    struct hugetlb_cgroup *h_cg = NULL;
    gfp_t gfp = htlb_alloc_mask(h) | __GFP_RETRY_MAYFAIL;

    idx = hstate_index(h);

    /* Whether we need a separate per-vma reservation? */
    if (cow_from_owner) {
        map_chg = MAP_CHG_ENFORCED; /* 2 */
    } else {
       /* A return code of zero indicates a reservation exists (no change). */
        retval = vma_needs_reservation(h, vma, addr);
        if (retval < 0)
            return ERR_PTR(-ENOMEM);
        map_chg = retval ? MAP_CHG_NEEDED/*1*/ : MAP_CHG_REUSE/*0*/;
    }

    /* Whether we need a separate global reservation?
    *
    * Processes that did not create the mapping will have no
    * reserves as indicated by the region/reserve map. Check
    * that the allocation will not exceed the subpool limit.
    * Or if it can get one from the pool reservation directly. */
    if (map_chg) {
        gbl_chg = hugepage_subpool_get_pages(spool, 1);
        if (gbl_chg < 0)
            goto out_end_reservation;
    } else {
        gbl_chg = 0;
    }

    /* If this allocation is not consuming a per-vma reservation,
    * charge the hugetlb cgroup now. */
    if (map_chg) {
        ret = hugetlb_cgroup_charge_cgroup_rsvd(idx, pages_per_huge_page(h), &h_cg);
        if (ret)
            goto out_subpool_put;
    }

    ret = hugetlb_cgroup_charge_cgroup(idx, pages_per_huge_page(h), &h_cg);
    if (ret)
        goto out_uncharge_cgroup_reservation;

    spin_lock_irq(&hugetlb_lock);
    /* glb_chg is passed to indicate whether or not a page must be taken
    * from the global free pool (global change).  gbl_chg == 0 indicates
    * a reservation exists for the allocation. */
    folio = dequeue_hugetlb_folio_vma(h, vma, addr, gbl_chg);
    if (!folio) {
        spin_unlock_irq(&hugetlb_lock);
        folio = alloc_buddy_hugetlb_folio_with_mpol(h, vma, addr);
        if (!folio)
            goto out_uncharge_cgroup;
        spin_lock_irq(&hugetlb_lock);
        list_add(&folio->lru, &h->hugepage_activelist);
        folio_ref_unfreeze(folio, 1);
        /* Fall through */
    }

    /* Either dequeued or buddy-allocated folio needs to add special
    * mark to the folio when it consumes a global reservation. */
    if (!gbl_chg) {
        folio_set_hugetlb_restore_reserve(folio);
        h->resv_huge_pages--;
    }

    hugetlb_cgroup_commit_charge(idx, pages_per_huge_page(h), h_cg, folio);
    /* If allocation is not consuming a reservation, also store the
    * hugetlb_cgroup pointer on the page. */
    if (map_chg) {
        hugetlb_cgroup_commit_charge_rsvd(idx, pages_per_huge_page(h), h_cg, folio);
    }

    spin_unlock_irq(&hugetlb_lock);

    hugetlb_set_folio_subpool(folio, spool) {
        folio->_hugetlb_subpool = subpool;
    }

    if (map_chg != MAP_CHG_ENFORCED) {
        /* commit() is only needed if the map_chg is not enforced */
        retval = vma_commit_reservation(h, vma, addr);

        if (unlikely(map_chg == MAP_CHG_NEEDED && retval == 0)) {
            long rsv_adjust;

            rsv_adjust = hugepage_subpool_put_pages(spool, 1);
            hugetlb_acct_memory(h, -rsv_adjust);
            if (map_chg) {
                spin_lock_irq(&hugetlb_lock);
                hugetlb_cgroup_uncharge_folio_rsvd(
                    hstate_index(h), pages_per_huge_page(h),
                    folio);
                spin_unlock_irq(&hugetlb_lock);
            }
        }
    }

    ret = mem_cgroup_charge_hugetlb(folio, gfp);
    /* Unconditionally increment NR_HUGETLB here. If it turns out that
    * mem_cgroup_charge_hugetlb failed, then immediately free the page and
    * decrement NR_HUGETLB. */
    lruvec_stat_mod_folio(folio, NR_HUGETLB, pages_per_huge_page(h));

    if (ret == -ENOMEM) {
        free_huge_folio(folio);
        return ERR_PTR(-ENOMEM);
    }

    return folio;

out_uncharge_cgroup:
    hugetlb_cgroup_uncharge_cgroup(idx, pages_per_huge_page(h), h_cg);
out_uncharge_cgroup_reservation:
    if (map_chg)
        hugetlb_cgroup_uncharge_cgroup_rsvd(idx, pages_per_huge_page(h), h_cg);
out_subpool_put:
    if (map_chg && !gbl_chg) {
        gbl_reserve = hugepage_subpool_put_pages(spool, 1);
        hugetlb_acct_memory(h, -gbl_reserve);
    }


out_end_reservation:
    if (map_chg != MAP_CHG_ENFORCED)
        vma_end_reservation(h, vma, addr);
    return ERR_PTR(-ENOSPC);
}
```

### dequeue_hugetlb_folio_vma

```c
static struct folio *dequeue_hugetlb_folio_vma(struct hstate *h,
                struct vm_area_struct *vma,
                unsigned long address, long gbl_chg)
{
    struct folio *folio = NULL;
    struct mempolicy *mpol;
    gfp_t gfp_mask;
    nodemask_t *nodemask;
    int nid;

    /* gbl_chg==1 means the allocation requires a new page that was not
    * reserved before.  Making sure there's at least one free page. */
    if (gbl_chg && !available_huge_pages(h))
        goto err;

    gfp_mask = htlb_alloc_mask(h);
    nid = huge_node(vma, address, gfp_mask, &mpol, &nodemask);

    if (mpol_is_preferred_many(mpol)) { /* (pol->mode == MPOL_PREFERRED_MANY) */
        folio = dequeue_hugetlb_folio_nodemask(h, gfp_mask, nid, nodemask);

        /* Fallback to all nodes if page==NULL */
        nodemask = NULL;
    }

    if (!folio) {
        folio = dequeue_hugetlb_folio_nodemask(h, gfp_mask, nid, nodemask) {
            unsigned int cpuset_mems_cookie;
            struct zonelist *zonelist;
            struct zone *zone;
            struct zoneref *z;
            int node = NUMA_NO_NODE;

            /* 'nid' should not be NUMA_NO_NODE. Try to catch any misuse of it and rectifiy. */
            if (nid == NUMA_NO_NODE)
                nid = numa_node_id();

            zonelist = node_zonelist(nid, gfp_mask);

        retry_cpuset:
            cpuset_mems_cookie = read_mems_allowed_begin();
            for_each_zone_zonelist_nodemask(zone, z, zonelist, gfp_zone(gfp_mask), nmask) {
                struct folio *folio;

                if (!cpuset_zone_allowed(zone, gfp_mask))
                    continue;
                /* no need to ask again on the same node. Pool is node rather than
                * zone aware */
                if (zone_to_nid(zone) == node)
                    continue;
                node = zone_to_nid(zone);

                folio = dequeue_hugetlb_folio_node_exact(h, node) {
                    struct folio *folio;
                    bool pin = !!(current->flags & PF_MEMALLOC_PIN);

                    lockdep_assert_held(&hugetlb_lock);
                    list_for_each_entry(folio, &h->hugepage_freelists[nid], lru) {
                        if (pin && !folio_is_longterm_pinnable(folio))
                            continue;

                        if (folio_test_hwpoison(folio))
                            continue;

                        if (is_migrate_isolate_page(&folio->page))
                            continue;

                        list_move(&folio->lru, &h->hugepage_activelist);
                        folio_ref_unfreeze(folio, 1);
                        folio_clear_hugetlb_freed(folio);
                        h->free_huge_pages--;
                        h->free_huge_pages_node[nid]--;
                        return folio;
                    }

                    return NULL;
                }
                if (folio)
                    return folio;
            }
            if (unlikely(read_mems_allowed_retry(cpuset_mems_cookie)))
                goto retry_cpuset;

            return NULL;
        }
    }

    mpol_cond_put(mpol);
    return folio;

err:
    return NULL;
}
```

### alloc_buddy_hugetlb_folio_with_mpol

```c
struct folio *alloc_buddy_hugetlb_folio_with_mpol(struct hstate *h,
        struct vm_area_struct *vma, unsigned long addr)
{
    struct folio *folio = NULL;
    struct mempolicy *mpol;
    gfp_t gfp_mask = htlb_alloc_mask(h);
    int nid;
    nodemask_t *nodemask;

    nid = huge_node(vma, addr, gfp_mask, &mpol, &nodemask);
    if (mpol_is_preferred_many(mpol)) { /* (pol->mode == MPOL_PREFERRED_MANY) */
        gfp_t gfp = gfp_mask & ~(__GFP_DIRECT_RECLAIM | __GFP_NOFAIL);

        folio = alloc_surplus_hugetlb_folio(h, gfp, nid, nodemask);

        /* Fallback to all nodes if page==NULL */
        nodemask = NULL;
    }

    if (!folio) {
        folio = alloc_surplus_hugetlb_folio(h, gfp_mask, nid, nodemask) {
            struct folio *folio = NULL;

            if (hstate_is_gigantic(h))
                return NULL;

            spin_lock_irq(&hugetlb_lock);
            if (h->surplus_huge_pages >= h->nr_overcommit_huge_pages)
                goto out_unlock;
            spin_unlock_irq(&hugetlb_lock);

            folio = only_alloc_fresh_hugetlb_folio(h, gfp_mask, nid, nmask, NULL) {
                if (hstate_is_gigantic(h))
                    folio = alloc_gigantic_folio(h, gfp_mask, nid, nmask);
                else
                    folio = alloc_buddy_hugetlb_folio(h, gfp_mask, nid, nmask, node_alloc_noretry);
            }

            if (!folio)
                return NULL;

            hugetlb_vmemmap_optimize_folio(h, folio);

            spin_lock_irq(&hugetlb_lock);
            /* nr_huge_pages needs to be adjusted within the same lock cycle
            * as surplus_pages, otherwise it might confuse
            * persistent_huge_pages() momentarily. */
            __prep_account_new_huge_page(h, folio_nid(folio));

            /* We could have raced with the pool size change.
            * Double check that and simply deallocate the new page
            * if we would end up overcommiting the surpluses. Abuse
            * temporary page to workaround the nasty free_huge_folio
            * codeflow */
            if (h->surplus_huge_pages >= h->nr_overcommit_huge_pages) {
                folio_set_hugetlb_temporary(folio);
                spin_unlock_irq(&hugetlb_lock);
                free_huge_folio(folio);
                return NULL;
            }

            h->surplus_huge_pages++;
            h->surplus_huge_pages_node[folio_nid(folio)]++;

        out_unlock:
            spin_unlock_irq(&hugetlb_lock);

            return folio;
        }
    }
    mpol_cond_put(mpol);
    return folio;
}
```

### hugetlb_acct_memory

```c
int hugetlb_acct_memory(struct hstate *h, long delta)
{
    int ret = -ENOMEM;

    if (!delta)
        return 0;

    spin_lock_irq(&hugetlb_lock);

    if (delta > 0) {
        if (gather_surplus_pages(h, delta) < 0)
            goto out;

        if (delta > allowed_mems_nr(h)) {
            return_unused_surplus_pages(h, delta);
            goto out;
        }
    }

    ret = 0;
    if (delta < 0)
        return_unused_surplus_pages(h, (unsigned long) -delta);

out:
    spin_unlock_irq(&hugetlb_lock);
    return ret;
}

int gather_surplus_pages(struct hstate *h, long delta)
    __must_hold(&hugetlb_lock)
{
    LIST_HEAD(surplus_list);
    struct folio *folio, *tmp;
    int ret;
    long i;
    long needed, allocated;
    bool alloc_ok = true;
    nodemask_t *mbind_nodemask, alloc_nodemask;

    mbind_nodemask = policy_mbind_nodemask(htlb_alloc_mask(h));
    if (mbind_nodemask)
        nodes_and(alloc_nodemask, *mbind_nodemask, cpuset_current_mems_allowed);
    else
        alloc_nodemask = cpuset_current_mems_allowed;

    lockdep_assert_held(&hugetlb_lock);
    needed = (h->resv_huge_pages + delta) - h->free_huge_pages;
    if (needed <= 0) {
        h->resv_huge_pages += delta;
        return 0;
    }

    allocated = 0;

    ret = -ENOMEM;
retry:
    spin_unlock_irq(&hugetlb_lock);
    for (i = 0; i < needed; i++) {
        folio = NULL;

        folio = alloc_surplus_hugetlb_folio(h, htlb_alloc_mask(h), NUMA_NO_NODE, &alloc_nodemask);
        if (!folio) {
            alloc_ok = false;
            break;
        }
        list_add(&folio->lru, &surplus_list);
        cond_resched();
    }
    allocated += i;

    spin_lock_irq(&hugetlb_lock);
    needed = (h->resv_huge_pages + delta) - (h->free_huge_pages + allocated);
    if (needed > 0) {
        if (alloc_ok)
            goto retry;
        goto free;
    }

    needed += allocated;
    h->resv_huge_pages += delta;
    ret = 0;

    /* Free the needed pages to the hugetlb pool */
    list_for_each_entry_safe(folio, tmp, &surplus_list, lru) {
        if ((--needed) < 0)
            break;
        /* Add the page to the hugetlb allocator */
        enqueue_hugetlb_folio(h, folio);
    }
free:
    spin_unlock_irq(&hugetlb_lock);

    /* Free unnecessary surplus pages to the buddy allocator.
     * Pages have no ref count, call free_huge_folio directly. */
    list_for_each_entry_safe(folio, tmp, &surplus_list, lru)
        free_huge_folio(folio);
    spin_lock_irq(&hugetlb_lock);

    return ret;
}
```

## free_huge_folio

```c
void free_huge_folio(struct folio *folio)
{
    /* Can't pass hstate in here because it is called from the
    * generic mm code. */
    struct hstate *h = folio_hstate(folio);
    int nid = folio_nid(folio);
    struct hugepage_subpool *spool = hugetlb_folio_subpool(folio);
    bool restore_reserve;
    unsigned long flags;

    VM_BUG_ON_FOLIO(folio_ref_count(folio), folio);
    VM_BUG_ON_FOLIO(folio_mapcount(folio), folio);

    hugetlb_set_folio_subpool(folio, NULL);
    if (folio_test_anon(folio))
        __ClearPageAnonExclusive(&folio->page);
    folio->mapping = NULL;
    restore_reserve = folio_test_hugetlb_restore_reserve(folio);
    folio_clear_hugetlb_restore_reserve(folio);

    if (!restore_reserve) {
        /* A return code of zero implies that the subpool will be
        * under its minimum size if the reservation is not restored
        * after page is free.  Therefore, force restore_reserve
        * operation. */
        ret = hugepage_subpool_put_pages(spool, 1) {
            long ret = delta;
            unsigned long flags;

            if (!spool)
                return delta;

            spin_lock_irqsave(&spool->lock, flags);

            if (spool->max_hpages != -1)        /* maximum size accounting */
                spool->used_hpages -= delta;

            /* minimum size accounting */
            if (spool->min_hpages != -1 && spool->used_hpages < spool->min_hpages) {
                if (spool->rsv_hpages + delta <= spool->min_hpages)
                    ret = 0;
                else
                    ret = spool->rsv_hpages + delta - spool->min_hpages;

                spool->rsv_hpages += delta;
                if (spool->rsv_hpages > spool->min_hpages)
                    spool->rsv_hpages = spool->min_hpages;
            }

            unlock_or_release_subpool(spool, flags) {
                spin_unlock_irqrestore(&spool->lock, irq_flags);

                if (subpool_is_free(spool)) {
                    if (spool->min_hpages != -1)
                        hugetlb_acct_memory(spool->hstate, -spool->min_hpages);
                    kfree(spool);
                }
            }

            return ret;
        }
        if (ret == 0)
            restore_reserve = true;
    }

    spin_lock_irqsave(&hugetlb_lock, flags);
    folio_clear_hugetlb_migratable(folio);
    hugetlb_cgroup_uncharge_folio(hstate_index(h), pages_per_huge_page(h), folio);
    hugetlb_cgroup_uncharge_folio_rsvd(hstate_index(h), pages_per_huge_page(h), folio);
    lruvec_stat_mod_folio(folio, NR_HUGETLB, -pages_per_huge_page(h));
    mem_cgroup_uncharge(folio);
    if (restore_reserve)
        h->resv_huge_pages++;

    if (folio_test_hugetlb_temporary(folio)) {
        remove_hugetlb_folio(h, folio, false);
        spin_unlock_irqrestore(&hugetlb_lock, flags);
        update_and_free_hugetlb_folio(h, folio, true);  /* 1. free to buddy */
    } else if (h->surplus_huge_pages_node[nid]) {
        /* remove the page from active list */
        remove_hugetlb_folio(h, folio, true);
            --->
        spin_unlock_irqrestore(&hugetlb_lock, flags);
        update_and_free_hugetlb_folio(h, folio, true);  /* 2. free to buddy */
            --->
    } else {
        arch_clear_hugetlb_flags(folio);
        enqueue_hugetlb_folio(h, folio);                /* 3. free hugetlb poll */
        spin_unlock_irqrestore(&hugetlb_lock, flags);
    }
}

void remove_hugetlb_folio(struct hstate *h, struct folio *folio,
                            bool adjust_surplus)
{
    int nid = folio_nid(folio);

    VM_BUG_ON_FOLIO(hugetlb_cgroup_from_folio(folio), folio);
    VM_BUG_ON_FOLIO(hugetlb_cgroup_from_folio_rsvd(folio), folio);

    lockdep_assert_held(&hugetlb_lock);
    if (hstate_is_gigantic(h) && !gigantic_page_runtime_supported())
        return;

    list_del(&folio->lru);

    if (folio_test_hugetlb_freed(folio)) {
        folio_clear_hugetlb_freed(folio);
        h->free_huge_pages--;
        h->free_huge_pages_node[nid]--;
    }
    if (adjust_surplus) {
        h->surplus_huge_pages--;
        h->surplus_huge_pages_node[nid]--;
    }

    /* We can only clear the hugetlb flag after allocating vmemmap
     * pages.  Otherwise, someone (memory error handling) may try to write
     * to tail struct pages. */
    if (!folio_test_hugetlb_vmemmap_optimized(folio))
        __folio_clear_hugetlb(folio);

    h->nr_huge_pages--;
    h->nr_huge_pages_node[nid]--;
}
```

### update_and_free_hugetlb_folio

```c
static void update_and_free_hugetlb_folio(struct hstate *h, struct folio *folio,
                 bool atomic)
{
    if (!folio_test_hugetlb_vmemmap_optimized(folio) || !atomic) {
        __update_and_free_hugetlb_folio(h, folio);
        return;
    }

    /* Defer freeing to avoid using GFP_ATOMIC to allocate vmemmap pages.
     *
     * Only call schedule_work() if hpage_freelist is previously
     * empty. Otherwise, schedule_work() had been called but the workfn
     * hasn't retrieved the list yet. */
    if (llist_add((struct llist_node *)&folio->mapping, &hpage_freelist))
        schedule_work(&free_hpage_work);
}
```

```c
static LLIST_HEAD(hpage_freelist);
static DECLARE_WORK(free_hpage_work, free_hpage_workfn);

static void free_hpage_workfn(struct work_struct *work)
{
    struct llist_node *node;

    node = llist_del_all(&hpage_freelist);

    while (node) {
        struct folio *folio;
        struct hstate *h;

        folio = container_of((struct address_space **)node, struct folio, mapping);
        node = node->next;
        folio->mapping = NULL;
        /* The VM_BUG_ON_FOLIO(!folio_test_hugetlb(folio), folio) in
         * folio_hstate() is going to trigger because a previous call to
         * remove_hugetlb_folio() will clear the hugetlb bit, so do
         * not use folio_hstate() directly. */
        h = size_to_hstate(folio_size(folio));

        __update_and_free_hugetlb_folio(h, folio) {
            bool clear_flag = folio_test_hugetlb_vmemmap_optimized(folio);

            if (hstate_is_gigantic(h) && !gigantic_page_runtime_supported())
                return;

            /* If we don't know which subpages are hwpoisoned, we can't free
            * the hugepage, so it's leaked intentionally. */
            if (folio_test_hugetlb_raw_hwp_unreliable(folio))
                return;

            /* If folio is not vmemmap optimized (!clear_flag), then the folio
            * is no longer identified as a hugetlb page.  hugetlb_vmemmap_restore_folio
            * can only be passed hugetlb pages and will BUG otherwise. */
            if (clear_flag && hugetlb_vmemmap_restore_folio(h, folio)) {
                spin_lock_irq(&hugetlb_lock);
                /* If we cannot allocate vmemmap pages, just refuse to free the
                * page and put the page back on the hugetlb free list and treat
                * as a surplus page. */
                add_hugetlb_folio(h, folio, true) {
                    int nid = folio_nid(folio);

                    INIT_LIST_HEAD(&folio->lru);
                    h->nr_huge_pages++;
                    h->nr_huge_pages_node[nid]++;

                    if (adjust_surplus) {
                        h->surplus_huge_pages++;
                        h->surplus_huge_pages_node[nid]++;
                    }

                    __folio_set_hugetlb(folio);
                    folio_change_private(folio, NULL);
                    /* We have to set hugetlb_vmemmap_optimized again as above
                    * folio_change_private(folio, NULL) cleared it. */
                    folio_set_hugetlb_vmemmap_optimized(folio);

                    arch_clear_hugetlb_flags(folio);
                    enqueue_hugetlb_folio(h, folio);
                }
                spin_unlock_irq(&hugetlb_lock);
                return;
            }

            /* If vmemmap pages were allocated above, then we need to clear the
            * hugetlb flag under the hugetlb lock. */
            if (folio_test_hugetlb(folio)) {
                spin_lock_irq(&hugetlb_lock);
                __folio_clear_hugetlb(folio);
                spin_unlock_irq(&hugetlb_lock);
            }

            /* Move PageHWPoison flag from head page to the raw error pages,
            * which makes any healthy subpages reusable. */
            if (unlikely(folio_test_hwpoison(folio)))
                folio_clear_hugetlb_hwpoison(folio);

            folio_ref_unfreeze(folio, 1);

            hugetlb_free_folio(folio) {
                if (folio_test_hugetlb_cma(folio)) {
                    hugetlb_cma_free_folio(folio) {
                        int nid = folio_nid(folio);

                        WARN_ON_ONCE(!cma_free_folio(hugetlb_cma[nid], folio));
                    }
                    return;
                }

                folio_put(folio) {
                    if (folio_put_testzero(folio)) {
                        __folio_put(folio) {
                            if (unlikely(folio_is_zone_device(folio))) {
                                free_zone_device_folio(folio);
                                return;
                            }

                            if (folio_test_hugetlb(folio)) {
                                free_huge_folio(folio);
                                return;
                            }

                            page_cache_release(folio);
                            folio_unqueue_deferred_split(folio);
                            mem_cgroup_uncharge(folio);
                            free_frozen_pages(&folio->page, folio_order(folio));
                        }
                    }
                }
            }
        }

        cond_resched();
    }
}
```

## khugepaged

```c
/* default scan 8*512 pte (or vmas) every 30 second */
static unsigned int khugepaged_pages_to_scan __read_mostly;
static unsigned int khugepaged_pages_collapsed;
static unsigned int khugepaged_full_scans;
static unsigned int khugepaged_scan_sleep_millisecs __read_mostly = 10000;
/* during fragmentation poll the hugepage allocator once every minute */
static unsigned int khugepaged_alloc_sleep_millisecs __read_mostly = 60000;
static unsigned long khugepaged_sleep_expire;

static DEFINE_SPINLOCK(khugepaged_mm_lock);
static DECLARE_WAIT_QUEUE_HEAD(khugepaged_wait);

static DEFINE_READ_MOSTLY_HASHTABLE(mm_slots_hash, MM_SLOTS_HASH_BITS);


struct khugepaged_scan {
    struct list_head            mm_head;
    struct khugepaged_mm_slot   *mm_slot;
    unsigned long               address;
};

struct khugepaged_mm_slot {
    struct mm_slot slot;
};

struct mm_slot {
    struct hlist_node   hash;
    struct list_head    mm_node;
    struct mm_struct    *mm;
};

int __init khugepaged_init(void)
{
    mm_slot_cache = KMEM_CACHE(khugepaged_mm_slot, 0);
    if (!mm_slot_cache)
        return -ENOMEM;

    khugepaged_pages_to_scan = HPAGE_PMD_NR * 8;
    khugepaged_max_ptes_none = HPAGE_PMD_NR - 1;
    khugepaged_max_ptes_swap = HPAGE_PMD_NR / 8;
    khugepaged_max_ptes_shared = HPAGE_PMD_NR / 2;

    return 0;
}
```

```c
void khugepaged_enter_vma(struct vm_area_struct *vma,
              vm_flags_t vm_flags)
{
    if (!test_bit(MMF_VM_HUGEPAGE, &vma->vm_mm->flags) && hugepage_pmd_enabled()) {
        if (thp_vma_allowable_order(vma, vm_flags, TVA_ENFORCE_SYSFS, PMD_ORDER)) {

            __khugepaged_enter(vma->vm_mm) {
                struct khugepaged_mm_slot *mm_slot;
                struct mm_slot *slot;
                int wakeup;

                /* __khugepaged_exit() must not run from under us */
                VM_BUG_ON_MM(hpage_collapse_test_exit(mm), mm);
                if (unlikely(test_and_set_bit(MMF_VM_HUGEPAGE, &mm->flags)))
                    return;

                mm_slot = mm_slot_alloc(mm_slot_cache);
                if (!mm_slot)
                    return;

                slot = &mm_slot->slot;

                spin_lock(&khugepaged_mm_lock);
                mm_slot_insert(mm_slots_hash, mm, slot);
                /* Insert just behind the scanning cursor, to let the area settle
                * down a little. */
                wakeup = list_empty(&khugepaged_scan.mm_head);
                list_add_tail(&slot->mm_node, &khugepaged_scan.mm_head);
                spin_unlock(&khugepaged_mm_lock);

                mmgrab(mm);
                if (wakeup)
                    wake_up_interruptible(&khugepaged_wait);
            }
        }
    }
}

static int khugepaged(void *none)
{
    struct khugepaged_mm_slot *mm_slot;

    set_freezable();
    set_user_nice(current, MAX_NICE);

    while (!kthread_should_stop()) {
        khugepaged_do_scan(&khugepaged_collapse_control);

        khugepaged_wait_work() {
            if (khugepaged_has_work()) {
                const unsigned long scan_sleep_jiffies =
                    msecs_to_jiffies(khugepaged_scan_sleep_millisecs);

                if (!scan_sleep_jiffies)
                    return;

                khugepaged_sleep_expire = jiffies + scan_sleep_jiffies;
                wait_event_freezable_timeout(khugepaged_wait,
                                khugepaged_should_wakeup(),
                                scan_sleep_jiffies);
                return;
            }

            if (hugepage_pmd_enabled())
                wait_event_freezable(khugepaged_wait, khugepaged_wait_event());
        }
    }

    spin_lock(&khugepaged_mm_lock);
    mm_slot = khugepaged_scan.mm_slot;
    khugepaged_scan.mm_slot = NULL;
    if (mm_slot){
        collect_mm_slot(mm_slot) {
            struct mm_slot *slot = &mm_slot->slot;
            struct mm_struct *mm = slot->mm;

            lockdep_assert_held(&khugepaged_mm_lock);

            if (hpage_collapse_test_exit(mm)) { /* return atomic_read(&mm->mm_users) == 0; */
                /* free mm_slot */
                hash_del(&slot->hash);
                list_del(&slot->mm_node);

                mm_slot_free(mm_slot_cache, mm_slot) {
                    kmem_cache_free(cache, objp);
                }
                mmdrop(mm);
            }
        }
    }
    spin_unlock(&khugepaged_mm_lock);
    return 0;
}

static void khugepaged_do_scan(struct collapse_control *cc)
{
    unsigned int progress = 0, pass_through_head = 0;
    unsigned int pages = READ_ONCE(khugepaged_pages_to_scan);
    bool wait = true;
    int result = SCAN_SUCCEED;

    lru_add_drain_all();

    while (true) {
        cond_resched();

        if (unlikely(kthread_should_stop()))
            break;

        spin_lock(&khugepaged_mm_lock);
        if (!khugepaged_scan.mm_slot)
            pass_through_head++;

        if (khugepaged_has_work() && pass_through_head < 2)
            progress += khugepaged_scan_mm_slot(pages - progress, &result, cc);
        else
            progress = pages;
        spin_unlock(&khugepaged_mm_lock);

        if (progress >= pages)
            break;

        if (result == SCAN_ALLOC_HUGE_PAGE_FAIL) {
            /* If fail to allocate the first time, try to sleep for
            * a while.  When hit again, cancel the scan. */
            if (!wait)
                break;
            wait = false;
            khugepaged_alloc_sleep() {
                DEFINE_WAIT(wait);

                add_wait_queue(&khugepaged_wait, &wait);
                __set_current_state(TASK_INTERRUPTIBLE|TASK_FREEZABLE);
                schedule_timeout(msecs_to_jiffies(khugepaged_alloc_sleep_millisecs));
                remove_wait_queue(&khugepaged_wait, &wait);
            }
        }
    }
}

static unsigned int khugepaged_scan_mm_slot(unsigned int pages, int *result,
                        struct collapse_control *cc)
    __releases(&khugepaged_mm_lock)
    __acquires(&khugepaged_mm_lock)
{
    struct vma_iterator vmi;
    struct khugepaged_mm_slot *mm_slot;
    struct mm_slot *slot;
    struct mm_struct *mm;
    struct vm_area_struct *vma;
    int progress = 0;

    VM_BUG_ON(!pages);
    lockdep_assert_held(&khugepaged_mm_lock);
    *result = SCAN_FAIL;

    if (khugepaged_scan.mm_slot) {
        mm_slot = khugepaged_scan.mm_slot;
        slot = &mm_slot->slot;
    } else {
        slot = list_entry(khugepaged_scan.mm_head.next, struct mm_slot, mm_node);
        mm_slot = mm_slot_entry(slot, struct khugepaged_mm_slot, slot);
        khugepaged_scan.address = 0;
        khugepaged_scan.mm_slot = mm_slot;
    }
    spin_unlock(&khugepaged_mm_lock);

    mm = slot->mm;
    /* Don't wait for semaphore (to avoid long wait times).  Just move to
    * the next mm on the list. */
    vma = NULL;
    if (unlikely(!mmap_read_trylock(mm)))
        goto breakouterloop_mmap_lock;

    progress++;
    if (unlikely(hpage_collapse_test_exit_or_disable(mm)))
        goto breakouterloop;

    vma_iter_init(&vmi, mm, khugepaged_scan.address);
    for_each_vma(vmi, vma) {
        unsigned long hstart, hend;

        cond_resched();
        if (unlikely(hpage_collapse_test_exit_or_disable(mm))) {
            progress++;
            break;
        }
        if (!thp_vma_allowable_order(vma, vma->vm_flags, TVA_ENFORCE_SYSFS, PMD_ORDER)) {
skip:
            progress++;
            continue;
        }
        hstart = round_up(vma->vm_start, HPAGE_PMD_SIZE);
        hend = round_down(vma->vm_end, HPAGE_PMD_SIZE);
        if (khugepaged_scan.address > hend)
            goto skip;
        if (khugepaged_scan.address < hstart)
            khugepaged_scan.address = hstart;
        VM_BUG_ON(khugepaged_scan.address & ~HPAGE_PMD_MASK);

        while (khugepaged_scan.address < hend) {
            bool mmap_locked = true;

            cond_resched();
            if (unlikely(hpage_collapse_test_exit_or_disable(mm)))
                goto breakouterloop;

            if (!vma_is_anonymous(vma)) {
                struct file *file = get_file(vma->vm_file);
                pgoff_t pgoff = linear_page_index(vma, khugepaged_scan.address);

                mmap_read_unlock(mm);
                mmap_locked = false;
                *result = hpage_collapse_scan_file(mm, khugepaged_scan.address, file, pgoff, cc);
                fput(file);

                if (*result == SCAN_PTE_MAPPED_HUGEPAGE) {
                    mmap_read_lock(mm);
                    ret = hpage_collapse_test_exit_or_disable(mm) {
                        ret = hpage_collapse_test_exit(mm) {
                            return atomic_read(&mm->mm_users) == 0;
                        }
                        return ret || test_bit(MMF_DISABLE_THP, &mm->flags);
                    }
                    if (ret)
                        goto breakouterloop;

                    *result = collapse_pte_mapped_thp(mm, khugepaged_scan.address, false);
                    if (*result == SCAN_PMD_MAPPED)
                        *result = SCAN_SUCCEED;
                    mmap_read_unlock(mm);
                }
            } else {
                *result = hpage_collapse_scan_pmd(mm, vma,
                    khugepaged_scan.address, &mmap_locked, cc);
            }

            if (*result == SCAN_SUCCEED)
                ++khugepaged_pages_collapsed;

            /* move to next address */
            khugepaged_scan.address += HPAGE_PMD_SIZE;
            progress += HPAGE_PMD_NR;
            if (!mmap_locked)
                /* We released mmap_lock so break loop.  Note
                * that we drop mmap_lock before all hugepage
                * allocations, so if allocation fails, we are
                * guaranteed to break here and report the
                * correct result back to caller. */
                goto breakouterloop_mmap_lock;
            if (progress >= pages)
                goto breakouterloop;
        }
    }

breakouterloop:
    mmap_read_unlock(mm); /* exit_mmap will destroy ptes after this */
breakouterloop_mmap_lock:

    spin_lock(&khugepaged_mm_lock);
    VM_BUG_ON(khugepaged_scan.mm_slot != mm_slot);
    /* Release the current mm_slot if this mm is about to die, or
    * if we scanned all vmas of this mm. */
    if (hpage_collapse_test_exit(mm) || !vma) {
        /* Make sure that if mm_users is reaching zero while
        * khugepaged runs here, khugepaged_exit will find
        * mm_slot not pointing to the exiting mm. */
        if (slot->mm_node.next != &khugepaged_scan.mm_head) {
            slot = list_entry(slot->mm_node.next, struct mm_slot, mm_node);
            khugepaged_scan.mm_slot =
                mm_slot_entry(slot, struct khugepaged_mm_slot, slot);
            khugepaged_scan.address = 0;
        } else {
            khugepaged_scan.mm_slot = NULL;
            khugepaged_full_scans++;
        }

        collect_mm_slot(mm_slot);
    }

    return progress;
}
```

### hpage_collapse_scan_file

```c
int hpage_collapse_scan_file(struct mm_struct *mm, unsigned long addr,
                    struct file *file, pgoff_t start,
                    struct collapse_control *cc)
{
    struct folio *folio = NULL;
    struct address_space *mapping = file->f_mapping;
    XA_STATE(xas, &mapping->i_pages, start);
    int present, swap;
    int node = NUMA_NO_NODE;
    int result = SCAN_SUCCEED;

    present = 0;
    swap = 0;
    memset(cc->node_load, 0, sizeof(cc->node_load));
    nodes_clear(cc->alloc_nmask);

/* 2. Main Loop: Scanning the Page Cache */
    rcu_read_lock();
    xas_for_each(&xas, folio, start + HPAGE_PMD_NR - 1) {
        if (xas_retry(&xas, folio))
            continue;

        /* 2.1. Handling Swap Entries */
        if (xa_is_value(folio)) {
            swap += 1 << xas_get_order(&xas);
            if (cc->is_khugepaged && swap > khugepaged_max_ptes_swap) {
                result = SCAN_EXCEED_SWAP_PTE;
                count_vm_event(THP_SCAN_EXCEED_SWAP_PTE);
                break;
            }
            continue;
        }

        if (!folio_try_get(folio)) {
            xas_reset(&xas);
            continue;
        }

        if (unlikely(folio != xas_reload(&xas))) {
            folio_put(folio);
            xas_reset(&xas);
            continue;
        }

        if (folio_order(folio) == HPAGE_PMD_ORDER && folio->index == start) {
            /* Maybe PMD-mapped */
            result = SCAN_PTE_MAPPED_HUGEPAGE;
            /* For SCAN_PTE_MAPPED_HUGEPAGE, further processing
             * by the caller won't touch the page cache, and so
             * it's safe to skip LRU and refcount checks before
             * returning. */
            folio_put(folio);
            break;
        }

        node = folio_nid(folio);
        if (hpage_collapse_scan_abort(node, cc)) {
            result = SCAN_SCAN_ABORT;
            folio_put(folio);
            break;
        }
        cc->node_load[node]++;

        if (!folio_test_lru(folio)) {
            result = SCAN_PAGE_LRU;
            folio_put(folio);
            break;
        }

        /* adding 1 for the reference taken by folio_try_get. */
        if (folio_expected_ref_count(folio) + 1 != folio_ref_count(folio)) {
            result = SCAN_PAGE_COUNT;
            folio_put(folio);
            break;
        }

        /* We probably should check if the folio is referenced
         * here, but nobody would transfer pte_young() to
         * folio_test_referenced() for us.  And rmap walk here
         * is just too costly... */

        present += folio_nr_pages(folio);
        folio_put(folio);

        if (need_resched()) {
            xas_pause(&xas);
            cond_resched_rcu();
        }
    }
    rcu_read_unlock();

    if (result == SCAN_SUCCEED) {
        if (cc->is_khugepaged &&
            present < HPAGE_PMD_NR - khugepaged_max_ptes_none) {
            result = SCAN_EXCEED_NONE_PTE;
            count_vm_event(THP_SCAN_EXCEED_NONE_PTE);
        } else {
            result = collapse_file(mm, addr, file, start, cc);
        }
    }

    trace_mm_khugepaged_scan_file(mm, folio, file, present, swap, result);
    return result;
}
```

#### collpase_file

```c
/* collapse filemap/tmpfs/shmem pages into huge one.
 *
 * Basic scheme is simple, details are more complex:
 *  - allocate and lock a new huge page;
 *  - scan page cache, locking old pages
 *    + swap/gup in pages if necessary;
 *  - copy data to new page
 *  - handle shmem holes
 *    + re-validate that holes weren't filled by someone else
 *    + check for userfaultfd
 *  - finalize updates to the page cache;
 *  - if replacing succeeds:
 *    + unlock huge page;
 *    + free old pages;
 *  - if replacing failed;
 *    + unlock old pages
 *    + unlock and free huge page; */
static int collapse_file(struct mm_struct *mm, unsigned long addr,
             struct file *file, pgoff_t start,
             struct collapse_control *cc)
{
    struct address_space *mapping = file->f_mapping;
    struct page *dst;
    struct folio *folio, *tmp, *new_folio;
    pgoff_t index = 0, end = start + HPAGE_PMD_NR;
    LIST_HEAD(pagelist);
    XA_STATE_ORDER(xas, &mapping->i_pages, start, HPAGE_PMD_ORDER);
    int nr_none = 0, result = SCAN_SUCCEED;
    bool is_shmem = shmem_file(file);

    VM_BUG_ON(!IS_ENABLED(CONFIG_READ_ONLY_THP_FOR_FS) && !is_shmem);
    VM_BUG_ON(start & (HPAGE_PMD_NR - 1));

/* CF.2 Allocating the New Huge Folio */
    result = alloc_charge_folio(&new_folio, mm, cc);
    if (result != SCAN_SUCCEED)
        goto out;

/* CF.3. Preparing the New Folio */
    mapping_set_update(&xas, mapping);

    __folio_set_locked(new_folio);
    if (is_shmem)
        __folio_set_swapbacked(new_folio);
    new_folio->index = start;
    new_folio->mapping = mapping;

/* CF.4. Ensuring XArray Slots */
    /* Ensure we have slots for all the pages in the range.  This is
     * almost certainly a no-op because most of the pages must be present */
    do {
        xas_lock_irq(&xas);
        xas_create_range(&xas);
        if (!xas_error(&xas))
            break;
        xas_unlock_irq(&xas);
        if (!xas_nomem(&xas, GFP_KERNEL)) {
            result = SCAN_FAIL;
            goto rollback;
        }
    } while (1);

/* CF.5. Main Loop: Collecting Source Folios */
    for (index = start; index < end;) {
        xas_set(&xas, index);
        folio = xas_load(&xas);

        VM_BUG_ON(index != xas.xa_index);
        if (is_shmem) {
            if (!folio) {
                /* Stop if extent has been truncated or
                 * hole-punched, and is now completely
                 * empty. */
                if (index == start) {
                    if (!xas_next_entry(&xas, end - 1)) {
                        result = SCAN_TRUNCATED;
                        goto xa_locked;
                    }
                }
                nr_none++;
                index++;
                continue;
            }

            if (xa_is_value(folio) || !folio_test_uptodate(folio)) {
                xas_unlock_irq(&xas);
                /* swap in or instantiate fallocated page */
                if (shmem_get_folio(mapping->host, index, 0, &folio, SGP_NOALLOC)) {
                    result = SCAN_FAIL;
                    goto xa_unlocked;
                }
                /* drain lru cache to help folio_isolate_lru() */
                lru_add_drain();
            } else if (folio_trylock(folio)) {
                folio_get(folio);
                xas_unlock_irq(&xas);
            } else {
                result = SCAN_PAGE_LOCK;
                goto xa_locked;
            }
        } else {    /* !is_shmem */
            if (!folio || xa_is_value(folio)) {
                xas_unlock_irq(&xas);
                page_cache_sync_readahead(mapping, &file->f_ra, file, index, end - index);
                /* drain lru cache to help folio_isolate_lru() */
                lru_add_drain();
                folio = filemap_lock_folio(mapping, index);
                if (IS_ERR(folio)) {
                    result = SCAN_FAIL;
                    goto xa_unlocked;
                }
            } else if (folio_test_dirty(folio)) {
                /* khugepaged only works on **read-only fd**,
                 * so this page is dirty because it hasn't
                 * been flushed since first write. There
                 * won't be new dirty pages.
                 *
                 * Trigger async flush here and hope the
                 * writeback is done when khugepaged
                 * revisits this page.
                 *
                 * This is a one-off situation. We are not
                 * forcing writeback in loop. */
                xas_unlock_irq(&xas);
                filemap_flush(mapping);
                result = SCAN_FAIL;
                goto xa_unlocked;
            } else if (folio_test_writeback(folio)) {
                xas_unlock_irq(&xas);
                result = SCAN_FAIL;
                goto xa_unlocked;
            } else if (folio_trylock(folio)) {
                folio_get(folio);
                xas_unlock_irq(&xas);
            } else {
                result = SCAN_PAGE_LOCK;
                goto xa_locked;
            }
        }

        /* The folio must be locked, so we can drop the i_pages lock
         * without racing with truncate. */
        VM_BUG_ON_FOLIO(!folio_test_locked(folio), folio);

        /* make sure the folio is up to date */
        if (unlikely(!folio_test_uptodate(folio))) {
            result = SCAN_FAIL;
            goto out_unlock;
        }

        /* If file was truncated then extended, or hole-punched, before
         * we locked the first folio, then a THP might be there already.
         * This will be discovered on the first iteration. */
        if (folio_order(folio) == HPAGE_PMD_ORDER && folio->index == start) {
            /* Maybe PMD-mapped */
            result = SCAN_PTE_MAPPED_HUGEPAGE;
            goto out_unlock;
        }

        if (folio_mapping(folio) != mapping) {
            result = SCAN_TRUNCATED;
            goto out_unlock;
        }

        if (!is_shmem && (folio_test_dirty(folio) || folio_test_writeback(folio))) {
            /* khugepaged only works on read-only fd, so this
             * folio is dirty because it hasn't been flushed
             * since first write. */
            result = SCAN_FAIL;
            goto out_unlock;
        }

        if (!folio_isolate_lru(folio)) {
            result = SCAN_DEL_PAGE_LRU;
            goto out_unlock;
        }

        if (!filemap_release_folio(folio, GFP_KERNEL)) {
            result = SCAN_PAGE_HAS_PRIVATE;
            folio_putback_lru(folio);
            goto out_unlock;
        }

        if (folio_mapped(folio))
            try_to_unmap(folio, TTU_IGNORE_MLOCK | TTU_BATCH_FLUSH);

        xas_lock_irq(&xas);

        VM_BUG_ON_FOLIO(folio != xa_load(xas.xa, index), folio);

        /* We control 2 + nr_pages references to the folio:
         *  - we hold a pin on it;
         *  - nr_pages reference from page cache;
         *  - one from lru_isolate_folio;
         * If those are the only references, then any new usage
         * of the folio will have to fetch it from the page
         * cache. That requires locking the folio to handle
         * truncate, so any new usage will be blocked until we
         * unlock folio after collapse/during rollback. */
        if (folio_ref_count(folio) != 2 + folio_nr_pages(folio)) {
            result = SCAN_PAGE_COUNT;
            xas_unlock_irq(&xas);
            folio_putback_lru(folio);
            goto out_unlock;
        }

        /* Accumulate the folios that are being collapsed. */
        list_add_tail(&folio->lru, &pagelist);
        index += folio_nr_pages(folio);

        continue;

out_unlock:
        folio_unlock(folio);
        folio_put(folio);
        goto xa_unlocked;
    }

/* CF.6. Post-Loop Checks for Non-Shmem */
    if (!is_shmem) {
        filemap_nr_thps_inc(mapping);
        /* Paired with the fence in do_dentry_open() -> get_write_access()
         * to ensure i_writecount is up to date and the update to nr_thps
         * is visible. Ensures the page cache will be truncated if the
         * file is opened writable. */
        smp_mb();
        if (inode_is_open_for_write(mapping->host)) {
            result = SCAN_FAIL;
            filemap_nr_thps_dec(mapping);
        }
    }

xa_locked:
    xas_unlock_irq(&xas);
xa_unlocked:

/* CF.7. Flush and Charge Check */

    /* If collapse is successful, flush must be done now before copying.
     * If collapse is unsuccessful, does flush actually need to be done?
     * Do it anyway, to clear the state. */
    try_to_unmap_flush();

    if (result == SCAN_SUCCEED && nr_none && !shmem_charge(mapping->host, nr_none))
        result = SCAN_FAIL;
    if (result != SCAN_SUCCEED) {
        nr_none = 0;
        goto rollback;
    }

/* CF.8. Copying Data to New Folio
 * The old folios are locked, so they won't change anymore. */

    index = start;
    dst = folio_page(new_folio, 0);
    list_for_each_entry(folio, &pagelist, lru) {
        int i, nr_pages = folio_nr_pages(folio);

        while (index < folio->index) {
            clear_highpage(dst);
            index++;
            dst++;
        }

        for (i = 0; i < nr_pages; i++) {
            if (copy_mc_highpage(dst, folio_page(folio, i)) > 0) {
                result = SCAN_COPY_MC;
                goto rollback;
            }
            index++;
            dst++;
        }
    }
    while (index < end) {
        clear_highpage(dst);
        index++;
        dst++;
    }

/* CF.9. Handling Holes (If nr_none > 0) */
    if (nr_none) {
        struct vm_area_struct *vma;
        int nr_none_check = 0;

        i_mmap_lock_read(mapping);
        xas_lock_irq(&xas);

        xas_set(&xas, start);
        for (index = start; index < end; index++) {
            if (!xas_next(&xas)) {
                /* For holes, inserts XA_RETRY_ENTRY placeholders in XArray to trigger faults later */
                xas_store(&xas, XA_RETRY_ENTRY);
                if (xas_error(&xas)) {
                    result = SCAN_STORE_FAILED;
                    goto immap_locked;
                }
                nr_none_check++;
            }
        }

        if (nr_none != nr_none_check) {
            result = SCAN_PAGE_FILLED;
            goto immap_locked;
        }

        /* If userspace observed a missing page in a VMA with
         * a MODE_MISSING userfaultfd, then it might expect a
         * UFFD_EVENT_PAGEFAULT for that page. If so, we need to
         * roll back to avoid suppressing such an event. Since
         * wp/minor userfaultfds don't give userspace any
         * guarantees that the kernel doesn't fill a missing
         * page with a zero page, so they don't matter here.
         *
         * Any userfaultfds registered after this point will
         * not be able to observe any missing pages due to the
         * previously inserted retry entries. */
        vma_interval_tree_foreach(vma, &mapping->i_mmap, start, end) {
            if (userfaultfd_missing(vma)) {
                result = SCAN_EXCEED_NONE_PTE;
                goto immap_locked;
            }
        }

immap_locked:
        i_mmap_unlock_read(mapping);
        if (result != SCAN_SUCCEED) {
            xas_set(&xas, start);
            for (index = start; index < end; index++) {
                if (xas_next(&xas) == XA_RETRY_ENTRY)
                    xas_store(&xas, NULL);
            }

            xas_unlock_irq(&xas);
            goto rollback;
        }
    } else {
        xas_lock_irq(&xas);
    }

/* CF.10. Updating Stats and Inserting New Folio */
    if (is_shmem)
        __lruvec_stat_mod_folio(new_folio, NR_SHMEM_THPS, HPAGE_PMD_NR);
    else
        __lruvec_stat_mod_folio(new_folio, NR_FILE_THPS, HPAGE_PMD_NR);

    if (nr_none) {
        __lruvec_stat_mod_folio(new_folio, NR_FILE_PAGES, nr_none);
        /* nr_none is always 0 for non-shmem. */
        __lruvec_stat_mod_folio(new_folio, NR_SHMEM, nr_none);
    }

    /* Mark new_folio as uptodate before inserting it into the
     * page cache so that it isn't mistaken for an fallocated but
     * unwritten page. */
    folio_mark_uptodate(new_folio);
    folio_ref_add(new_folio, HPAGE_PMD_NR - 1);

    if (is_shmem)
        folio_mark_dirty(new_folio);
    folio_add_lru(new_folio);

    /* Join all the small entries into a single multi-index entry. */
    xas_set_order(&xas, start, HPAGE_PMD_ORDER);
    xas_store(&xas, new_folio);
    WARN_ON_ONCE(xas_error(&xas));
    xas_unlock_irq(&xas);

/* CF.11. Retracting Page Tables and Unlocking */

    /* Remove pte page tables, so we can re-fault the page as huge.
     * If MADV_COLLAPSE, adjust result to call collapse_pte_mapped_thp(). */
    retract_page_tables(mapping, start);
    if (cc && !cc->is_khugepaged)
        result = SCAN_PTE_MAPPED_HUGEPAGE;
    folio_unlock(new_folio);

/* CF.12. Freeing Old Folios */
    /* The collapse has succeeded, so free the old folios. */
    list_for_each_entry_safe(folio, tmp, &pagelist, lru) {
        list_del(&folio->lru);
        folio->mapping = NULL;
        folio_clear_active(folio);
        folio_clear_unevictable(folio);
        folio_unlock(folio);
        folio_put_refs(folio, 2 + folio_nr_pages(folio));
    }

    goto out;

/* CF.13. Rollback on Failure */
rollback:
    /* Something went wrong: roll back page cache changes */
    if (nr_none) {
        xas_lock_irq(&xas);
        mapping->nrpages -= nr_none;
        xas_unlock_irq(&xas);
        shmem_uncharge(mapping->host, nr_none);
    }

    list_for_each_entry_safe(folio, tmp, &pagelist, lru) {
        list_del(&folio->lru);
        folio_unlock(folio);
        folio_putback_lru(folio);
        folio_put(folio);
    }
    /* Undo the updates of filemap_nr_thps_inc for non-SHMEM
     * file only. This undo is not needed unless failure is
     * due to SCAN_COPY_MC. */
    if (!is_shmem && result == SCAN_COPY_MC) {
        filemap_nr_thps_dec(mapping);
        /* Paired with the fence in do_dentry_open() -> get_write_access()
         * to ensure the update to nr_thps is visible. */
        smp_mb();
    }

    new_folio->mapping = NULL;

    folio_unlock(new_folio);
    folio_put(new_folio);

/* CF.14. Final Cleanup and Trace */
out:
    VM_BUG_ON(!list_empty(&pagelist));
    trace_mm_khugepaged_collapse_file(mm, new_folio, index, addr, is_shmem, file, HPAGE_PMD_NR, result);
    return result;
}
```

### collapse_pte_mapped_thp

```c
int collapse_pte_mapped_thp(struct mm_struct *mm, unsigned long addr,
                bool install_pmd)
{
    int nr_mapped_ptes = 0, result = SCAN_FAIL;
    unsigned int nr_batch_ptes;
    struct mmu_notifier_range range;
    bool notified = false;
    unsigned long haddr = addr & HPAGE_PMD_MASK;
    unsigned long end = haddr + HPAGE_PMD_SIZE;
    struct vm_area_struct *vma = vma_lookup(mm, haddr);
    struct folio *folio;
    pte_t *start_pte, *pte;
    pmd_t *pmd, pgt_pmd;
    spinlock_t *pml = NULL, *ptl;
    int i;

    mmap_assert_locked(mm);

    /* First check VMA found, in case page tables are being torn down */
    if (!vma || !vma->vm_file ||
        !range_in_vma(vma, haddr, haddr + HPAGE_PMD_SIZE))
        return SCAN_VMA_CHECK;

    /* Fast check before locking page if already PMD-mapped */
    result = find_pmd_or_thp_or_none(mm, haddr, &pmd);
    if (result == SCAN_PMD_MAPPED)
        return result;

    /* If we are here, we've succeeded in replacing all the native pages
    * in the page cache with a single hugepage. If a mm were to fault-in
    * this memory (mapped by a suitably aligned VMA), we'd get the hugepage
    * and map it by a PMD, regardless of sysfs THP settings. As such, let's
    * analogously elide sysfs THP settings here. */
    if (!thp_vma_allowable_order(vma, vma->vm_flags, 0, PMD_ORDER))
        return SCAN_VMA_CHECK;

    /* Keep pmd pgtable for uffd-wp; see comment in retract_page_tables() */
    if (userfaultfd_wp(vma))
        return SCAN_PTE_UFFD_WP;

    folio = filemap_lock_folio(vma->vm_file->f_mapping, linear_page_index(vma, haddr));
    if (IS_ERR(folio))
        return SCAN_PAGE_NULL;

    if (folio_order(folio) != HPAGE_PMD_ORDER) {
        result = SCAN_PAGE_COMPOUND;
        goto drop_folio;
    }

    result = find_pmd_or_thp_or_none(mm, haddr, &pmd);
    switch (result) {
    case SCAN_SUCCEED:
        break;
    case SCAN_PMD_NONE:
        /* All pte entries have been removed and pmd cleared.
        * Skip all the pte checks and just update the pmd mapping. */
        goto maybe_install_pmd;
    default:
        goto drop_folio;
    }

    result = SCAN_FAIL;
    start_pte = pte_offset_map_lock(mm, pmd, haddr, &ptl);
    if (!start_pte) /* mmap_lock + page lock should prevent this */
        goto drop_folio;

    /* step 1: check all mapped PTEs are to the right huge page */
    for (i = 0, addr = haddr, pte = start_pte;
        i < HPAGE_PMD_NR; i++, addr += PAGE_SIZE, pte++) {
        struct page *page;
        pte_t ptent = ptep_get(pte);

        /* empty pte, skip */
        if (pte_none(ptent))
            continue;

        /* page swapped out, abort */
        if (!pte_present(ptent)) {
            result = SCAN_PTE_NON_PRESENT;
            goto abort;
        }

        page = vm_normal_page(vma, addr, ptent);
        if (WARN_ON_ONCE(page && is_zone_device_page(page)))
            page = NULL;
        /* Note that uprobe, debugger, or MAP_PRIVATE may change the
        * page table, but the new page will not be a subpage of hpage. */
        if (folio_page(folio, i) != page)
            goto abort;
    }

    pte_unmap_unlock(start_pte, ptl);
    mmu_notifier_range_init(&range, MMU_NOTIFY_CLEAR, 0, mm, haddr, haddr + HPAGE_PMD_SIZE);
    mmu_notifier_invalidate_range_start(&range);
    notified = true;

    /* pmd_lock covers a wider range than ptl, and (if split from mm's
    * page_table_lock) ptl nests inside pml. The less time we hold pml,
    * the better; but userfaultfd's mfill_atomic_pte() on a private VMA
    * inserts a valid as-if-COWed PTE without even looking up page cache.
    * So page lock of folio does not protect from it, so we must not drop
    * ptl before pgt_pmd is removed, so uffd private needs pml taken now. */
    if (userfaultfd_armed(vma) && !(vma->vm_flags & VM_SHARED))
        pml = pmd_lock(mm, pmd);

    start_pte = pte_offset_map_rw_nolock(mm, pmd, haddr, &pgt_pmd, &ptl);
    if (!start_pte) /* mmap_lock + page lock should prevent this */
        goto abort;
    if (!pml)
        spin_lock(ptl);
    else if (ptl != pml)
        spin_lock_nested(ptl, SINGLE_DEPTH_NESTING);

    if (unlikely(!pmd_same(pgt_pmd, pmdp_get_lockless(pmd))))
        goto abort;

    /* step 2: clear page table and adjust rmap */
    for (i = 0, addr = haddr, pte = start_pte; i < HPAGE_PMD_NR;
        i += nr_batch_ptes, addr += nr_batch_ptes * PAGE_SIZE,
        pte += nr_batch_ptes) {
        unsigned int max_nr_batch_ptes = (end - addr) >> PAGE_SHIFT;
        struct page *page;
        pte_t ptent = ptep_get(pte);

        nr_batch_ptes = 1;

        if (pte_none(ptent))
            continue;
        /* We dropped ptl after the first scan, to do the mmu_notifier:
        * page lock stops more PTEs of the folio being faulted in, but
        * does not stop write faults COWing anon copies from existing
        * PTEs; and does not stop those being swapped out or migrated. */
        if (!pte_present(ptent)) {
            result = SCAN_PTE_NON_PRESENT;
            goto abort;
        }
        page = vm_normal_page(vma, addr, ptent);

        if (folio_page(folio, i) != page)
            goto abort;

        nr_batch_ptes = folio_pte_batch(folio, pte, ptent, max_nr_batch_ptes);

        /* Must clear entry, or a racing truncate may re-remove it.
        * TLB flush can be left until pmdp_collapse_flush() does it.
        * PTE dirty? Shmem page is already dirty; file is read-only. */
        clear_ptes(mm, addr, pte, nr_batch_ptes);
        folio_remove_rmap_ptes(folio, page, nr_batch_ptes, vma);
        nr_mapped_ptes += nr_batch_ptes;
    }

    if (!pml)
        spin_unlock(ptl);

    /* step 3: set proper refcount and mm_counters. */
    if (nr_mapped_ptes) {
        folio_ref_sub(folio, nr_mapped_ptes);
        add_mm_counter(mm, mm_counter_file(folio), -nr_mapped_ptes);
    }

    /* step 4: remove empty page table */
    if (!pml) {
        pml = pmd_lock(mm, pmd);
        if (ptl != pml) {
            spin_lock_nested(ptl, SINGLE_DEPTH_NESTING);
            if (unlikely(!pmd_same(pgt_pmd, pmdp_get_lockless(pmd)))) {
                flush_tlb_mm(mm);
                goto unlock;
            }
        }
    }
    pgt_pmd = pmdp_collapse_flush(vma, haddr, pmd);
    pmdp_get_lockless_sync();
    pte_unmap_unlock(start_pte, ptl);
    if (ptl != pml)
        spin_unlock(pml);

    mmu_notifier_invalidate_range_end(&range);

    mm_dec_nr_ptes(mm);
    page_table_check_pte_clear_range(mm, haddr, pgt_pmd);
    pte_free_defer(mm, pmd_pgtable(pgt_pmd));

maybe_install_pmd:
    /* step 5: install pmd entry */
    result = install_pmd
            ? set_huge_pmd(vma, haddr, pmd, folio, &folio->page)
            : SCAN_SUCCEED;
    goto drop_folio;
abort:
    if (nr_mapped_ptes) {
        flush_tlb_mm(mm);
        folio_ref_sub(folio, nr_mapped_ptes);
        add_mm_counter(mm, mm_counter_file(folio), -nr_mapped_ptes);
    }
unlock:
    if (start_pte)
        pte_unmap_unlock(start_pte, ptl);
    if (pml && pml != ptl)
        spin_unlock(pml);
    if (notified)
        mmu_notifier_invalidate_range_end(&range);
drop_folio:
    folio_unlock(folio);
    folio_put(folio);
    return result;
}
```

### hpage_collapse_scan_pmd

```c
static int hpage_collapse_scan_pmd(struct mm_struct *mm,
                   struct vm_area_struct *vma,
                   unsigned long address, bool *mmap_locked,
                   struct collapse_control *cc)
{
    pmd_t *pmd;
    pte_t *pte, *_pte;
    int result = SCAN_FAIL, referenced = 0;
    int none_or_zero = 0, shared = 0;
    struct page *page = NULL;
    struct folio *folio = NULL;
    unsigned long _address;
    spinlock_t *ptl;
    int node = NUMA_NO_NODE, unmapped = 0;
    bool writable = false;

    VM_BUG_ON(address & ~HPAGE_PMD_MASK);

    result = find_pmd_or_thp_or_none(mm, address, &pmd);
    if (result != SCAN_SUCCEED)
        goto out;

    memset(cc->node_load, 0, sizeof(cc->node_load));
    nodes_clear(cc->alloc_nmask);
    pte = pte_offset_map_lock(mm, pmd, address, &ptl);
    if (!pte) {
        result = SCAN_PMD_NULL;
        goto out;
    }

    for (_address = address, _pte = pte; _pte < pte + HPAGE_PMD_NR;
         _pte++, _address += PAGE_SIZE) {
        pte_t pteval = ptep_get(_pte);
        if (is_swap_pte(pteval)) { /* return !pte_none(pte) && !pte_present(pte); */
            ++unmapped;
            if (!cc->is_khugepaged ||
                unmapped <= khugepaged_max_ptes_swap) {
                if (pte_swp_uffd_wp_any(pteval)) {
                    result = SCAN_PTE_UFFD_WP;
                    goto out_unmap;
                }
                continue;
            } else {
                result = SCAN_EXCEED_SWAP_PTE;
                count_vm_event(THP_SCAN_EXCEED_SWAP_PTE);
                goto out_unmap;
            }
        }
        if (pte_none(pteval) || is_zero_pfn(pte_pfn(pteval))) {
            ++none_or_zero;
            if (!userfaultfd_armed(vma) &&
                (!cc->is_khugepaged ||
                 none_or_zero <= khugepaged_max_ptes_none)) {
                continue;
            } else {
                result = SCAN_EXCEED_NONE_PTE;
                count_vm_event(THP_SCAN_EXCEED_NONE_PTE);
                goto out_unmap;
            }
        }
        if (pte_uffd_wp(pteval)) { /* return !!(pte_val(pte) & PTE_UFFD_WP); */
            /* Don't collapse the page if any of the small
             * PTEs are armed with uffd write protection. */
            result = SCAN_PTE_UFFD_WP;
            goto out_unmap;
        }
        if (pte_write(pteval))
            writable = true;

        page = vm_normal_page(vma, _address, pteval);
        if (unlikely(!page) || unlikely(is_zone_device_page(page))) {
            result = SCAN_PAGE_NULL;
            goto out_unmap;
        }
        folio = page_folio(page);

        if (!folio_test_anon(folio)) {
            result = SCAN_PAGE_ANON;
            goto out_unmap;
        }

        /* We treat a single page as shared if any part of the THP
         * is shared. */
        if (folio_maybe_mapped_shared(folio)) {
            ++shared;
            if (cc->is_khugepaged &&
                shared > khugepaged_max_ptes_shared) {
                result = SCAN_EXCEED_SHARED_PTE;
                count_vm_event(THP_SCAN_EXCEED_SHARED_PTE);
                goto out_unmap;
            }
        }

        /* Record which node the original page is from and save this
         * information to cc->node_load[].
         * Khugepaged will allocate hugepage from the node has the max
         * hit record. */
        node = folio_nid(folio);
        if (hpage_collapse_scan_abort(node, cc)) {
            result = SCAN_SCAN_ABORT;
            goto out_unmap;
        }
        cc->node_load[node]++;
        if (!folio_test_lru(folio)) {
            result = SCAN_PAGE_LRU;
            goto out_unmap;
        }
        if (folio_test_locked(folio)) {
            result = SCAN_PAGE_LOCK;
            goto out_unmap;
        }

        /* Check if the page has any GUP (or other external) pins.
         *
         * Here the check may be racy:
         * it may see folio_mapcount() > folio_ref_count().
         * But such case is ephemeral we could always retry collapse
         * later.  However it may report false positive if the page
         * has excessive GUP pins (i.e. 512).  Anyway the same check
         * will be done again later the risk seems low. */
        if (folio_expected_ref_count(folio) != folio_ref_count(folio)) {
            result = SCAN_PAGE_COUNT;
            goto out_unmap;
        }

        /* If collapse was initiated by khugepaged, check that there is
         * enough young pte to justify collapsing the page */
        if (cc->is_khugepaged &&
            (pte_young(pteval) || folio_test_young(folio) ||
             folio_test_referenced(folio) || mmu_notifier_test_young(vma->vm_mm,
                                     address)))
            referenced++;
    }
    if (!writable) {
        result = SCAN_PAGE_RO;
    } else if (cc->is_khugepaged &&
           (!referenced ||
            (unmapped && referenced < HPAGE_PMD_NR / 2))) {
        result = SCAN_LACK_REFERENCED_PAGE;
    } else {
        result = SCAN_SUCCEED;
    }
out_unmap:
    pte_unmap_unlock(pte, ptl);
    if (result == SCAN_SUCCEED) {
        result = collapse_huge_page(mm, address, referenced, unmapped, cc);
        /* collapse_huge_page will return with the mmap_lock released */
        *mmap_locked = false;
    }
out:
    trace_mm_khugepaged_scan_pmd(mm, folio, writable, referenced,
                     none_or_zero, result, unmapped);
    return result;
}
```

#### collapse_huge_page

```c
static int collapse_huge_page(struct mm_struct *mm, unsigned long address,
                  int referenced, int unmapped,
                  struct collapse_control *cc)
{
    LIST_HEAD(compound_pagelist);
    pmd_t *pmd, _pmd;
    pte_t *pte;
    pgtable_t pgtable;
    struct folio *folio;
    spinlock_t *pmd_ptl, *pte_ptl;
    int result = SCAN_FAIL;
    struct vm_area_struct *vma;
    struct mmu_notifier_range range;

    VM_BUG_ON(address & ~HPAGE_PMD_MASK);

    /* Before allocating the hugepage, release the mmap_lock read lock.
     * The allocation can take potentially a long time if it involves
     * sync compaction, and we do not need to hold the mmap_lock during
     * that. We will recheck the vma after taking it again in write mode. */
    mmap_read_unlock(mm);

    result = alloc_charge_folio(&folio, mm, cc);
    if (result != SCAN_SUCCEED)
        goto out_nolock;

    mmap_read_lock(mm);
    result = hugepage_vma_revalidate(mm, address, true, &vma, cc);
    if (result != SCAN_SUCCEED) {
        mmap_read_unlock(mm);
        goto out_nolock;
    }

    result = find_pmd_or_thp_or_none(mm, address, &pmd);
    if (result != SCAN_SUCCEED) {
        mmap_read_unlock(mm);
        goto out_nolock;
    }

    if (unmapped) {
        /* __collapse_huge_page_swapin will return with mmap_lock
         * released when it fails. So we jump out_nolock directly in
         * that case.  Continuing to collapse causes inconsistency. */
        result = __collapse_huge_page_swapin(mm, vma, address, pmd,
                             referenced);
        if (result != SCAN_SUCCEED)
            goto out_nolock;
    }

    mmap_read_unlock(mm);
    /* Prevent all access to pagetables with the exception of
     * gup_fast later handled by the ptep_clear_flush and the VM
     * handled by the anon_vma lock + PG_lock.
     *
     * UFFDIO_MOVE is prevented to race as well thanks to the
     * mmap_lock. */
    mmap_write_lock(mm);
    result = hugepage_vma_revalidate(mm, address, true, &vma, cc);
    if (result != SCAN_SUCCEED)
        goto out_up_write;
    /* check if the pmd is still valid */
    vma_start_write(vma);
    result = check_pmd_still_valid(mm, address, pmd);
    if (result != SCAN_SUCCEED)
        goto out_up_write;

    anon_vma_lock_write(vma->anon_vma);

    mmu_notifier_range_init(&range, MMU_NOTIFY_CLEAR, 0, mm, address,
                address + HPAGE_PMD_SIZE);
    mmu_notifier_invalidate_range_start(&range);

    pmd_ptl = pmd_lock(mm, pmd); /* probably unnecessary */
    /* This removes any huge TLB entry from the CPU so we won't allow
     * huge and small TLB entries for the same virtual address to
     * avoid the risk of CPU bugs in that area.
     *
     * Parallel GUP-fast is fine since GUP-fast will back off when
     * it detects PMD is changed. */
    _pmd = pmdp_collapse_flush(vma, address, pmd);
    spin_unlock(pmd_ptl);
    mmu_notifier_invalidate_range_end(&range);
    tlb_remove_table_sync_one();

    pte = pte_offset_map_lock(mm, &_pmd, address, &pte_ptl);
    if (pte) {
        result = __collapse_huge_page_isolate(vma, address, pte, cc, &compound_pagelist);
        spin_unlock(pte_ptl);
    } else {
        result = SCAN_PMD_NULL;
    }

    if (unlikely(result != SCAN_SUCCEED)) {
        if (pte)
            pte_unmap(pte);
        spin_lock(pmd_ptl);
        BUG_ON(!pmd_none(*pmd));
        /* We can only use set_pmd_at when establishing
         * hugepmds and never for establishing regular pmds that
         * points to regular pagetables. Use pmd_populate for that */
        pmd_populate(mm, pmd, pmd_pgtable(_pmd));
        spin_unlock(pmd_ptl);
        anon_vma_unlock_write(vma->anon_vma);
        goto out_up_write;
    }

    /* All pages are isolated and locked so anon_vma rmap
     * can't run anymore. */
    anon_vma_unlock_write(vma->anon_vma);

    result = __collapse_huge_page_copy(pte, folio, pmd, _pmd,
        vma, address, pte_ptl, &compound_pagelist);
    pte_unmap(pte);
    if (unlikely(result != SCAN_SUCCEED))
        goto out_up_write;

    /* The smp_wmb() inside __folio_mark_uptodate() ensures the
     * copy_huge_page writes become visible before the set_pmd_at()
     * write. */
    __folio_mark_uptodate(folio);
    pgtable = pmd_pgtable(_pmd);

    _pmd = folio_mk_pmd(folio, vma->vm_page_prot);
    _pmd = maybe_pmd_mkwrite(pmd_mkdirty(_pmd), vma);

    spin_lock(pmd_ptl);
    BUG_ON(!pmd_none(*pmd));
    folio_add_new_anon_rmap(folio, vma, address, RMAP_EXCLUSIVE);
    folio_add_lru_vma(folio, vma);
    pgtable_trans_huge_deposit(mm, pmd, pgtable);
    set_pmd_at(mm, address, pmd, _pmd);
    update_mmu_cache_pmd(vma, address, pmd);
    deferred_split_folio(folio, false);
    spin_unlock(pmd_ptl);

    folio = NULL;

    result = SCAN_SUCCEED;
out_up_write:
    mmap_write_unlock(mm);
out_nolock:
    if (folio)
        folio_put(folio);
    trace_mm_collapse_huge_page(mm, result == SCAN_SUCCEED, result);
    return result;
}
```

## split_huge_page

Splitting to `order-1` anonymous folios is not supported
 * for non-file-backed folios, because `folio->_deferred_list`, which is used by partially mapped folios, is stored in subpage 2, but an order-1 folio only has subpages 0 and 1.
 * File-backed order-1 folios are supported, since they do not use _deferred_list.

The split process is similar to existing approach:
1. Unmap all page mappings (split PMD mappings if exist);
2. Split meta data like memcg, page owner, page alloc tag;
3. Copy meta data in struct folio to sub pages, but instead of spliting
   the whole folio into multiple smaller ones with the same order in a
   shot, this approach splits the folio iteratively. Taking the example
   above, this approach first splits the original order-9 into two order-8,
   then splits left part of order-8 to two order-7 and so on;
4. Post-process split folios, like write mapping->i_pages for pagecache,
   adjust folio refcounts, add split folios to corresponding list;
5. Remap split folios
6. Unlock split folios.

```c
static inline int split_huge_page(struct page *page)
{
    struct folio *folio = page_folio(page);
    int ret = min_order_for_split(folio);

    if (ret < 0)
        return ret;

    return split_huge_page_to_list_to_order(page, NULL, ret) {
        struct folio *folio = page_folio(page);

        return __folio_split(folio, new_order, &folio->page, page, list, true);
    }
}

static int __folio_split(struct folio *folio, unsigned int new_order,
        struct page *split_at, struct page *lock_at,
        struct list_head *list, bool uniform_split)
{
    struct deferred_split *ds_queue = get_deferred_split_queue(folio) {
        struct mem_cgroup *memcg = folio_memcg(folio);
        struct pglist_data *pgdat = NODE_DATA(folio_nid(folio));

        if (memcg)
            return &memcg->deferred_split_queue;
        else
            return &pgdat->deferred_split_queue;
    }

    XA_STATE(xas, &folio->mapping->i_pages, folio->index);
    struct folio *end_folio = folio_next(folio) {
        return (struct folio *)folio_page(folio, folio_nr_pages(folio));
    }
    bool is_anon = folio_test_anon(folio);
    struct address_space *mapping = NULL;
    struct anon_vma *anon_vma = NULL;
    int order = folio_order(folio);
    struct folio *new_folio, *next;
    int nr_shmem_dropped = 0;
    int remap_flags = 0;
    int extra_pins, ret;
    pgoff_t end;
    bool is_hzp;

    VM_WARN_ON_ONCE_FOLIO(!folio_test_locked(folio), folio);
    VM_WARN_ON_ONCE_FOLIO(!folio_test_large(folio), folio);

    if (folio != page_folio(split_at) || folio != page_folio(lock_at))
        return -EINVAL;

    if (new_order >= folio_order(folio))
        return -EINVAL;

    ret = uniform_split_supported(folio, new_order, true) {
        if (folio_test_anon(folio)) {
            return new_order != 1;
        } else  if (new_order) {
            if (IS_ENABLED(CONFIG_READ_ONLY_THP_FOR_FS) &&
                !mapping_large_folio_support(folio->mapping)) {
                return false;
            }
        }

        if (new_order && folio_test_swapcache(folio)) {
            return false;
        }

        return true;
    }
    if (uniform_split && !)
        return -EINVAL;

    if (!uniform_split &&
        !non_uniform_split_supported(folio, new_order, true))
        return -EINVAL;

    is_hzp = is_huge_zero_folio(folio) {
        return READ_ONCE(huge_zero_folio) == folio;
    }
    if (is_hzp) {
        pr_warn_ratelimited("Called split_huge_page for huge zero page\n");
        return -EBUSY;
    }

    if (folio_test_writeback(folio))
        return -EBUSY;

    if (is_anon) {
        anon_vma = folio_get_anon_vma(folio);
        if (!anon_vma) {
            ret = -EBUSY;
            goto out;
        }
        mapping = NULL;
        anon_vma_lock_write(anon_vma);
    } else {
        unsigned int min_order;
        gfp_t gfp;

        mapping = folio->mapping;

        if (!mapping) {
            ret = -EBUSY;
            goto out;
        }

        min_order = mapping_min_folio_order(folio->mapping) {
            if (!IS_ENABLED(CONFIG_TRANSPARENT_HUGEPAGE))
                return 0;
            return (mapping->flags & AS_FOLIO_ORDER_MIN_MASK) >> AS_FOLIO_ORDER_MIN;
        }
        if (new_order < min_order) {
            ret = -EINVAL;
            goto out;
        }

        gfp = current_gfp_context(mapping_gfp_mask(mapping) & GFP_RECLAIM_MASK);

        ret = filemap_release_folio(folio, gfp) {
            struct address_space * const mapping = folio->mapping;

            BUG_ON(!folio_test_locked(folio));
            if (!folio_needs_release(folio))
                return true;
            if (folio_test_writeback(folio))
                return false;

            if (mapping && mapping->a_ops->release_folio)
                return mapping->a_ops->release_folio(folio, gfp);
            return try_to_free_buffers(folio);
        }
        if (!ret) {
            ret = -EBUSY;
            goto out;
        }

        if (uniform_split) {
            xas_set_order(&xas, folio->index, new_order);
           /* allocate new nodes (and fill them with @entry) to prepare
            * for the upcoming split of an entry of @order size into
            * entries of the order stored in the @xas. */
            xas_split_alloc(&xas, folio, folio_order(folio), gfp);
            if (xas_error(&xas)) {
                ret = xas_error(&xas);
                goto out;
            }
        }

        anon_vma = NULL;
        i_mmap_lock_read(mapping);

        end = DIV_ROUND_UP(i_size_read(mapping->host), PAGE_SIZE);
        if (shmem_mapping(mapping))
            end = shmem_fallocend(mapping->host, end);
    }

    ret = can_split_folio(folio, 1/*caller_pins*/, &extra_pins) {
        int extra_pins;

        /* Additional pins from page cache */
        if (folio_test_anon(folio))
            extra_pins = folio_test_swapcache(folio) ? folio_nr_pages(folio) : 0;
        else
            extra_pins = folio_nr_pages(folio);
        if (pextra_pins)
            *pextra_pins = extra_pins;

        return folio_mapcount(folio) == folio_ref_count(folio) - extra_pins - caller_pins;
    }
    if (!ret) {
        ret = -EAGAIN;
        goto out_unlock;
    }

/* 1. Unmap all page mappings (split PMD mappings if exist); */
    unmap_folio(folio) {
        enum ttu_flags ttu_flags = TTU_RMAP_LOCKED | TTU_SYNC |
        TTU_BATCH_FLUSH;

        VM_BUG_ON_FOLIO(!folio_test_large(folio), folio);

        if (folio_test_pmd_mappable(folio))
            ttu_flags |= TTU_SPLIT_HUGE_PMD;

        /* Anon pages need migration entries to preserve them, but file
        * pages can simply be left unmapped, then faulted back on demand.
        * If that is ever changed (perhaps for mlock), update remap_page(). */
        if (folio_test_anon(folio)) {
            /* try to replace all page table mappings with swap entries */
            try_to_migrate(folio, ttu_flags);
        } else {
            /* try to remove all page table mappings to a folio */
            try_to_unmap(folio, ttu_flags | TTU_IGNORE_MLOCK);
        }

        try_to_unmap_flush();
    }

    /* block interrupt reentry in xa_lock and spinlock */
    local_irq_disable();
    if (mapping) {
        /* Check if the folio is present in page cache.
         * We assume all tail are present too, if folio is there. */
        xas_lock(&xas);
        xas_reset(&xas);
        if (xas_load(&xas) != folio) {
            ret = -EAGAIN;
            goto fail;
        }
    }

    /* Prevent deferred_split_scan() touching ->_refcount */
    spin_lock(&ds_queue->split_queue_lock);
    /* atomic_cmpxchg(&page->_refcount, count, 0) == count */
    if (folio_ref_freeze(folio, 1 + extra_pins)) {
        struct address_space *swap_cache = NULL;
        struct lruvec *lruvec;
        int expected_refs;

        if (folio_order(folio) > 1 && !list_empty(&folio->_deferred_list)) {
            ds_queue->split_queue_len--;
            if (folio_test_partially_mapped(folio)) {
                folio_clear_partially_mapped(folio);
                mod_mthp_stat(folio_order(folio), MTHP_STAT_NR_ANON_PARTIALLY_MAPPED, -1);
            }
            /* Reinitialize page_deferred_list after removing the
             * page from the split_queue, otherwise a subsequent
             * split will see list corruption when checking the
             * page_deferred_list. */
            list_del_init(&folio->_deferred_list);
        }
        spin_unlock(&ds_queue->split_queue_lock);
        if (mapping) {
            int nr = folio_nr_pages(folio);

            if (folio_test_pmd_mappable(folio) && new_order < HPAGE_PMD_ORDER) {
                if (folio_test_swapbacked(folio)) {
                    __lruvec_stat_mod_folio(folio, NR_SHMEM_THPS, -nr);
                } else {
                    __lruvec_stat_mod_folio(folio, NR_FILE_THPS, -nr);
                    filemap_nr_thps_dec(mapping);
                }
            }
        }

        if (folio_test_swapcache(folio)) {
            if (mapping) {
                VM_WARN_ON_ONCE_FOLIO(mapping, folio);
                ret = -EINVAL;
                goto fail;
            }

            swap_cache = swap_address_space(folio->swap);
            xa_lock(&swap_cache->i_pages);
        }

        /* lock lru list/PageCompound, ref frozen by page_ref_freeze */
        lruvec = folio_lruvec_lock(folio);

        ret = __split_unmapped_folio(folio, new_order, split_at, &xas,
                         mapping, uniform_split);
            --->

        /* Unfreeze after-split folios and put them back to the right
         * list. @folio should be kept frozon until page cache
         * entries are updated with all the other after-split folios
         * to prevent others seeing stale page cache entries.
         * As a result, new_folio starts from the next folio of
         * @folio. */
        for (new_folio = folio_next(folio); new_folio != end_folio; new_folio = next) {
            unsigned long nr_pages = folio_nr_pages(new_folio);

            next = folio_next(new_folio);

            /* taking references from the pagecache,
             * swapcache, PG_private and page table mappings into account. */
            expected_refs = folio_expected_ref_count(new_folio) + 1;
            folio_ref_unfreeze(new_folio, expected_refs);

            lru_add_split_folio(folio, new_folio, lruvec, list) {
                if (list) {
                    /* page reclaim is reclaiming a huge page */
                    VM_WARN_ON(folio_test_lru(folio));
                    folio_get(new_folio);
                    list_add_tail(&new_folio->lru, list);
                } else {
                    /* head is still on lru (and we have it frozen) */
                    VM_WARN_ON(!folio_test_lru(folio));
                    if (folio_test_unevictable(folio))
                        new_folio->mlock_count = 0;
                    else
                        list_add_tail(&new_folio->lru, &folio->lru);
                    folio_set_lru(new_folio);
                }
            }

            /* Anonymous folio with swap cache.
             * NOTE: shmem in swap cache is not supported yet. */
            if (swap_cache) {
                __xa_store(&swap_cache->i_pages,
                       swap_cache_index(new_folio->swap),
                       new_folio, 0);
                continue;
            }

            /* Anonymous folio without swap cache */
            if (!mapping)
                continue;

            /* Add the new folio to the page cache. */
            if (new_folio->index < end) {
                __xa_store(&mapping->i_pages, new_folio->index,
                       new_folio, 0);
                continue;
            }

            /* Drop folio beyond EOF: ->index >= end */
            if (shmem_mapping(mapping))
                nr_shmem_dropped += nr_pages;
            else if (folio_test_clear_dirty(new_folio))
                folio_account_cleaned(new_folio, inode_to_wb(mapping->host));

            __filemap_remove_folio(new_folio, NULL) {
                struct address_space *mapping = folio->mapping;

                filemap_unaccount_folio(mapping, folio);
                page_cache_delete(mapping, folio, shadow);
            }
            folio_put_refs(new_folio, nr_pages);
        }
        /* Unfreeze @folio only after all page cache entries, which
         * used to point to it, have been updated with new folios.
         * Otherwise, a parallel folio_try_get() can grab @folio
         * and its caller can see stale page cache entries. */
        expected_refs = folio_expected_ref_count(folio) + 1;
        folio_ref_unfreeze(folio, expected_refs);

        unlock_page_lruvec(lruvec);

        if (swap_cache)
            xa_unlock(&swap_cache->i_pages);
    } else {
        spin_unlock(&ds_queue->split_queue_lock);
        ret = -EAGAIN;
    }

fail:
    if (mapping)
        xas_unlock(&xas);

    local_irq_enable();

    if (nr_shmem_dropped)
        shmem_uncharge(mapping->host, nr_shmem_dropped);

    if (!ret && is_anon)
        remap_flags = RMP_USE_SHARED_ZEROPAGE;
    remap_page(folio, 1 << order, remap_flags) {
        int i = 0;

        /* If unmap_folio() uses try_to_migrate() on file, remove this check */
        if (!folio_test_anon(folio))
            return;

        for (;;) {
            remove_migration_ptes(folio, folio, RMP_LOCKED | flags) {
                struct rmap_walk_arg rmap_walk_arg = {
                    .folio = src,
                    .map_unused_to_zeropage = flags & RMP_USE_SHARED_ZEROPAGE,
                };

                struct rmap_walk_control rwc = {
                    .rmap_one = remove_migration_pte,
                    .arg = &rmap_walk_arg,
                };

                VM_BUG_ON_FOLIO((flags & RMP_USE_SHARED_ZEROPAGE) && (src != dst), src);

                if (flags & RMP_LOCKED)
                    rmap_walk_locked(dst, &rwc);
                else
                    rmap_walk(dst, &rwc);
            }
            i += folio_nr_pages(folio);
            if (i >= nr)
                break;
            folio = folio_next(folio);
        }
    }

    /* Unlock all after-split folios except the one containing
     * @lock_at page. If @folio is not split, it will be kept locked. */
    for (new_folio = folio; new_folio != end_folio; new_folio = next) {
        next = folio_next(new_folio);
        if (new_folio == page_folio(lock_at))
            continue;

        folio_unlock(new_folio);
        /* Subpages may be freed if there wasn't any mapping
         * like if add_to_swap() is running on a lru page that
         * had its mapping zapped. And freeing these pages
         * requires taking the lru_lock so we do the put_page
         * of the tail pages after the split is complete. */
        free_folio_and_swap_cache(new_folio) {
            free_swap_cache(folio);
            if (!is_huge_zero_folio(folio)) {
                folio_put(folio);
            }
        }
    }

out_unlock:
    if (anon_vma) {
        anon_vma_unlock_write(anon_vma);
        put_anon_vma(anon_vma);
    }
    if (mapping)
        i_mmap_unlock_read(mapping);
out:
    xas_destroy(&xas);
    if (order == HPAGE_PMD_ORDER)
        count_vm_event(!ret ? THP_SPLIT_PAGE : THP_SPLIT_PAGE_FAILED);
    count_mthp_stat(order, !ret ? MTHP_STAT_SPLIT : MTHP_STAT_SPLIT_FAILED);
    return ret;
}
```

### __split_unmapped_folio

```c
int __split_unmapped_folio(struct folio *folio, int new_order,
        struct page *split_at, struct xa_state *xas,
        struct address_space *mapping, bool uniform_split)
{
    int order = folio_order(folio);
    int start_order = uniform_split ? new_order : order - 1;
    bool stop_split = false;
    struct folio *next;
    int split_order;
    int ret = 0;

    if (folio_test_anon(folio))
        mod_mthp_stat(order, MTHP_STAT_NR_ANON, -1);

    folio_clear_has_hwpoisoned(folio);

    /* split to new_order one order at a time. For uniform split,
     * folio is split to new_order directly. */
    for (split_order = start_order;
         split_order >= new_order && !stop_split;
         split_order--) {
        struct folio *end_folio = folio_next(folio);
        int old_order = folio_order(folio);
        struct folio *new_folio;

        /* order-1 anonymous folio is not supported */
        if (folio_test_anon(folio) && split_order == 1)
            continue;
        if (uniform_split && split_order != new_order)
            continue;

        if (mapping) {
            /* uniform split has xas_split_alloc() called before
             * irq is disabled to allocate enough memory, whereas
             * non-uniform split can handle ENOMEM. */
            if (uniform_split)
                xas_split(xas, folio, old_order);
                    --->
            else {
                xas_set_order(xas, folio->index, split_order) {
                #ifdef CONFIG_XARRAY_MULTI
                    xas->xa_index = order < BITS_PER_LONG ? (index >> order) << order : 0;
                    xas->xa_shift = order - (order % XA_CHUNK_SHIFT);
                    xas->xa_sibs = (1 << (order % XA_CHUNK_SHIFT)) - 1;
                    xas->xa_node = XAS_RESTART;
                #else
                    BUG_ON(order > 0);
                    xas_set(xas, index);
                #endif
                }
                xas_try_split(xas, folio, old_order);
                    --->
                if (xas_error(xas)) {
                    ret = xas_error(xas);
                    stop_split = true;
                }
            }
        }

        if (!stop_split) {
            folio_split_memcg_refs(folio, old_order, split_order) {
                unsigned new_refs;

                if (mem_cgroup_disabled() || !folio_memcg_charged(folio))
                    return;

                new_refs = (1 << (old_order - new_order)) - 1;
                css_get_many(&__folio_memcg(folio)->css, new_refs);
            }
            split_page_owner(&folio->page, old_order, split_order) {
                struct page_ext_iter iter;
                struct page_ext *page_ext;
                struct page_owner *page_owner;

                rcu_read_lock();
                for_each_page_ext(page, 1 << old_order, page_ext, iter) {
                    page_owner = get_page_owner(page_ext);
                    page_owner->order = new_order;
                }
                rcu_read_unlock();
            }
            pgalloc_tag_split(folio, old_order, split_order);

            __split_folio_to_order(folio, old_order, split_order);
                --->
        }

        for (new_folio = folio; new_folio != end_folio; new_folio = next) {
            next = folio_next(new_folio);
            /* for buddy allocator like split, new_folio containing
             * @split_at page could be split again */
            if (new_folio == page_folio(split_at)) {
                folio = new_folio;
                if (split_order != new_order && !stop_split)
                    continue;
            }
            if (folio_test_anon(new_folio))
                mod_mthp_stat(folio_order(new_folio), MTHP_STAT_NR_ANON, 1);
        }
    }

    return ret;
}
```

### __split_folio_to_order

```c
/* can split a folio into any lower order and copies the @folio metadata to
 * all the resulting folios.
 *
 * For uniform split, __split_unmapped_folio() calls it once to split the
 * given folio to the new order.
 *
 * For buddy allocator like (non-uniform) split, __split_unmapped_folio()
 * calls it (folio_order - new_order) times and each time splits
 * the folio containing the given page to one lower */
static void __split_folio_to_order(struct folio *folio, int old_order,
        int new_order)
{
    long new_nr_pages = 1 << new_order;
    long nr_pages = 1 << old_order;
    long i;

    /* Skip the first new_nr_pages, since the new folio from them have all
     * the flags from the original folio. */
    for (i = new_nr_pages; i < nr_pages; i += new_nr_pages) {
        struct page *new_head = &folio->page + i;

        /* Careful: new_folio is not a "real" folio before we cleared PageTail.
         * Don't pass it around before clear_compound_head(). */
        struct folio *new_folio = (struct folio *)new_head;

        VM_BUG_ON_PAGE(atomic_read(&new_folio->_mapcount) != -1, new_head);

        /* Clone page flags before unfreezing refcount.
         *
         * After successful get_page_unless_zero() might follow flags change,
         * for example lock_page() which set PG_waiters.
         *
         * Note that for mapped sub-pages of an anonymous THP,
         * PG_anon_exclusive has been cleared in unmap_folio() and is stored in
         * the migration entry instead from where remap_page() will restore it.
         * We can still have PG_anon_exclusive set on effectively unmapped and
         * unreferenced sub-pages of an anonymous THP: we can simply drop
         * PG_anon_exclusive (-> PG_mappedtodisk) for these here. */
        new_folio->flags &= ~PAGE_FLAGS_CHECK_AT_PREP;
        new_folio->flags |= (folio->flags &
                ((1L << PG_referenced) |
                 (1L << PG_swapbacked) |
                 (1L << PG_swapcache) |
                 (1L << PG_mlocked) |
                 (1L << PG_uptodate) |
                 (1L << PG_active) |
                 (1L << PG_workingset) |
                 (1L << PG_locked) |
                 (1L << PG_unevictable) |
#ifdef CONFIG_ARCH_USES_PG_ARCH_2
                 (1L << PG_arch_2) |
#endif
#ifdef CONFIG_ARCH_USES_PG_ARCH_3
                 (1L << PG_arch_3) |
#endif
                 (1L << PG_dirty) |
                 LRU_GEN_MASK | LRU_REFS_MASK));

        new_folio->mapping = folio->mapping;
        new_folio->index = folio->index + i;

        /* page->private should not be set in tail pages. Fix up and warn once
         * if private is unexpectedly set. */
        if (unlikely(new_folio->private)) {
            VM_WARN_ON_ONCE_PAGE(true, new_head);
            new_folio->private = NULL;
        }

        if (folio_test_swapcache(folio))
            new_folio->swap.val = folio->swap.val + i;

        /* Page flags must be visible before we make the page non-compound. */
        smp_wmb();

        /* Clear PageTail before unfreezing page refcount.
         *
         * After successful get_page_unless_zero() might follow put_page()
         * which needs correct compound_head(). */
        clear_compound_head(new_head);
        if (new_order) {
            prep_compound_page(new_head, new_order);
            folio_set_large_rmappable(new_folio);
        }

        if (folio_test_young(folio))
            folio_set_young(new_folio);
        if (folio_test_idle(folio))
            folio_set_idle(new_folio);
#ifdef CONFIG_MEMCG
        new_folio->memcg_data = folio->memcg_data;
#endif

        folio_xchg_last_cpupid(new_folio, folio_last_cpupid(folio));
    }

    if (new_order)
        folio_set_order(folio, new_order);
    else
        ClearPageCompound(&folio->page);
}
```

### xas_split

```c
void xas_split_alloc(struct xa_state *xas, void *entry, unsigned int order,
        gfp_t gfp)
{
    unsigned int sibs = (1 << (order % XA_CHUNK_SHIFT)) - 1;

    /* XXX: no support for splitting really large entries yet */
    if (WARN_ON(xas->xa_shift + 2 * XA_CHUNK_SHIFT <= order))
        goto nomem;
    if (xas->xa_shift + XA_CHUNK_SHIFT > order)
        return;

    do {
        struct xa_node *node;

        node = kmem_cache_alloc_lru(radix_tree_node_cachep, xas->xa_lru, gfp);
        if (!node)
            goto nomem;

        __xas_init_node_for_split(xas, node, entry) {
            unsigned int i;
            void *sibling = NULL;
            unsigned int mask = xas->xa_sibs;

            if (!node)
                return;
            node->array = xas->xa;
            for (i = 0; i < XA_CHUNK_SIZE; i++) {
                if ((i & mask) == 0) {
                    RCU_INIT_POINTER(node->slots[i], entry);
                    sibling = xa_mk_sibling(i);
                } else {
                    RCU_INIT_POINTER(node->slots[i], sibling);
                }
            }
        }
        RCU_INIT_POINTER(node->parent, xas->xa_alloc);
        xas->xa_alloc = node;
    } while (sibs-- > 0);

    return;
nomem:
    xas_destroy(xas);
    xas_set_err(xas, -ENOMEM);
}
```

```c
void xas_split(struct xa_state *xas, void *entry, unsigned int order)
{
    unsigned int sibs = (1 << (order % XA_CHUNK_SHIFT)) - 1;
    unsigned int offset, marks;
    struct xa_node *node;
    void *curr = xas_load(xas);
    int values = 0;

    node = xas->xa_node;
    if (xas_top(node))
        return;

    marks = node_get_marks(node, xas->xa_offset);

    offset = xas->xa_offset + sibs;
    do {
        if (xas->xa_shift < node->shift) {
            struct xa_node *child = xas->xa_alloc;

            xas->xa_alloc = rcu_dereference_raw(child->parent);
            child->shift = node->shift - XA_CHUNK_SHIFT;
            child->offset = offset;
            child->count = XA_CHUNK_SIZE;
            child->nr_values = xa_is_value(entry) ? XA_CHUNK_SIZE : 0;
            RCU_INIT_POINTER(child->parent, node);
            node_set_marks(node, offset, child, xas->xa_sibs, marks);
            rcu_assign_pointer(node->slots[offset], xa_mk_node(child));
            if (xa_is_value(curr))
                values--;
            xas_update(xas, child);
        } else {
            unsigned int canon = offset - xas->xa_sibs;

            node_set_marks(node, canon, NULL, 0, marks);
            rcu_assign_pointer(node->slots[canon], entry);
            while (offset > canon)
                rcu_assign_pointer(node->slots[offset--],
                        xa_mk_sibling(canon));
            values += (xa_is_value(entry) - xa_is_value(curr)) *
                    (xas->xa_sibs + 1);
        }
    } while (offset-- > xas->xa_offset);

    node->nr_values += values;
    xas_update(xas, node);
}
```

### xas_try_split

```c
void xas_try_split(struct xa_state *xas, void *entry, unsigned int order)
{
    unsigned int sibs = (1 << (order % XA_CHUNK_SHIFT)) - 1;
    unsigned int offset, marks;
    struct xa_node *node;
    void *curr = xas_load(xas);
    int values = 0;
    gfp_t gfp = GFP_NOWAIT;

    node = xas->xa_node;
    if (xas_top(node))
        return;

    if (xas->xa->xa_flags & XA_FLAGS_ACCOUNT)
        gfp |= __GFP_ACCOUNT;

    marks = node_get_marks(node, xas->xa_offset);

    offset = xas->xa_offset + sibs;

    if (xas->xa_shift < node->shift) {
        struct xa_node *child = xas->xa_alloc;
        unsigned int expected_sibs =
            (1 << ((order - 1) % XA_CHUNK_SHIFT)) - 1;

        /* No support for splitting sibling entries
        * (horizontally) or cascade split (vertically), which
        * requires two or more new xa_nodes.
        * Since if one xa_node allocation fails,
        * it is hard to free the prior allocations. */
        if (sibs || xas->xa_sibs != expected_sibs) {
            xas_destroy(xas);
            xas_set_err(xas, -EINVAL);
            return;
        }

        if (!child) {
            child = kmem_cache_alloc_lru(radix_tree_node_cachep,
                            xas->xa_lru, gfp);
            if (!child) {
                xas_destroy(xas);
                xas_set_err(xas, -ENOMEM);
                return;
            }
            RCU_INIT_POINTER(child->parent, xas->xa_alloc);
        }
        __xas_init_node_for_split(xas, child, entry);

        xas->xa_alloc = rcu_dereference_raw(child->parent);
        child->shift = node->shift - XA_CHUNK_SHIFT;
        child->offset = offset;
        child->count = XA_CHUNK_SIZE;
        child->nr_values = xa_is_value(entry) ?
                XA_CHUNK_SIZE : 0;
        RCU_INIT_POINTER(child->parent, node);
        node_set_marks(node, offset, child, xas->xa_sibs,
                marks);
        rcu_assign_pointer(node->slots[offset],
                xa_mk_node(child));
        if (xa_is_value(curr))
            values--;
        xas_update(xas, child);

    } else {
        do {
            unsigned int canon = offset - xas->xa_sibs;

            node_set_marks(node, canon, NULL, 0, marks);
            rcu_assign_pointer(node->slots[canon], entry);
            while (offset > canon)
                rcu_assign_pointer(node->slots[offset--],
                        xa_mk_sibling(canon));
            values += (xa_is_value(entry) - xa_is_value(curr)) *
                    (xas->xa_sibs + 1);
        } while (offset-- > xas->xa_offset);
    }

    node->nr_values += values;
    xas_update(xas, node);
}
```

## HVO

![](../images/kernel/mem-hvo.svg)

* [A vmemmap diet for HugeTLB and Device DAX](https://docs.kernel.org/mm/vmemmap_dedup.html)

```c
static const struct ctl_table hugetlb_vmemmap_sysctls[] = {
    {
        .procname       = "hugetlb_optimize_vmemmap",
        .data           = &vmemmap_optimize_enabled,
        .maxlen         = sizeof(vmemmap_optimize_enabled),
        .mode           = 0644,
        .proc_handler   = proc_dobool,
    },
};

static int __init hugetlb_vmemmap_init(void)
{
    const struct hstate *h;

    /* HUGETLB_VMEMMAP_RESERVE_SIZE should cover all used struct pages */
    BUILD_BUG_ON(__NR_USED_SUBPAGE > HUGETLB_VMEMMAP_RESERVE_PAGES);

    for_each_hstate(h) {
        ret = hugetlb_vmemmap_optimizable(h) {
            return 0 != hugetlb_vmemmap_optimizable_size(h) {
                #define HUGETLB_VMEMMAP_RESERVE_SIZE    PAGE_SIZE
                #define HUGETLB_VMEMMAP_RESERVE_PAGES   (HUGETLB_VMEMMAP_RESERVE_SIZE / sizeof(struct page))

                int size = hugetlb_vmemmap_size(h) {
                    return pages_per_huge_page(h) {
                        eturn 1 << h->order;
                    } * sizeof(struct page)
                } - HUGETLB_VMEMMAP_RESERVE_SIZE;

                if (!is_power_of_2(sizeof(struct page)))
                    return 0;
                return size > 0 ? size : 0;
            };
        }
        if (ret) {
            register_sysctl_init("vm", hugetlb_vmemmap_sysctls);
            break;
        }
    }
    return 0;
}
late_initcall(hugetlb_vmemmap_init);
```

### hugetlb_vmemmap_init_early

```c
void __init hugetlb_vmemmap_init_early(int nid)
{
    unsigned long psize, paddr, section_size;
    unsigned long ns, i, pnum, pfn, nr_pages;
    unsigned long start, end;
    struct huge_bootmem_page {
        struct list_head    list;
        struct hstate       *hstate;
        unsigned long       flags;
        struct cma          *cma;
    } *m = NULL;
    void *map;

    if (!hugetlb_bootmem_allocated())
        return;

    if (!READ_ONCE(vmemmap_optimize_enabled))
        return;

    section_size = (1UL << PA_SECTION_SHIFT);

    list_for_each_entry(m, &huge_boot_pages[nid], list) {
        ret = vmemmap_should_optimize_bootmem_page(m) {
            unsigned long section_size, psize, pmd_vmemmap_size;
            phys_addr_t paddr;

            if (!READ_ONCE(vmemmap_optimize_enabled))
                return false;

            if (!hugetlb_vmemmap_optimizable(m->hstate))
                return false;

            psize = huge_page_size(m->hstate);
            paddr = virt_to_phys(m);

            /* Pre-HVO only works if the bootmem huge page
            * is aligned to the section size. */
            section_size = (1UL << PA_SECTION_SHIFT);
            if (!IS_ALIGNED(paddr, section_size) ||
                !IS_ALIGNED(psize, section_size))
                return false;

            /* The pre-HVO code does not deal with splitting PMDS,
            * so the bootmem page must be aligned to the number
            * of base pages that can be mapped with one vmemmap PMD. */
            pmd_vmemmap_size = (PMD_SIZE / (sizeof(struct page))) << PAGE_SHIFT;
            if (!IS_ALIGNED(paddr, pmd_vmemmap_size) ||
                !IS_ALIGNED(psize, pmd_vmemmap_size))
                return false;

            return true;
        }
        if (!ret)
            continue;

        nr_pages = pages_per_huge_page(m->hstate);
        psize = nr_pages << PAGE_SHIFT;
        paddr = virt_to_phys(m);
        pfn = PHYS_PFN(paddr);
        map = pfn_to_page(pfn);
        start = (unsigned long)map;
        end = start + nr_pages * sizeof(struct page);

        if (vmemmap_populate_hvo(start, end, nid,
                    HUGETLB_VMEMMAP_RESERVE_SIZE) < 0)
            continue;

        memmap_boot_pages_add(HUGETLB_VMEMMAP_RESERVE_SIZE / PAGE_SIZE);

        pnum = pfn_to_section_nr(pfn);
        ns = psize / section_size;

        for (i = 0; i < ns; i++) {
            sparse_init_early_section(nid, map, pnum,
                    SECTION_IS_VMEMMAP_PREINIT);
            map += section_map_size();
            pnum++;
        }

        m->flags |= HUGE_BOOTMEM_HVO;
    }
}
```

### hugetlb_vmemmap_init_late

```c
void __init hugetlb_vmemmap_init_late(int nid)
{
    struct huge_bootmem_page *m, *tm;
    unsigned long phys, nr_pages, start, end;
    unsigned long pfn, nr_mmap;
    struct hstate *h;
    void *map;

    if (!hugetlb_bootmem_allocated())
        return;

    if (!READ_ONCE(vmemmap_optimize_enabled))
        return;

    list_for_each_entry_safe(m, tm, &huge_boot_pages[nid], list) {
        if (!(m->flags & HUGE_BOOTMEM_HVO))
            continue;

        phys = virt_to_phys(m);
        h = m->hstate;
        pfn = PHYS_PFN(phys);
        nr_pages = pages_per_huge_page(h);

        if (!hugetlb_bootmem_page_zones_valid(nid, m)) {
            /* Oops, the hugetlb page spans multiple zones.
             * Remove it from the list, and undo HVO. */
            list_del(&m->list);

            map = pfn_to_page(pfn);

            start = (unsigned long)map;
            end = start + nr_pages * sizeof(struct page);

            vmemmap_undo_hvo(start, end, nid,
                     HUGETLB_VMEMMAP_RESERVE_SIZE);
            nr_mmap = end - start - HUGETLB_VMEMMAP_RESERVE_SIZE;
            memmap_boot_pages_add(DIV_ROUND_UP(nr_mmap, PAGE_SIZE));

            memblock_phys_free(phys, huge_page_size(h));
            continue;
        } else
            m->flags |= HUGE_BOOTMEM_ZONES_VALID;
    }
}
```

### hugetlb_vmemmap_optimize_folio

```c
void hugetlb_vmemmap_optimize_folio(const struct hstate *h, struct folio *folio) {
    LIST_HEAD(vmemmap_pages);

    __hugetlb_vmemmap_optimize_folio(h, folio, &vmemmap_pages, VMEMMAP_SYNCHRONIZE_RCU);

    free_vmemmap_page_list(&vmemmap_pages) {
        struct page *page, *next;

        list_for_each_entry_safe(page, next, list, lru) {
            free_vmemmap_page(page) {
                if (PageReserved(page)) {
                    memmap_boot_pages_add(-1);
                    free_bootmem_page(page);
                } else {
                    memmap_pages_add(-1);
                    __free_page(page);
                }
            }
        }
    }
}

static int __hugetlb_vmemmap_optimize_folio(const struct hstate *h,
    struct folio *folio,
    struct list_head *vmemmap_pages,
    unsigned long flags)
{
    int ret = 0;
    unsigned long vmemmap_start = (unsigned long)&folio->page, vmemmap_end;
    unsigned long vmemmap_reuse;

    VM_WARN_ON_ONCE_FOLIO(!folio_test_hugetlb(folio), folio);
    VM_WARN_ON_ONCE_FOLIO(folio_ref_count(folio), folio);

    if (!vmemmap_should_optimize_folio(h, folio))
        return ret;

    static_branch_inc(&hugetlb_optimize_vmemmap_key);

    if (flags & VMEMMAP_SYNCHRONIZE_RCU)
        synchronize_rcu();

    folio_set_hugetlb_vmemmap_optimized(folio);

    vmemmap_end     = vmemmap_start + hugetlb_vmemmap_size(h) {
        return pages_per_huge_page(h) * sizeof(struct page);
    }
    vmemmap_reuse   = vmemmap_start;
    vmemmap_start   += HUGETLB_VMEMMAP_RESERVE_SIZE;

    /* Remap the vmemmap virtual address range [@vmemmap_start, @vmemmap_end)
    * to the page which @vmemmap_reuse is mapped to.  Add pages previously
    * mapping the range to vmemmap_pages list so that they can be freed by
    * the caller. */
    ret = vmemmap_remap_free(vmemmap_start, vmemmap_end, vmemmap_reuse, vmemmap_pages, flags) {
        int ret;
        struct vmemmap_remap_walk walk = {
            .remap_pte      = vmemmap_remap_pte,
            .reuse_addr     = reuse,
            .reuse_page     = NULL,
            .vmemmap_pages  = vmemmap_pages,
            .flags          = flags,
        };
        int nid = page_to_nid((struct page *)reuse);
        gfp_t gfp_mask = GFP_KERNEL | __GFP_NORETRY | __GFP_NOWARN;

        /* Allocate a new head vmemmap page to avoid breaking a contiguous
        * block of struct page memory when freeing it back to page allocator
        * in free_vmemmap_page_list(). */
        walk.reuse_page = alloc_pages_node(nid, gfp_mask, 0);
        if (walk.reuse_page) {
            copy_page(page_to_virt(walk.reuse_page), (void *)walk.reuse_addr);
            list_add(&walk.reuse_page->lru, vmemmap_pages);
            memmap_pages_add(1);
        }

        BUG_ON(start - reuse != PAGE_SIZE);

        ret = vmemmap_remap_range(reuse, end, &walk)  {
            int ret;
            mmap_read_lock(&init_mm);
            ret = walk_kernel_page_table_range(start, end, &vmemmap_remap_ops, NULL, walk);
            mmap_read_unlock(&init_mm);
            if (ret)
                return ret;

            if (walk->remap_pte && !(walk->flags & VMEMMAP_REMAP_NO_TLB_FLUSH))
                flush_tlb_kernel_range(start, end);

            return 0;
        }
        if (ret && walk.nr_walked) {
            end = reuse + walk.nr_walked * PAGE_SIZE;
            walk = (struct vmemmap_remap_walk) {
                .remap_pte      = vmemmap_restore_pte,
                .reuse_addr     = reuse,
                .vmemmap_pages  = vmemmap_pages,
                .flags          = 0,
            };

            vmemmap_remap_range(reuse, end, &walk);
        }

        return ret;
    }
    if (ret) {
        static_branch_dec(&hugetlb_optimize_vmemmap_key);
        folio_clear_hugetlb_vmemmap_optimized(folio);
    }

    return ret;
}
```

#### walk_kernel_page_table_range

```c
int walk_kernel_page_table_range(unsigned long start, unsigned long end,
        const struct mm_walk_ops *ops, pgd_t *pgd, void *private)
{
    struct mm_struct *mm = &init_mm;
    struct mm_walk walk = {
        .ops        = ops,
        .mm         = mm,
        .pgd        = pgd,
        .private    = private,
        .no_vma     = true,
        .vma        = NULL,
    };

    if (start >= end)
        return -EINVAL;
    if (!check_ops_valid(ops))
        return -EINVAL;

    mmap_assert_locked(mm);

    return walk_pgd_range(start, end, &walk) {
        pgd_t *pgd;
        unsigned long next;
        const struct mm_walk_ops *ops = walk->ops;
        bool has_handler = ops->p4d_entry || ops->pud_entry || ops->pmd_entry ||
            ops->pte_entry;
        bool has_install = ops->install_pte;
        int err = 0;

        if (walk->pgd)
            pgd = walk->pgd + pgd_index(addr);
        else
            pgd = pgd_offset(walk->mm, addr);

        do {
            next = pgd_addr_end(addr, end);
            if (pgd_none_or_clear_bad(pgd)) {
                if (has_install)
                    err = __p4d_alloc(walk->mm, pgd, addr);
                else if (ops->pte_hole)
                    err = ops->pte_hole(addr, next, 0, walk);
                if (err)
                    break;
                if (!has_install)
                    continue;
            }
            if (ops->pgd_entry) {
                err = ops->pgd_entry(pgd, addr, next, walk);
                if (err)
                    break;
            }
            if (has_handler || has_install)
                err = walk_p4d_range(pgd, addr, next, walk);
            if (err)
                break;
        } while (pgd++, addr = next, addr != end);

        return err;
    }
}

static int walk_p4d_range(pgd_t *pgd, unsigned long addr, unsigned long end,
              struct mm_walk *walk)
{
    p4d_t *p4d;
    unsigned long next;
    const struct mm_walk_ops *ops = walk->ops;
    bool has_handler = ops->pud_entry || ops->pmd_entry || ops->pte_entry;
    bool has_install = ops->install_pte;
    int err = 0;
    int depth = real_depth(1);

    p4d = p4d_offset(pgd, addr);
    do {
        next = p4d_addr_end(addr, end);
        if (p4d_none_or_clear_bad(p4d)) {
            if (has_install)
                err = __pud_alloc(walk->mm, p4d, addr);
            else if (ops->pte_hole)
                err = ops->pte_hole(addr, next, depth, walk);
            if (err)
                break;
            if (!has_install)
                continue;
        }
        if (ops->p4d_entry) {
            err = ops->p4d_entry(p4d, addr, next, walk);
            if (err)
                break;
        }
        if (has_handler || has_install)
            err = walk_pud_range(p4d, addr, next, walk);
        if (err)
            break;
    } while (p4d++, addr = next, addr != end);

    return err;
}

int walk_pud_range(p4d_t *p4d, unsigned long addr, unsigned long end,
              struct mm_walk *walk)
{
    pud_t *pud;
    unsigned long next;
    const struct mm_walk_ops *ops = walk->ops;
    bool has_handler = ops->pmd_entry || ops->pte_entry;
    bool has_install = ops->install_pte;
    int err = 0;
    int depth = real_depth(2);

    pud = pud_offset(p4d, addr);
    do {
 again:
        next = pud_addr_end(addr, end);
        if (pud_none(*pud)) {
            if (has_install)
                err = __pmd_alloc(walk->mm, pud, addr);
            else if (ops->pte_hole)
                err = ops->pte_hole(addr, next, depth, walk);
            if (err)
                break;
            if (!has_install)
                continue;
        }

        walk->action = ACTION_SUBTREE;

        if (ops->pud_entry)
            err = ops->pud_entry(pud, addr, next, walk);
        if (err)
            break;

        if (walk->action == ACTION_AGAIN)
            goto again;
        if (walk->action == ACTION_CONTINUE)
            continue;

        if (!has_handler) { /* No handlers for lower page tables. */
            if (!has_install)
                continue; /* Nothing to do. */
            /* We are ONLY installing, so avoid unnecessarily
             * splitting a present huge page. */
            if (pud_present(*pud) && pud_trans_huge(*pud))
                continue;
        }

        if (walk->vma)
            split_huge_pud(walk->vma, pud, addr);
        else if (pud_leaf(*pud) || !pud_present(*pud))
            continue; /* Nothing to do. */

        if (pud_none(*pud))
            goto again;

        err = walk_pmd_range(pud, addr, next, walk);
        if (err)
            break;
    } while (pud++, addr = next, addr != end);

    return err;
}

int walk_pmd_range(pud_t *pud, unsigned long addr, unsigned long end,
              struct mm_walk *walk)
{
    pmd_t *pmd;
    unsigned long next;
    const struct mm_walk_ops *ops = walk->ops;
    bool has_handler = ops->pte_entry;
    bool has_install = ops->install_pte;
    int err = 0;
    int depth = real_depth(3);

    pmd = pmd_offset(pud, addr);
    do {
again:
        next = pmd_addr_end(addr, end);
        if (pmd_none(*pmd)) {
            if (has_install)
                err = __pte_alloc(walk->mm, pmd);
            else if (ops->pte_hole)
                err = ops->pte_hole(addr, next, depth, walk);
            if (err)
                break;
            if (!has_install)
                continue;
        }

        walk->action = ACTION_SUBTREE;

        if (ops->pmd_entry)
            err = ops->pmd_entry(pmd, addr, next, walk);
        if (err)
            break;

        if (walk->action == ACTION_AGAIN)
            goto again;
        if (walk->action == ACTION_CONTINUE)
            continue;

        if (!has_handler) { /* No handlers for lower page tables. */
            if (!has_install)
                continue; /* Nothing to do. */
            /* We are ONLY installing, so avoid unnecessarily
             * splitting a present huge page. */
            if (pmd_present(*pmd) && pmd_trans_huge(*pmd))
                continue;
        }

        if (walk->vma)
            split_huge_pmd(walk->vma, pmd, addr);
        else if (pmd_leaf(*pmd) || !pmd_present(*pmd))
            continue; /* Nothing to do. */

        err = walk_pte_range(pmd, addr, next, walk);
        if (err)
            break;

        if (walk->action == ACTION_AGAIN)
            goto again;

    } while (pmd++, addr = next, addr != end);

    return err;
}

int walk_pte_range(pmd_t *pmd, unsigned long addr, unsigned long end,
              struct mm_walk *walk)
{
    pte_t *pte;
    int err = 0;
    spinlock_t *ptl;

    if (walk->no_vma) {
        /* pte_offset_map() might apply user-specific validation.
         * Indeed, on x86_64 the pmd entries set up by init_espfix_ap()
         * fit its pmd_bad() check (_PAGE_NX set and _PAGE_RW clear),
         * and CONFIG_EFI_PGT_DUMP efi_mm goes so far as to walk them. */
        if (walk->mm == &init_mm || addr >= TASK_SIZE)
            pte = pte_offset_kernel(pmd, addr);
        else
            pte = pte_offset_map(pmd, addr);

        if (pte) {
            err = walk_pte_range_inner(pte, addr, end, walk);
            if (walk->mm != &init_mm && addr < TASK_SIZE)
                pte_unmap(pte);
        }
    } else {
        pte = pte_offset_map_lock(walk->mm, pmd, addr, &ptl);
        if (pte) {
            err = walk_pte_range_inner(pte, addr, end, walk);
            pte_unmap_unlock(pte, ptl);
        }
    }
    if (!pte)
        walk->action = ACTION_AGAIN;
    return err;
}

static int walk_pte_range_inner(pte_t *pte, unsigned long addr,
                unsigned long end, struct mm_walk *walk)
{
    const struct mm_walk_ops *ops = walk->ops;
    int err = 0;

    for (;;) {
        if (ops->install_pte && pte_none(ptep_get(pte))) {
            pte_t new_pte;

            err = ops->install_pte(addr, addr + PAGE_SIZE, &new_pte, walk);
            if (err)
                break;

            set_pte_at(walk->mm, addr, pte, new_pte);
            /* Non-present before, so for arches that need it. */
            if (!WARN_ON_ONCE(walk->no_vma))
                update_mmu_cache(walk->vma, addr, pte);
        } else {
            err = ops->pte_entry(pte, addr, addr + PAGE_SIZE, walk);
            if (err)
                break;
        }
        if (addr >= end - PAGE_SIZE)
            break;
        addr += PAGE_SIZE;
        pte++;
    }
    return err;
}
```

#### vmemmap_remap_ops

```c
static const struct mm_walk_ops vmemmap_remap_ops = {
    .pmd_entry    = vmemmap_pmd_entry,
    .pte_entry    = vmemmap_pte_entry,
};

int vmemmap_pmd_entry(pmd_t *pmd, unsigned long addr,
                 unsigned long next, struct mm_walk *walk)
{
    int ret = 0;
    struct page *head;
    struct vmemmap_remap_walk *vmemmap_walk = walk->private;

    /* Only splitting, not remapping the vmemmap pages. */
    if (!vmemmap_walk->remap_pte)
        walk->action = ACTION_CONTINUE;

    spin_lock(&init_mm.page_table_lock);
    head = pmd_leaf(*pmd) ? pmd_page(*pmd) : NULL;
    /* Due to HugeTLB alignment requirements and the vmemmap
     * pages being at the start of the hotplugged memory
     * region in memory_hotplug.memmap_on_memory case. Checking
     * the vmemmap page associated with the first vmemmap page
     * if it is self-hosted is sufficient.
     *
     * [                  hotplugged memory                  ]
     * [        section        ][...][        section        ]
     * [ vmemmap ][              usable memory               ]
     *   ^  | ^                        |
     *   +--+ |                        |
     *        +------------------------+ */
    if (IS_ENABLED(CONFIG_MEMORY_HOTPLUG) && unlikely(!vmemmap_walk->nr_walked)) {
        struct page *page = head
            ? head + pte_index(addr)
            :  pte_page(ptep_get(pte_offset_kernel(pmd, addr)));

        if (PageVmemmapSelfHosted(page))
            ret = -ENOTSUPP;
    }
    spin_unlock(&init_mm.page_table_lock);

    if (!head || ret)
        return ret;

    /* The vmemmap area, which holds the metadata (struct page array) for a huge page,
     * is often initially mapped by a single 2MB PMD entry. To remap the tail(0-7) pages,
     * this huge vmemmap PMD needs to be split */
    return vmemmap_split_pmd(pmd, head, addr & PMD_MASK, vmemmap_walk) {
        pmd_t __pmd;
        int i;
        unsigned long addr = start;
        pte_t *pgtable;

        /* pte_t is unsinged long which is 8 bytes,
         * pgtable is 4KB which can comodate 4KB / 8 = 512 pte_t */
        pgtable = pte_alloc_one_kernel(&init_mm);
        if (!pgtable)
            return -ENOMEM;

        pmd_populate_kernel(&init_mm, &__pmd, pgtable);

        /* E.g., PMD points to a huge 2MB memory which contains
         * 512 struct page to represent 512 4kb-sized page */
        for (i = 0; i < PTRS_PER_PTE; i++, addr += PAGE_SIZE) {
            pte_t entry, *pte;
            pgprot_t pgprot = PAGE_KERNEL;

            entry = mk_pte(head + i, pgprot) { /* phys addr of next level PTE table */
                pfn = page_to_pfn(page) {
                    return (unsigned long)((page) - vmemmap);
                }
                return pfn_pte(pfn, pgprot) {
                    phys = __pfn_to_phys(pfn) {
                        return ((phys_addr_t)(x) << PAGE_SHIFT);
                    }
                    return __pte(phys | pgprot_val(prot))
                }
            }
            pte = pte_offset_kernel(&__pmd, addr);
            set_pte_at(&init_mm, addr, pte, entry);
        }

        spin_lock(&init_mm.page_table_lock);
        if (likely(pmd_leaf(*pmd))) { /* (pmd_present(pmd) && !pmd_table(pmd)) */
            /* Higher order allocations from buddy allocator must be able to
            * be treated as indepdenent small pages (as they can be freed individually). */
            if (!PageReserved(head)) {
                split_page(head, get_order(PMD_SIZE)) {
                    int i;

                    VM_BUG_ON_PAGE(PageCompound(page), page);
                    VM_BUG_ON_PAGE(!page_count(page), page);

                    for (i = 1; i < (1 << order); i++)
                        set_page_refcounted(page + i);
                    split_page_owner(page, order, 0);
                    pgalloc_tag_split(page_folio(page), order, 0);
                    split_page_memcg(page, order);
                }
            }

            /* Make pte visible before pmd. See comment in pmd_install(). */
            smp_wmb();
            pmd_populate_kernel(&init_mm, pmd, pgtable)  {
                __pmd_populate(pmdp, __pa(ptep), PMD_TYPE_TABLE | PMD_TABLE_AF | PMD_TABLE_UXN); {
                    set_pmd(pmdp, __pmd(__phys_to_pmd_val(ptep) | prot)) {
                        WRITE_ONCE(*pmdp, pmd);

                        if (pmd_valid(pmd)) {
                            queue_pte_barriers() {
                                unsigned long flags;

                                if (in_interrupt()) {
                                    emit_pte_barriers();
                                    return;
                                }

                                flags = read_thread_flags();

                                if (flags & BIT(TIF_LAZY_MMU)) {
                                    /* Avoid the atomic op if already set. */
                                    if (!(flags & BIT(TIF_LAZY_MMU_PENDING)))
                                        set_thread_flag(TIF_LAZY_MMU_PENDING);
                                } else {
                                    emit_pte_barriers() {
                                        dsb(ishst);
                                        isb();
                                    }
                                }
                            }
                        }
                    }
                }
            }
            if (!(walk->flags & VMEMMAP_SPLIT_NO_TLB_FLUSH))
                flush_tlb_kernel_range(start, start + PMD_SIZE);
        } else {
            pte_free_kernel(&init_mm, pgtable) {
                pagetable_dtor_free(virt_to_ptdesc(pte)) {
                    pagetable_dtor(ptdesc) {
                        struct folio *folio = ptdesc_folio(ptdesc);

                        ptlock_free(ptdesc);
                        __folio_clear_pgtable(folio);
                        lruvec_stat_sub_folio(folio, NR_PAGETABLE);
                    }

                    pagetable_free(ptdesc) {
                        struct page *page = ptdesc_page(pt);

                        __free_pages(page, compound_order(page));
                    }
                }
            }
        }
        spin_unlock(&init_mm.page_table_lock);

        return 0;
    }
}

static int vmemmap_pte_entry(pte_t *pte, unsigned long addr,
                 unsigned long next, struct mm_walk *walk)
{
    struct vmemmap_remap_walk *vmemmap_walk = walk->private;

    /* The reuse_page is found 'first' in page table walking before
     * starting remapping. */
    if (!vmemmap_walk->reuse_page)
        vmemmap_walk->reuse_page = pte_page(ptep_get(pte));
    else
        vmemmap_walk->remap_pte(pte, addr, vmemmap_walk); /* vmemmap_remap_pte */
    vmemmap_walk->nr_walked++;

    return 0;
}

/* pte: A pointer to the page table entry (PTE) in the vmemmap region being processed.
 * addr: The virtual address in the vmemmap region corresponding to the PTE. */
static void vmemmap_remap_pte(pte_t *pte, unsigned long addr,
                  struct vmemmap_remap_walk *walk)
{
    /* Remap the tail pages as read-only to catch illegal write operation
     * to the tail pages. */
    pgprot_t pgprot = PAGE_KERNEL_RO;
    struct page *page = pte_page(ptep_get(pte));
    pte_t entry;

    /* Remapping the head page requires r/w */
    if (unlikely(addr == walk->reuse_addr)) {
        pgprot = PAGE_KERNEL;
        list_del(&walk->reuse_page->lru);

        /* Makes sure that preceding stores to the page contents from
         * vmemmap_remap_free() become visible before the set_pte_at()
         * write. */
        smp_wmb();
    }

    entry = mk_pte(walk->reuse_page, pgprot);
    list_add(&page->lru, walk->vmemmap_pages);
    set_pte_at(&init_mm, addr, pte, entry);
}
```

### hugetlb_vmemmap_restore_folio

```c
int hugetlb_vmemmap_restore_folio(const struct hstate *h, struct folio *folio)
{
    return __hugetlb_vmemmap_restore_folio(h, folio, VMEMMAP_SYNCHRONIZE_RCU);
}

int __hugetlb_vmemmap_restore_folio(const struct hstate *h,
                       struct folio *folio, unsigned long flags)
{
    int ret;
    unsigned long vmemmap_start = (unsigned long)&folio->page, vmemmap_end;
    unsigned long vmemmap_reuse;

    VM_WARN_ON_ONCE_FOLIO(!folio_test_hugetlb(folio), folio);
    VM_WARN_ON_ONCE_FOLIO(folio_ref_count(folio), folio);

    if (!folio_test_hugetlb_vmemmap_optimized(folio))
        return 0;

    if (flags & VMEMMAP_SYNCHRONIZE_RCU)
        synchronize_rcu();

    vmemmap_end     = vmemmap_start + hugetlb_vmemmap_size(h);
    vmemmap_reuse   = vmemmap_start;
    vmemmap_start   += HUGETLB_VMEMMAP_RESERVE_SIZE;

    /* The pages which the vmemmap virtual address range [@vmemmap_start,
     * @vmemmap_end) are mapped to are freed to the buddy allocator, and
     * the range is mapped to the page which @vmemmap_reuse is mapped to.
     * When a HugeTLB page is freed to the buddy allocator, previously
     * discarded vmemmap pages must be allocated and remapping. */
    ret = vmemmap_remap_alloc(vmemmap_start, vmemmap_end, vmemmap_reuse, flags);
    if (!ret) {
        folio_clear_hugetlb_vmemmap_optimized(folio);
        static_branch_dec(&hugetlb_optimize_vmemmap_key);
    }

    return ret;
}

static int vmemmap_remap_alloc(unsigned long start, unsigned long end,
                   unsigned long reuse, unsigned long flags)
{
    LIST_HEAD(vmemmap_pages);
    struct vmemmap_remap_walk walk = {
        .remap_pte = vmemmap_restore_pte(pte_t *pte, unsigned long addr, struct vmemmap_remap_walk *walk) {
            pgprot_t pgprot = PAGE_KERNEL;
            struct page *page;
            void *to;

            BUG_ON(pte_page(ptep_get(pte)) != walk->reuse_page);

            page = list_first_entry(walk->vmemmap_pages, struct page, lru);
            list_del(&page->lru);
            to = page_to_virt(page);
            copy_page(to, (void *)walk->reuse_addr);
            reset_struct_pages(to) {
                struct page *from = start + NR_RESET_STRUCT_PAGE;

                BUILD_BUG_ON(NR_RESET_STRUCT_PAGE * 2 > PAGE_SIZE / sizeof(struct page));
                memcpy(start, from, sizeof(*from) * NR_RESET_STRUCT_PAGE);
            }

            /* Makes sure that preceding stores to the page contents become visible
            * before the set_pte_at() write. */
            smp_wmb();
            set_pte_at(&init_mm, addr, pte, mk_pte(page, pgprot));
        },
        .reuse_addr         = reuse,
        .vmemmap_pages      = &vmemmap_pages,
        .flags              = flags,
    };

    /* See the comment in the vmemmap_remap_free(). */
    BUG_ON(start - reuse != PAGE_SIZE);

    ret = alloc_vmemmap_page_list(start, end, &vmemmap_pages) {
        gfp_t gfp_mask = GFP_KERNEL | __GFP_RETRY_MAYFAIL;
        unsigned long nr_pages = (end - start) >> PAGE_SHIFT;
        int nid = page_to_nid((struct page *)start);
        struct page *page, *next;
        int i;

        for (i = 0; i < nr_pages; i++) {
            page = alloc_pages_node(nid, gfp_mask, 0);
            if (!page)
                goto out;
            list_add(&page->lru, list);
        }
        memmap_pages_add(nr_pages) {
            atomic_long_add(delta, &nr_memmap_pages);
        }

        return 0;
    out:
        list_for_each_entry_safe(page, next, list, lru)
            __free_page(page);
        return -ENOMEM;
    }
    if (ret)
        return -ENOMEM;

    return vmemmap_remap_range(reuse, end, &walk);
        --->
}
```

### hugetlb_folio_init_vmemmap

```c
void __init hugetlb_folio_init_vmemmap(struct folio *folio,
                          struct hstate *h,
                          unsigned long nr_pages)
{
    int ret;

    /* Prepare folio head */
    __folio_clear_reserved(folio);
    __folio_set_head(folio);
    ret = folio_ref_freeze(folio, 1);
    VM_BUG_ON(!ret);
    /* Initialize the necessary tail struct pages */
    hugetlb_folio_init_tail_vmemmap(folio, 1, nr_pages) {
        enum zone_type zone = zone_idx(folio_zone(folio));
        int nid = folio_nid(folio);
        unsigned long head_pfn = folio_pfn(folio);
        unsigned long pfn, end_pfn = head_pfn + end_page_number;
        int ret;

        for (pfn = head_pfn + start_page_number; pfn < end_pfn; pfn++) {
            struct page *page = pfn_to_page(pfn);

            __init_single_page(page, pfn, zone, nid) {
                mm_zero_struct_page(page);
                set_page_links(page, zone, nid, pfn) {
                    set_page_zone(page, zone) {
                        page->flags &= ~(ZONES_MASK << ZONES_PGSHIFT);
                        page->flags |= (zone & ZONES_MASK) << ZONES_PGSHIFT;
                    }
                    set_page_node(page, node) {
                        page->flags &= ~(NODES_MASK << NODES_PGSHIFT);
                        page->flags |= (node & NODES_MASK) << NODES_PGSHIFT;
                    }
                    set_page_section(page, pfn_to_section_nr(pfn)) {
                        page->flags &= ~(SECTIONS_MASK << SECTIONS_PGSHIFT);
                        page->flags |= (section & SECTIONS_MASK) << SECTIONS_PGSHIFT;
                    }
                }
                init_page_count(page);
                atomic_set(&page->_mapcount, -1);
                page_cpupid_reset_last(page);
                page_kasan_tag_reset(page);

                INIT_LIST_HEAD(&page->lru);
            #ifdef WANT_PAGE_VIRTUAL
                /* The shift won't overflow because ZONE_NORMAL is below 4G. */
                if (!is_highmem_idx(zone))
                    set_page_address(page, __va(pfn << PAGE_SHIFT));
            #endif
            }
            prep_compound_tail((struct page *)folio, pfn - head_pfn);
            ret = page_ref_freeze(page, 1);
            VM_BUG_ON(!ret);
        }
    }
    prep_compound_head((struct page *)folio, huge_page_order(h));
}
```

## THP

* [LWN - Transparent huge pages in the page cache](https://lwn.net/Articles/686690/)

**Config Macro**
* CONFIG_TRANSPARENT_HUGEPAGE
* CONFIG_TRANSPARENT_HUGEPAGE_ALWAYS
* CONFIG_TRANSPARENT_HUGEPAGE_MADVISE
* CONFIG_TRANSPARENT_HUGE_PAGECACHE

**kernel boot args to enable mTHP**
* transparent_hugepage=always
* transparent_hugepage=madvise
* transparent_hugepage=never

```sh
/sys/kernel/mm/transparent_hugepage/ # Main mTHP configuration directory
├── enabled                          # Global THP/mTHP enablement
│                                    # Values: always, madvise, never
│
├── defrag                           # Defragmentation behavior
│                                    # Values: always, defer, defer+madvise, madvise, never
│
├── shmem_enabled                    # THP/mTHP for shmem (shared memory)
│                                    # Values: always, within_size, advise, never, deny, force
│
├── use_zero_page                    # Use zero page for huge pages (0 or 1)
├── hpage_pmd_size                   # Default huge page size for PMD (e.g., 2MB)
├── khugepaged/                      # khugepaged daemon configuration
│   ├── alloc_sleep_millisecs        # Sleep time between allocation attempts
│   ├── scan_sleep_millisecs         # Sleep time between scans
│   ├── max_ptes_none                # Max PTEs with no mapping
│   ├── max_ptes_swap                # Max PTEs in swap
│   ├── max_ptes_shared              # Max PTEs in shared mappings
│   ├── pages_to_scan                # Pages to scan per cycle
│   └── defrag                       # khugepaged defragmentation (0 or 1)
├── hugepages-*/                     # Directories for specific huge page sizes
│   ├── [size]kB/                    # e.g., hugepages-64kB, hugepages-512kB, etc.
│   │   ├── enabled                  # Per-size enablement
│   │   │                            # Values: always, madvise, inherit
│   │   ├── nr_hugepages             # Number of huge pages allocated
│   │   ├── free_hugepages           # Number of free huge pages
│   │   ├── resv_hugepages           # Number of reserved huge pages
│   │   ├── surplus_hugepages        # Number of surplus huge pages
│   │   └── numa/                    # NUMA-specific configuration (if NUMA enabled)
│   │       ├── node[0-N]/           # Per-NUMA node settings
│   │       │   ├── nr_hugepages     # Huge pages allocated on this node
│   │       │   ├── free_hugepages   # Free huge pages on this node
│   │       │   └── surplus_hugepages # Surplus huge pages on this node

/sys/devices/system/node/            # NUMA node-specific mTHP info
├── node[0-N]/                       # Per-NUMA node directories
│   ├── hugepages/                   # HugeTLB/mTHP info for the node
│   │   ├── hugepages-[size]kB/      # e.g., hugepages-64kB, hugepages-2048kB
│   │       ├── nr_hugepages         # Number of huge pages on this node
│   │       ├── free_hugepages       # Free huge pages on this node
│   │       └── surplus_hugepages    # Surplus huge pages on this node
```

```c
static ssize_t enabled_store(struct kobject *kobj,
                struct kobj_attribute *attr,
                const char *buf, size_t count)
{
    ssize_t ret = count;

    if (sysfs_streq(buf, "always")) {
        clear_bit(TRANSPARENT_HUGEPAGE_REQ_MADV_FLAG, &transparent_hugepage_flags);
        set_bit(TRANSPARENT_HUGEPAGE_FLAG, &transparent_hugepage_flags);
    } else if (sysfs_streq(buf, "madvise")) {
        clear_bit(TRANSPARENT_HUGEPAGE_FLAG, &transparent_hugepage_flags);
        set_bit(TRANSPARENT_HUGEPAGE_REQ_MADV_FLAG, &transparent_hugepage_flags);
    } else if (sysfs_streq(buf, "never")) {
        clear_bit(TRANSPARENT_HUGEPAGE_FLAG, &transparent_hugepage_flags);
        clear_bit(TRANSPARENT_HUGEPAGE_REQ_MADV_FLAG, &transparent_hugepage_flags);
    } else
        ret = -EINVAL;

    if (ret > 0) {
        int err = start_stop_khugepaged();
        if (err)
            ret = err;
    }
    return ret;
}
```

## create_huge_pmd

```c
static inline vm_fault_t create_huge_pmd(struct vm_fault *vmf)
{
    struct vm_area_struct *vma = vmf->vma;
    if (vma_is_anonymous(vma))
        return do_huge_pmd_anonymous_page(vmf);
    if (vma->vm_ops->huge_fault)
        return vma->vm_ops->huge_fault(vmf, PMD_ORDER);
    return VM_FAULT_FALLBACK;
}

vm_fault_t do_huge_pmd_anonymous_page(struct vm_fault *vmf)
{
    struct vm_area_struct *vma = vmf->vma;
    unsigned long haddr = vmf->address & HPAGE_PMD_MASK;
    vm_fault_t ret;

    if (!thp_vma_suitable_order(vma, haddr, PMD_ORDER))
        return VM_FAULT_FALLBACK;

    ret = vmf_anon_prepare(vmf);
    if (ret)
        return ret;
    khugepaged_enter_vma(vma, vma->vm_flags);

    if (!(vmf->flags & FAULT_FLAG_WRITE) &&
            !mm_forbids_zeropage(vma->vm_mm) &&
            transparent_hugepage_use_zero_page()) {
        pgtable_t pgtable;
        struct folio *zero_folio;
        vm_fault_t ret;

        pgtable = pte_alloc_one(vma->vm_mm);
        if (unlikely(!pgtable))
            return VM_FAULT_OOM;
        zero_folio = mm_get_huge_zero_folio(vma->vm_mm);
        if (unlikely(!zero_folio)) {
            pte_free(vma->vm_mm, pgtable);
            count_vm_event(THP_FAULT_FALLBACK);
            return VM_FAULT_FALLBACK;
        }
        vmf->ptl = pmd_lock(vma->vm_mm, vmf->pmd);
        ret = 0;
        if (pmd_none(*vmf->pmd)) {
            ret = check_stable_address_space(vma->vm_mm);
            if (ret) {
                spin_unlock(vmf->ptl);
                pte_free(vma->vm_mm, pgtable);
            } else if (userfaultfd_missing(vma)) {
                spin_unlock(vmf->ptl);
                pte_free(vma->vm_mm, pgtable);
                ret = handle_userfault(vmf, VM_UFFD_MISSING);
                VM_BUG_ON(ret & VM_FAULT_FALLBACK);
            } else {
                set_huge_zero_folio(pgtable, vma->vm_mm, vma,
                           haddr, vmf->pmd, zero_folio);
                update_mmu_cache_pmd(vma, vmf->address, vmf->pmd);
                spin_unlock(vmf->ptl);
            }
        } else {
            spin_unlock(vmf->ptl);
            pte_free(vma->vm_mm, pgtable);
        }
        return ret;
    }

    return __do_huge_pmd_anonymous_page(vmf) {
        unsigned long haddr = vmf->address & HPAGE_PMD_MASK;
        struct vm_area_struct *vma = vmf->vma;
        struct folio *folio;
        pgtable_t pgtable;
        vm_fault_t ret = 0;

        folio = vma_alloc_anon_folio_pmd(vma, vmf->address);
        if (unlikely(!folio))
            return VM_FAULT_FALLBACK;

        pgtable = pte_alloc_one(vma->vm_mm);
        if (unlikely(!pgtable)) {
            ret = VM_FAULT_OOM;
            goto release;
        }

        vmf->ptl = pmd_lock(vma->vm_mm, vmf->pmd);
        if (unlikely(!pmd_none(*vmf->pmd))) {
            goto unlock_release;
        } else {
            ret = check_stable_address_space(vma->vm_mm);
            if (ret)
                goto unlock_release;

            /* Deliver the page fault to userland */
            if (userfaultfd_missing(vma)) {
                spin_unlock(vmf->ptl);
                folio_put(folio);
                pte_free(vma->vm_mm, pgtable);
                ret = handle_userfault(vmf, VM_UFFD_MISSING);
                VM_BUG_ON(ret & VM_FAULT_FALLBACK);
                return ret;
            }
            pgtable_trans_huge_deposit(vma->vm_mm, vmf->pmd, pgtable);

            map_anon_folio_pmd(folio, vmf->pmd, vma, haddr) {
                pmd_t entry;

                entry = folio_mk_pmd(folio, vma->vm_page_prot);
                entry = maybe_pmd_mkwrite(pmd_mkdirty(entry), vma);
                folio_add_new_anon_rmap(folio, vma, haddr, RMAP_EXCLUSIVE);
                folio_add_lru_vma(folio, vma);
                set_pmd_at(vma->vm_mm, haddr, pmd, entry);
                update_mmu_cache_pmd(vma, haddr, pmd);
                add_mm_counter(vma->vm_mm, MM_ANONPAGES, HPAGE_PMD_NR);
                count_vm_event(THP_FAULT_ALLOC);
                count_mthp_stat(HPAGE_PMD_ORDER, MTHP_STAT_ANON_FAULT_ALLOC);
                count_memcg_event_mm(vma->vm_mm, THP_FAULT_ALLOC);
            }

            mm_inc_nr_ptes(vma->vm_mm);
            deferred_split_folio(folio, false) {
                struct deferred_split *ds_queue = get_deferred_split_queue(folio);
                struct mem_cgroup *memcg = folio_memcg(folio);
                unsigned long flags;

                if (folio_order(folio) <= 1)
                    return;

                if (!partially_mapped && !split_underused_thp)
                    return;

                if (folio_test_swapcache(folio))
                    return;

                spin_lock_irqsave(&ds_queue->split_queue_lock, flags);
                if (partially_mapped) {
                    if (!folio_test_partially_mapped(folio)) {
                        folio_set_partially_mapped(folio);
                        if (folio_test_pmd_mappable(folio))
                            count_vm_event(THP_DEFERRED_SPLIT_PAGE);
                        count_mthp_stat(folio_order(folio), MTHP_STAT_SPLIT_DEFERRED);
                        mod_mthp_stat(folio_order(folio), MTHP_STAT_NR_ANON_PARTIALLY_MAPPED, 1);

                    }
                } else {
                    /* partially mapped folios cannot become non-partially mapped */
                    VM_WARN_ON_FOLIO(folio_test_partially_mapped(folio), folio);
                }
                if (list_empty(&folio->_deferred_list)) {
                    list_add_tail(&folio->_deferred_list, &ds_queue->split_queue);
                    ds_queue->split_queue_len++;
                    if (memcg)
                        set_shrinker_bit(memcg, folio_nid(folio), deferred_split_shrinker->id);
                }
                spin_unlock_irqrestore(&ds_queue->split_queue_lock, flags);
            }
            spin_unlock(vmf->ptl);
        }

        return 0;

    unlock_release:
        spin_unlock(vmf->ptl);
    release:
        if (pgtable)
            pte_free(vma->vm_mm, pgtable);
        folio_put(folio);
        return ret;
    }
}
```

# hugetlbfs

```c
static struct file_system_type hugetlbfs_fs_type = {
    .name               = "hugetlbfs",
    .init_fs_context    = hugetlbfs_init_fs_context,
    .parameters         = hugetlb_fs_parameters,
    .kill_sb            = kill_litter_super,
    .fs_flags           = FS_ALLOW_IDMAP,
};

static const struct super_operations hugetlbfs_ops = {
    .alloc_inode    = hugetlbfs_alloc_inode,
    .free_inode     = hugetlbfs_free_inode,
    .destroy_inode  = hugetlbfs_destroy_inode,
    .evict_inode    = c,
    .statfs         = hugetlbfs_statfs,
    .put_super      = hugetlbfs_put_super,
    .show_options   = hugetlbfs_show_options,
};

struct hugetlbfs_inode_info {
    struct inode        vfs_inode;
    unsigned int        seals;
};

static const struct inode_operations hugetlbfs_inode_operations = {
    .setattr    = hugetlbfs_setattr,
};

static struct vfsmount *hugetlbfs_vfsmount[HUGE_MAX_HSTATE];

static const struct inode_operations hugetlbfs_dir_inode_operations = {
    .create         = hugetlbfs_create,
    .lookup         = simple_lookup,
    .link           = simple_link,
    .unlink         = simple_unlink,
    .symlink        = hugetlbfs_symlink,
    .mkdir          = hugetlbfs_mkdir,
    .rmdir          = simple_rmdir,
    .mknod          = hugetlbfs_mknod,
    .rename         = simple_rename,
    .setattr        = hugetlbfs_setattr,
    .tmpfile        = hugetlbfs_tmpfile,
};

static const struct file_operations hugetlbfs_file_operations = {
    .read_iter          = hugetlbfs_read_iter,
    .mmap               = hugetlbfs_file_mmap,
    .fsync              = noop_fsync,
    .get_unmapped_area  = hugetlb_get_unmapped_area,
    .llseek             = default_llseek,
    .fallocate          = hugetlbfs_fallocate,
    .fop_flags          = FOP_HUGE_PAGES,
};

const struct vm_operations_struct hugetlb_vm_ops = {
    .fault      = hugetlb_vm_op_fault,
    .open       = hugetlb_vm_op_open,
    .close      = hugetlb_vm_op_close,
    .may_split  = hugetlb_vm_op_split,
    .pagesize   = hugetlb_vm_op_pagesize,
};
```

```c
struct hugetlbfs_sb_info {
    long        max_inodes;   /* inodes allowed */
    long        free_inodes;  /* inodes free */
    spinlock_t  stat_lock;
    struct hstate           *hstate;
    struct hugepage_subpool *spool;
    kuid_t                  uid;
    kgid_t                  gid;
    umode_t                 mode;
};

/* There is a struct hstate associated with each huge page size. The hstate tracks all huge pages of the specified size.
 * A subpool represents a subset of pages within a hstate that is associated with a mounted hugetlbfs filesystem. */

struct hugepage_subpool {
    spinlock_t lock;
    long count;
    long max_hpages;    /* Maximum huge pages or -1 if no maximum. */
    long used_hpages;   /* Used count against maximum, includes */
                        /* both allocated and reserved pages. */
    struct hstate *hstate;
    long min_hpages;    /* Minimum huge pages or -1 if no minimum. */
    long rsv_hpages;    /* Pages reserved against global pool to */
                        /* satisfy minimum size. */
};
```

```c
static const struct ctl_table hugetlb_table[] = {
    {
        .procname       = "nr_hugepages",
        .data           = NULL,
        .maxlen         = sizeof(unsigned long),
        .mode           = 0644,
        .proc_handler   = hugetlb_sysctl_handler,
    },
#ifdef CONFIG_NUMA
    {
        .procname       = "nr_hugepages_mempolicy",
        .data           = NULL,
        .maxlen         = sizeof(unsigned long),
        .mode           = 0644,
        .proc_handler   = &hugetlb_mempolicy_sysctl_handler,
    },
#endif
    {
        .procname       = "hugetlb_shm_group",
        .data           = &sysctl_hugetlb_shm_group,
        .maxlen         = sizeof(gid_t),
        .mode           = 0644,
        .proc_handler   = proc_dointvec,
    },
    {
        .procname       = "nr_overcommit_hugepages",
        .data           = NULL,
        .maxlen         = sizeof(unsigned long),
        .mode           = 0644,
        .proc_handler   = hugetlb_overcommit_handler,
    },
};
```

## init_hugetlbfs_fs

```c
int __init init_hugetlbfs_fs(void)
{
    struct vfsmount *mnt;
    struct hstate *h;
    int error;
    int i;

    if (!hugepages_supported()) {
        pr_info("disabling because there are no supported hugepage sizes\n");
        return -ENOTSUPP;
    }

    error = -ENOMEM;
    hugetlbfs_inode_cachep = kmem_cache_create("hugetlbfs_inode_cache",
                    sizeof(struct hugetlbfs_inode_info),
                    0, SLAB_ACCOUNT, init_once);
    if (hugetlbfs_inode_cachep == NULL)
        goto out;

    error = register_filesystem(&hugetlbfs_fs_type);
    if (error)
        goto out_free;

    /* default hstate mount is required */
    mnt = mount_one_hugetlbfs(&default_hstate);
    if (IS_ERR(mnt)) {
        error = PTR_ERR(mnt);
        goto out_unreg;
    }
    hugetlbfs_vfsmount[default_hstate_idx] = mnt;

    /* other hstates are optional */
    i = 0;
    for_each_hstate(h) {
        if (i == default_hstate_idx) {
            i++;
            continue;
        }

        mnt = mount_one_hugetlbfs(h);
        if (IS_ERR(mnt))
            hugetlbfs_vfsmount[i] = NULL;
        else
            hugetlbfs_vfsmount[i] = mnt;
        i++;
    }

    return 0;

 out_unreg:
    (void)unregister_filesystem(&hugetlbfs_fs_type);
 out_free:
    kmem_cache_destroy(hugetlbfs_inode_cachep);
 out:
    return error;
}

static struct vfsmount *__init mount_one_hugetlbfs(struct hstate *h)
{
    struct fs_context *fc;
    struct vfsmount *mnt;

    fc = fs_context_for_mount(&hugetlbfs_fs_type, SB_KERNMOUNT) {
        return alloc_fs_context(fs_type, NULL, sb_flags, 0, FS_CONTEXT_FOR_MOUNT) {
            int (*init_fs_context)(struct fs_context *);
            struct fs_context *fc;
            int ret = -ENOMEM;

            fc = kzalloc(sizeof(struct fs_context), GFP_KERNEL_ACCOUNT);
            if (!fc)
                return ERR_PTR(-ENOMEM);

            fc->purpose         = purpose;
            fc->sb_flags        = sb_flags;
            fc->sb_flags_mask   = sb_flags_mask;
            fc->fs_type         = get_filesystem(fs_type);
            fc->cred            = get_current_cred();
            fc->net_ns          = get_net(current->nsproxy->net_ns);
            fc->log.prefix      = fs_type->name;

            mutex_init(&fc->uapi_mutex);

            switch (purpose) {
            case FS_CONTEXT_FOR_MOUNT:
                fc->user_ns = get_user_ns(fc->cred->user_ns);
                break;
            case FS_CONTEXT_FOR_SUBMOUNT:
                fc->user_ns = get_user_ns(reference->d_sb->s_user_ns);
                break;
            case FS_CONTEXT_FOR_RECONFIGURE:
                atomic_inc(&reference->d_sb->s_active);
                fc->user_ns = get_user_ns(reference->d_sb->s_user_ns);
                fc->root = dget(reference);
                break;
            }

            /* TODO: Make all filesystems support this unconditionally */
            init_fs_context = fc->fs_type->init_fs_context;
            if (!init_fs_context)
                init_fs_context = legacy_init_fs_context;

            ret = init_fs_context(fc); /* hugetlbfs_init_fs_context */
            if (ret < 0)
                goto err_fc;
            fc->need_free = true;
            return fc;
        }
    }
    if (IS_ERR(fc)) {
        mnt = ERR_CAST(fc);
    } else {
        struct hugetlbfs_fs_context *ctx = fc->fs_private;
        ctx->hstate = h;
        mnt = fc_mount_longterm(fc);
        put_fs_context(fc);
    }
    if (IS_ERR(mnt))
        pr_err("Cannot mount internal hugetlbfs for page size %luK",
               huge_page_size(h) / SZ_1K);
    return mnt;
}

static int hugetlbfs_init_fs_context(struct fs_context *fc)
{
    struct hugetlbfs_fs_context *ctx;

    ctx = kzalloc(sizeof(struct hugetlbfs_fs_context), GFP_KERNEL);
    if (!ctx)
        return -ENOMEM;

    ctx->max_hpages    = -1; /* No limit on size by default */
    ctx->nr_inodes    = -1; /* No limit on number of inodes by default */
    ctx->uid    = current_fsuid();
    ctx->gid    = current_fsgid();
    ctx->mode    = 0755;
    ctx->hstate    = &default_hstate;
    ctx->min_hpages    = -1; /* No default minimum size */
    ctx->max_val_type = NO_SIZE;
    ctx->min_val_type = NO_SIZE;
    fc->fs_private = ctx;
    fc->ops    = &hugetlbfs_fs_context_ops;
    return 0;
}

static const struct fs_context_operations hugetlbfs_fs_context_ops = {
    .free           = hugetlbfs_fs_context_free,
    .parse_param    = hugetlbfs_parse_param,
    .get_tree       = hugetlbfs_get_tree,
};

static int hugetlbfs_get_tree(struct fs_context *fc)
{
    int err = hugetlbfs_validate(fc);
    if (err)
        return err;
    return get_tree_nodev(fc, hugetlbfs_fill_super) {
        return vfs_get_super(fc, NULL, fill_super) {
            struct super_block *sb;
            int err;

            /* Find or create a superblock */
            sb = sget_fc(fc, test, set_anon_super_fc);
            if (IS_ERR(sb))
                return PTR_ERR(sb);

            if (!sb->s_root) {
                err = fill_super(sb, fc);
                if (err)
                    goto error;

                sb->s_flags |= SB_ACTIVE;
            }

            fc->root = dget(sb->s_root);
            return 0;
        }
    }
}
```

## hugetlbfs_fill_super

```c
static int
hugetlbfs_fill_super(struct super_block *sb, struct fs_context *fc)
{
    struct hugetlbfs_fs_context *ctx = fc->fs_private;
    struct hugetlbfs_sb_info *sbinfo;

    sbinfo = kmalloc(sizeof(struct hugetlbfs_sb_info), GFP_KERNEL);
    if (!sbinfo)
        return -ENOMEM;
    sb->s_fs_info = sbinfo;
    spin_lock_init(&sbinfo->stat_lock);
    sbinfo->hstate          = ctx->hstate;
    sbinfo->max_inodes      = ctx->nr_inodes;
    sbinfo->free_inodes     = ctx->nr_inodes;
    sbinfo->spool           = NULL;
    sbinfo->uid             = ctx->uid;
    sbinfo->gid             = ctx->gid;
    sbinfo->mode            = ctx->mode;

    /* Allocate and initialize subpool if maximum or minimum size is
     * specified.  Any needed reservations (for minimum size) are taken
     * when the subpool is created. */
    if (ctx->max_hpages != -1 || ctx->min_hpages != -1) {
        sbinfo->spool = hugepage_new_subpool(ctx->hstate, ctx->max_hpages, ctx->min_hpages) {

            struct hugepage_subpool *spool;

            spool = kzalloc(sizeof(*spool), GFP_KERNEL);
            if (!spool)
                return NULL;

            spin_lock_init(&spool->lock);
            spool->count = 1;
            spool->max_hpages = max_hpages;
            spool->hstate = h;
            spool->min_hpages = min_hpages;

            if (min_hpages != -1 && hugetlb_acct_memory(h, min_hpages)) {
                kfree(spool);
                return NULL;
            }
            spool->rsv_hpages = min_hpages;

            return spool;
        }
        if (!sbinfo->spool)
            goto out_free;
    }
    sb->s_maxbytes = MAX_LFS_FILESIZE;
    sb->s_blocksize = huge_page_size(ctx->hstate);
    sb->s_blocksize_bits = huge_page_shift(ctx->hstate);
    sb->s_magic = HUGETLBFS_MAGIC;
    sb->s_op = &hugetlbfs_ops;
    sb->s_d_flags = DCACHE_DONTCACHE;
    sb->s_time_gran = 1;

    /* Due to the special and limited functionality of hugetlbfs, it does
     * not work well as a stacking filesystem. */
    sb->s_stack_depth = FILESYSTEM_MAX_STACK_DEPTH;
    rtnd = hugetlbfs_get_root(sb, ctx) {
        struct inode *inode;

        inode = new_inode(sb);
        if (inode) {
            inode->i_ino = get_next_ino();
            inode->i_mode = S_IFDIR | ctx->mode;
            inode->i_uid = ctx->uid;
            inode->i_gid = ctx->gid;
            simple_inode_init_ts(inode);
            inode->i_op = &hugetlbfs_dir_inode_operations;
            inode->i_fop = &simple_dir_operations;
            /* directory inodes start off with i_nlink == 2 (for "." entry) */
            inc_nlink(inode);
            lockdep_annotate_inode_mutex_key(inode);
        }
        return inode;
    }
    sb->s_root = d_make_root(rtnd);
    if (!sb->s_root)
        goto out_free;
    return 0;
out_free:
    kfree(sbinfo->spool);
    kfree(sbinfo);
    return -ENOMEM;
}
```

## hugetlb_file_setup

```c
mmap() {
    ksys_mmap_pgoff() {
        if (flags & MAP_HUGETLB) {
            struct hstate *hs;

            hs = hstate_sizelog((flags >> MAP_HUGE_SHIFT) & MAP_HUGE_MASK);
            if (!hs)
                return -EINVAL;

            len = ALIGN(len, huge_page_size(hs));
            file = hugetlb_file_setup(HUGETLB_ANON_FILE, len,
                    VM_NORESERVE,
                    HUGETLB_ANONHUGE_INODE,
                    (flags >> MAP_HUGE_SHIFT) & MAP_HUGE_MASK);
            if (IS_ERR(file))
                return PTR_ERR(file);
        }

         vm_mmap_pgoff(file, addr, len, prot, flags, pgoff);
    }
}
```

```c
struct file *hugetlb_file_setup(const char *name, size_t size,
                vm_flags_t acctflag, int creat_flags,
                int page_size_log)
{
    struct inode *inode;
    struct vfsmount *mnt;
    int hstate_idx;
    struct file *file;

    hstate_idx = get_hstate_idx(page_size_log);
    if (hstate_idx < 0)
        return ERR_PTR(-ENODEV);

    mnt = hugetlbfs_vfsmount[hstate_idx];
    if (!mnt)
        return ERR_PTR(-ENOENT);

    if (creat_flags == HUGETLB_SHMFS_INODE && !can_do_hugetlb_shm()) {
        struct ucounts *ucounts = current_ucounts();

        if (user_shm_lock(size, ucounts)) {
            user_shm_unlock(size, ucounts);
        }
        return ERR_PTR(-EPERM);
    }

    file = ERR_PTR(-ENOSPC);
    /* hugetlbfs_vfsmount[] mounts do not use idmapped mounts.  */
    inode = hugetlbfs_get_inode(mnt->mnt_sb, &nop_mnt_idmap, NULL, S_IFREG | S_IRWXUGO, 0) {
        struct inode *inode;
        struct resv_map *resv_map = NULL;

        /* Reserve maps are only needed for inodes that can have associated
        * page allocations. */
        if (S_ISREG(mode) || S_ISLNK(mode)) {
            resv_map = resv_map_alloc();
            if (!resv_map)
                return NULL;
        }

        inode = new_inode(sb);
        if (inode) {
            struct hugetlbfs_inode_info *info = HUGETLBFS_I(inode);

            inode->i_ino = get_next_ino();
            inode_init_owner(idmap, inode, dir, mode);
            lockdep_set_class(&inode->i_mapping->i_mmap_rwsem, &hugetlbfs_i_mmap_rwsem_key);
            inode->i_mapping->a_ops = &hugetlbfs_aops;
            simple_inode_init_ts(inode);
            inode->i_mapping->i_private_data = resv_map;
            info->seals = F_SEAL_SEAL;

            switch (mode & S_IFMT) {
            default:
                init_special_inode(inode, mode, dev) {
                    inode->i_mode = mode;
                    if (S_ISCHR(mode)) {
                        inode->i_fop = &def_chr_fops;
                        inode->i_rdev = rdev;
                    } else if (S_ISBLK(mode)) {
                        if (IS_ENABLED(CONFIG_BLOCK))
                            inode->i_fop = &def_blk_fops;
                        inode->i_rdev = rdev;
                    } else if (S_ISFIFO(mode))
                        inode->i_fop = &pipefifo_fops;
                    else if (S_ISSOCK(mode))
                        ;    /* leave it no_open_fops */
                }
                break;
            case S_IFREG:
                inode->i_op = &hugetlbfs_inode_operations;
                inode->i_fop = &hugetlbfs_file_operations;
                break;
            case S_IFDIR:
                inode->i_op = &hugetlbfs_dir_inode_operations;
                inode->i_fop = &simple_dir_operations;

                /* directory inodes start off with i_nlink == 2 (for "." entry) */
                inc_nlink(inode);
                break;
            case S_IFLNK:
                inode->i_op = &page_symlink_inode_operations;
                inode_nohighmem(inode);
                break;
            }
            lockdep_annotate_inode_mutex_key(inode);
            trace_hugetlbfs_alloc_inode(inode, dir, mode);
        } else {
            if (resv_map)
                kref_put(&resv_map->refs, resv_map_release);
        }

        return inode;
    }
    if (!inode)
        goto out;
    if (creat_flags == HUGETLB_SHMFS_INODE)
        inode->i_flags |= S_PRIVATE;

    inode->i_size = size;
    clear_nlink(inode);

    if (hugetlb_reserve_pages(inode, 0,
            size >> huge_page_shift(hstate_inode(inode)), NULL, acctflag) < 0)
        file = ERR_PTR(-ENOMEM);
    else
        file = alloc_file_pseudo(inode, mnt, name, O_RDWR, &hugetlbfs_file_operations);
    if (!IS_ERR(file))
        return file;

    iput(inode);
out:
    return file;
}
```

## hugetlb_fault

```c
vm_fault_t handle_mm_fault(struct vm_area_struct *vma, unsigned long address,
               unsigned int flags, struct pt_regs *regs)
{
    /* return !!(vma->vm_flags & VM_HUGETLB); */
    if (unlikely(is_vm_hugetlb_page(vma))) {
        ret = hugetlb_fault(vma->vm_mm, vma, address, flags);
    } else {
        ret = __handle_mm_fault(vma, address, flags) {
            thp_allowed = thp_vma_allowable_order(vma, vm_flags, TVA_IN_PF | TVA_ENFORCE_SYSFS, PUD_ORDER);
            if (pud_none(*vmf.pud) && thp_allowed) {
                ret = create_huge_pud(&vmf);
                if (!(ret & VM_FAULT_FALLBACK))
                    return ret;
            }
        }
    }
}

vm_fault_t hugetlb_fault(struct mm_struct *mm, struct vm_area_struct *vma,
            unsigned long address, unsigned int flags)
{
    vm_fault_t ret;
    u32 hash;
    struct folio *folio = NULL;
    struct hstate *h = hstate_vma(vma) {
        return hstate_file(vma->vm_file) {
            return hstate_inode(file_inode(f) { return f->f_inode; }) {
                return HUGETLBFS_SB(i->i_sb){
                    return sb->s_fs_info;
                } ->hstate;
            }
        }
    }
    struct address_space *mapping;
    bool need_wait_lock = false;
    struct vm_fault vmf = {
        .vma = vma,
        .address = address & huge_page_mask(h),
        .real_address = address,
        .flags = flags,
        .pgoff = vma_hugecache_offset(h, vma, address & huge_page_mask(h)),
        /* TODO: Track hugetlb faults using vm_fault */

        /* Some fields may not be initialized, be careful as it may
        * be hard to debug if called functions make assumptions */
    };

    /* Serialize hugepage allocation and instantiation, so that we don't
    * get spurious allocation failures if two CPUs race to instantiate
    * the same page in the page cache. */
    mapping = vma->vm_file->f_mapping;
    hash = hugetlb_fault_mutex_hash(mapping, vmf.pgoff) {
        unsigned long key[2];
        u32 hash;

        key[0] = (unsigned long) mapping;
        key[1] = idx;

        hash = jhash2((u32 *)&key, sizeof(key)/(sizeof(u32)), 0);

        return hash & (num_fault_mutexes - 1);
    }
    mutex_lock(&hugetlb_fault_mutex_table[hash]);

    /* Acquire vma lock before calling huge_pte_alloc and hold
    * until finished with vmf.pte.  This prevents huge_pmd_unshare from
    * being called elsewhere and making the vmf.pte no longer valid. */
    hugetlb_vma_lock_read(vma);
    vmf.pte = huge_pte_alloc(mm, vma, vmf.address, huge_page_size(h));
    if (!vmf.pte) {
        hugetlb_vma_unlock_read(vma);
        mutex_unlock(&hugetlb_fault_mutex_table[hash]);
        return VM_FAULT_OOM;
    }

    vmf.orig_pte = huge_ptep_get(mm, vmf.address, vmf.pte) {
        int ncontig, i;
        size_t pgsize;
        pte_t orig_pte = __ptep_get(ptep);

        if (!pte_present(orig_pte) || !pte_cont(orig_pte))
            return orig_pte;

        ncontig = find_num_contig(mm, addr, ptep, &pgsize) {
            pgd_t *pgdp = pgd_offset(mm, addr);
            p4d_t *p4dp;
            pud_t *pudp;
            pmd_t *pmdp;

            *pgsize = PAGE_SIZE;
            p4dp = p4d_offset(pgdp, addr);
            pudp = pud_offset(p4dp, addr);
            pmdp = pmd_offset(pudp, addr);
            if ((pte_t *)pmdp == ptep) {
                *pgsize = PMD_SIZE;
                return CONT_PMDS;
            }
            return CONT_PTES;
        }

        for (i = 0; i < ncontig; i++, ptep++) {
            pte_t pte = __ptep_get(ptep);

            if (pte_dirty(pte))
                orig_pte = pte_mkdirty(orig_pte);

            if (pte_young(pte))
                orig_pte = pte_mkyoung(orig_pte);
        }
        return orig_pte;
    }

    if (huge_pte_none_mostly(vmf.orig_pte)) { /* huge_pte_none(pte) || is_pte_marker(pte) */
        if (is_pte_marker(vmf.orig_pte)) {
            pte_marker marker = pte_marker_get(pte_to_swp_entry(vmf.orig_pte));

            if (marker & PTE_MARKER_POISONED) {
                ret = VM_FAULT_HWPOISON_LARGE | VM_FAULT_SET_HINDEX(hstate_index(h));
                goto out_mutex;
            } else if (WARN_ON_ONCE(marker & PTE_MARKER_GUARD)) {
                /* This isn't supported in hugetlb. */
                ret = VM_FAULT_SIGSEGV;
                goto out_mutex;
            }
        }

        /* Other PTE markers should be handled the same way as none PTE.
        *
        * hugetlb_no_page will drop vma lock and hugetlb fault
        * mutex internally, which make us return immediately. */
        return hugetlb_no_page(mapping, &vmf);
    }

    ret = 0;

    /* Not present, either a migration or a hwpoisoned entry */
    if (!pte_present(vmf.orig_pte)) {
        if (is_hugetlb_entry_migration(vmf.orig_pte)) {
            mutex_unlock(&hugetlb_fault_mutex_table[hash]);
            migration_entry_wait_huge(vma, vmf.address, vmf.pte);
            return 0;
        } else if (is_hugetlb_entry_hwpoisoned(vmf.orig_pte))
            ret = VM_FAULT_HWPOISON_LARGE | VM_FAULT_SET_HINDEX(hstate_index(h));
        goto out_mutex;
    }

    /* If we are going to COW/unshare the mapping later, we examine the
    * pending reservations for this page now. This will ensure that any
    * allocations necessary to record that reservation occur outside the
    * spinlock. */
    if ((flags & (FAULT_FLAG_WRITE|FAULT_FLAG_UNSHARE)) &&
        !(vma->vm_flags & VM_MAYSHARE) && !huge_pte_write(vmf.orig_pte)) {

        if (vma_needs_reservation(h, vma, vmf.address) < 0) {
            ret = VM_FAULT_OOM;
            goto out_mutex;
        }
        /* Just decrements count, does not deallocate */
        vma_end_reservation(h, vma, vmf.address);
    }

    vmf.ptl = huge_pte_lock(h, mm, vmf.pte);

    /* Check for a racing update before calling hugetlb_wp() */
    if (unlikely(!pte_same(vmf.orig_pte, huge_ptep_get(mm, vmf.address, vmf.pte))))
        goto out_ptl;

    /* Handle userfault-wp first, before trying to lock more pages */
    if (userfaultfd_wp(vma) && huge_pte_uffd_wp(huge_ptep_get(mm, vmf.address, vmf.pte)) &&
        (flags & FAULT_FLAG_WRITE) && !huge_pte_write(vmf.orig_pte)) {
        if (!userfaultfd_wp_async(vma)) {
            spin_unlock(vmf.ptl);
            hugetlb_vma_unlock_read(vma);
            mutex_unlock(&hugetlb_fault_mutex_table[hash]);
            return handle_userfault(&vmf, VM_UFFD_WP);
        }

        vmf.orig_pte = huge_pte_clear_uffd_wp(vmf.orig_pte);
        set_huge_pte_at(mm, vmf.address, vmf.pte, vmf.orig_pte, huge_page_size(hstate_vma(vma)));
        /* Fallthrough to CoW */
    }

    if (flags & (FAULT_FLAG_WRITE|FAULT_FLAG_UNSHARE)) {
        if (!huge_pte_write(vmf.orig_pte)) {
            /* Anonymous folios need to be lock since hugetlb_wp()
            * checks whether we can re-use the folio exclusively
            * for us in case we are the only user of it. */
            folio = page_folio(pte_page(vmf.orig_pte));
            if (folio_test_anon(folio) && !folio_trylock(folio)) {
                need_wait_lock = true;
                goto out_ptl;
            }
            folio_get(folio);
            ret = hugetlb_wp(&vmf);
            if (folio_test_anon(folio))
                folio_unlock(folio);
            folio_put(folio);
            goto out_ptl;
        } else if (likely(flags & FAULT_FLAG_WRITE)) {
            vmf.orig_pte = huge_pte_mkdirty(vmf.orig_pte);
        }
    }
    vmf.orig_pte = pte_mkyoung(vmf.orig_pte);
    if (huge_ptep_set_access_flags(vma, vmf.address, vmf.pte, vmf.orig_pte, flags & FAULT_FLAG_WRITE))
        update_mmu_cache(vma, vmf.address, vmf.pte);

out_ptl:
    spin_unlock(vmf.ptl);
out_mutex:
    hugetlb_vma_unlock_read(vma);

    /* We must check to release the per-VMA lock. __vmf_anon_prepare() in
    * hugetlb_wp() is the only way ret can be set to VM_FAULT_RETRY. */
    if (unlikely(ret & VM_FAULT_RETRY))
        vma_end_read(vma);

    mutex_unlock(&hugetlb_fault_mutex_table[hash]);
    /* hugetlb_wp drops all the locks, but the folio lock, before trying to
    * unmap the folio from other processes. During that window, if another
    * process mapping that folio faults in, it will take the mutex and then
    * it will wait on folio_lock, causing an ABBA deadlock.
    * Use trylock instead and bail out if we fail.
    *
    * Ideally, we should hold a refcount on the folio we wait for, but we do
    * not want to use the folio after it becomes unlocked, but rather just
    * wait for it to become unlocked, so hopefully next fault successes on
    * the trylock. */
    if (need_wait_lock)
        folio_wait_locked(folio);
    return ret;
}
```

### hugetlb_no_page

```c
static vm_fault_t hugetlb_no_page(struct address_space *mapping,
            struct vm_fault *vmf)
{
    u32 hash = hugetlb_fault_mutex_hash(mapping, vmf->pgoff);
    bool new_folio, new_anon_folio = false;
    struct vm_area_struct *vma = vmf->vma;
    struct mm_struct *mm = vma->vm_mm;
    struct hstate *h = hstate_vma(vma);
    vm_fault_t ret = VM_FAULT_SIGBUS;
    bool folio_locked = true;
    struct folio *folio;
    unsigned long size;
    pte_t new_pte;

    /* Currently, we are forced to kill the process in the event the
    * original mapper has unmapped pages from the child due to a failed
    * COW/unsharing. Warn that such a situation has occurred as it may not
    * be obvious. */
    if (is_vma_resv_set(vma, HPAGE_RESV_UNMAPPED)) {
        pr_warn_ratelimited("PID %d killed due to inadequate hugepage pool\n",
            current->pid);
        goto out;
    }

    /* Use page lock to guard against racing truncation
    * before we get page_table_lock. */
    new_folio = false;
    folio = filemap_lock_hugetlb_folio(h, mapping, vmf->pgoff) {
        return filemap_lock_folio(mapping, idx << huge_page_order(h)) {
            return __filemap_get_folio(mapping, index, FGP_LOCK, 0);
        }
    }

    if (IS_ERR(folio)) {
        size = i_size_read(mapping->host) >> huge_page_shift(h);
        if (vmf->pgoff >= size)
            goto out;
        /* Check for page in userfault range */
        if (userfaultfd_missing(vma)) {
            /* Since hugetlb_no_page() was examining pte
            * without pgtable lock, we need to re-test under
            * lock because the pte may not be stable and could
            * have changed from under us.  Try to detect
            * either changed or during-changing ptes and retry
            * properly when needed.
            *
            * Note that userfaultfd is actually fine with
            * false positives (e.g. caused by pte changed),
            * but not wrong logical events (e.g. caused by
            * reading a pte during changing).  The latter can
            * confuse the userspace, so the strictness is very
            * much preferred.  E.g., MISSING event should
            * never happen on the page after UFFDIO_COPY has
            * correctly installed the page and returned. */
            if (!hugetlb_pte_stable(h, mm, vmf->address, vmf->pte, vmf->orig_pte)) {
                ret = 0;
                goto out;
            }

            return hugetlb_handle_userfault(vmf, mapping, VM_UFFD_MISSING);
        }

        if (!(vma->vm_flags & VM_MAYSHARE)) {
            ret = __vmf_anon_prepare(vmf);
                --->
            if (unlikely(ret))
                goto out;
        }

        folio = alloc_hugetlb_folio(vma, vmf->address, false);
            --->

        if (IS_ERR(folio)) {
            /* Returning error will result in faulting task being
            * sent SIGBUS.  The hugetlb fault mutex prevents two
            * tasks from racing to fault in the same page which
            * could result in false unable to allocate errors.
            * Page migration does not take the fault mutex, but
            * does a clear then write of pte's under page table
            * lock.  Page fault code could race with migration,
            * notice the clear pte and try to allocate a page
            * here.  Before returning error, get ptl and make
            * sure there really is no pte entry. */
            if (hugetlb_pte_stable(h, mm, vmf->address, vmf->pte, vmf->orig_pte))
                ret = vmf_error(PTR_ERR(folio));
            else
                ret = 0;
            goto out;
        }
        folio_zero_user(folio, vmf->real_address); /* memset(0) */
        __folio_mark_uptodate(folio);
        new_folio = true;

        if (vma->vm_flags & VM_MAYSHARE) {
            int err = hugetlb_add_to_page_cache(folio, mapping, vmf->pgoff);
            if (err) {
                /* err can't be -EEXIST which implies someone
                * else consumed the reservation since hugetlb
                * fault mutex is held when add a hugetlb page
                * to the page cache. So it's safe to call
                * restore_reserve_on_error() here. */
                restore_reserve_on_error(h, vma, vmf->address, folio);
                folio_put(folio);
                ret = VM_FAULT_SIGBUS;
                goto out;
            }
        } else {
            new_anon_folio = true;
            folio_lock(folio);
        }
    } else {
        /* If memory error occurs between mmap() and fault, some process
        * don't have hwpoisoned swap entry for errored virtual address.
        * So we need to block hugepage fault by PG_hwpoison bit check. */
        if (unlikely(folio_test_hwpoison(folio))) {
            ret = VM_FAULT_HWPOISON_LARGE | VM_FAULT_SET_HINDEX(hstate_index(h));
            goto backout_unlocked;
        }

        /* Check for page in userfault range. */
        if (userfaultfd_minor(vma)) {
            folio_unlock(folio);
            folio_put(folio);
            /* See comment in userfaultfd_missing() block above */
            if (!hugetlb_pte_stable(h, mm, vmf->address, vmf->pte, vmf->orig_pte)) {
                ret = 0;
                goto out;
            }
            return hugetlb_handle_userfault(vmf, mapping, VM_UFFD_MINOR);
        }
    }

    /* If we are going to COW a private mapping later, we examine the
    * pending reservations for this page now. This will ensure that
    * any allocations necessary to record that reservation occur outside
    * the spinlock. */
    if ((vmf->flags & FAULT_FLAG_WRITE) && !(vma->vm_flags & VM_SHARED)) {
        if (vma_needs_reservation(h, vma, vmf->address) < 0) {
            ret = VM_FAULT_OOM;
            goto backout_unlocked;
        }
        /* Just decrements count, does not deallocate */
        vma_end_reservation(h, vma, vmf->address);
    }

    vmf->ptl = huge_pte_lock(h, mm, vmf->pte);
    ret = 0;
    /* If pte changed from under us, retry */
    if (!pte_same(huge_ptep_get(mm, vmf->address, vmf->pte), vmf->orig_pte))
        goto backout;

    if (new_anon_folio)
        hugetlb_add_new_anon_rmap(folio, vma, vmf->address);
    else
        hugetlb_add_file_rmap(folio);

    new_pte = make_huge_pte(vma, folio, vma->vm_flags & VM_SHARED);
    /* If this pte was previously wr-protected, keep it wr-protected even
    * if populated. */
    if (unlikely(pte_marker_uffd_wp(vmf->orig_pte)))
        new_pte = huge_pte_mkuffd_wp(new_pte);

    set_huge_pte_at(mm, vmf->address, vmf->pte, new_pte, huge_page_size(h));

    hugetlb_count_add(pages_per_huge_page(h), mm) {
        atomic_long_add(l, &mm->hugetlb_usage);
    }

    if ((vmf->flags & FAULT_FLAG_WRITE) && !(vma->vm_flags & VM_SHARED)) {
        /* No need to keep file folios locked. See comment in
        * hugetlb_fault(). */
        if (!new_anon_folio) {
            folio_locked = false;
            folio_unlock(folio);
        }
        /* Optimization, do the COW without a second fault */
        ret = hugetlb_wp(vmf);
    }

    spin_unlock(vmf->ptl);

    /* Only set hugetlb_migratable in newly allocated pages.  Existing pages
    * found in the pagecache may not have hugetlb_migratable if they have
    * been isolated for migration. */
    if (new_folio)
        folio_set_hugetlb_migratable(folio);

    if (folio_locked)
        folio_unlock(folio);
out:
    hugetlb_vma_unlock_read(vma);

    /* We must check to release the per-VMA lock. __vmf_anon_prepare() is
    * the only way ret can be set to VM_FAULT_RETRY. */
    if (unlikely(ret & VM_FAULT_RETRY))
        vma_end_read(vma);

    mutex_unlock(&hugetlb_fault_mutex_table[hash]);
    return ret;

backout:
    spin_unlock(vmf->ptl);
backout_unlocked:
    /* We only need to restore reservations for private mappings */
    if (new_anon_folio)
        restore_reserve_on_error(h, vma, vmf->address, folio);

    folio_unlock(folio);
    folio_put(folio);
    goto out;
}
```

### hugetlb_wp

```c
static vm_fault_t hugetlb_wp(struct vm_fault *vmf)
{
    struct vm_area_struct *vma = vmf->vma;
    struct mm_struct *mm = vma->vm_mm;
    const bool unshare = vmf->flags & FAULT_FLAG_UNSHARE;
    pte_t pte = huge_ptep_get(mm, vmf->address, vmf->pte);
    struct hstate *h = hstate_vma(vma);
    struct folio *old_folio;
    struct folio *new_folio;
    bool cow_from_owner = 0;
    vm_fault_t ret = 0;
    struct mmu_notifier_range range;

    if (!unshare && huge_pte_uffd_wp(pte))
        return 0;

    /* Let's take out MAP_SHARED mappings first. */
    if (vma->vm_flags & VM_MAYSHARE) {
        set_huge_ptep_writable(vma, vmf->address, vmf->pte);
        return 0;
    }

    old_folio = page_folio(pte_page(pte));

    delayacct_wpcopy_start();

retry_avoidcopy:

    if (folio_mapcount(old_folio) == 1 && folio_test_anon(old_folio)) {
        if (!PageAnonExclusive(&old_folio->page)) {
            folio_move_anon_rmap(old_folio, vma) {
                /* move a folio to our anon_vma */
                void *anon_vma = vma->anon_vma;
                anon_vma += FOLIO_MAPPING_ANON;
                WRITE_ONCE(folio->mapping, anon_vma);
            }
            SetPageAnonExclusive(&old_folio->page);
        }
        if (likely(!unshare))
            set_huge_ptep_maybe_writable(vma, vmf->address, vmf->pte);

        delayacct_wpcopy_end();
        return 0;
    }

    if (is_vma_resv_set(vma, HPAGE_RESV_OWNER) && folio_test_anon(old_folio))
        cow_from_owner = true;

    folio_get(old_folio);


    spin_unlock(vmf->ptl);
    new_folio = alloc_hugetlb_folio(vma, vmf->address, cow_from_owner);

    if (IS_ERR(new_folio)) {
        /* If a process owning a MAP_PRIVATE mapping fails to COW,
         * it is due to references held by a child and an insufficient
         * huge page pool. To guarantee the original mappers
         * reliability, unmap the page from child processes. The child
         * may get SIGKILLed if it later faults. */
        if (cow_from_owner) {
            struct address_space *mapping = vma->vm_file->f_mapping;
            pgoff_t idx;
            u32 hash;

            folio_put(old_folio);
            /* Drop hugetlb_fault_mutex and vma_lock before
             * unmapping.  unmapping needs to hold vma_lock
             * in write mode.  Dropping vma_lock in read mode
             * here is OK as COW mappings do not interact with
             * PMD sharing.
             *
             * Reacquire both after unmap operation. */
            idx = vma_hugecache_offset(h, vma, vmf->address);
            hash = hugetlb_fault_mutex_hash(mapping, idx);
            hugetlb_vma_unlock_read(vma);
            mutex_unlock(&hugetlb_fault_mutex_table[hash]);

            unmap_ref_private(mm, vma, old_folio, vmf->address) {
                struct hstate *h = hstate_vma(vma);
                struct vm_area_struct *iter_vma;
                struct address_space *mapping;
                pgoff_t pgoff;

                address = address & huge_page_mask(h);
                pgoff = ((address - vma->vm_start) >> PAGE_SHIFT) + vma->vm_pgoff;
                mapping = vma->vm_file->f_mapping;

                i_mmap_lock_write(mapping);
                vma_interval_tree_foreach(iter_vma, &mapping->i_mmap, pgoff, pgoff) {
                    /* Do not unmap the current VMA */
                    if (iter_vma == vma)
                        continue;

                    /* Shared VMAs have their own reserves and do not affect
                    * MAP_PRIVATE accounting but it is possible that a shared
                    * VMA is using the same page so check and skip such VMAs. */
                    if (iter_vma->vm_flags & VM_MAYSHARE)
                        continue;

                    /* Unmap the page from other VMAs without their own reserves. */
                    if (!is_vma_resv_set(iter_vma, HPAGE_RESV_OWNER))
                        unmap_hugepage_range(iter_vma, address,
                            address + huge_page_size(h), folio, 0);
                }
                i_mmap_unlock_write(mapping);
            }
            mutex_lock(&hugetlb_fault_mutex_table[hash]);
            hugetlb_vma_lock_read(vma);
            spin_lock(vmf->ptl);
            vmf->pte = hugetlb_walk(vma, vmf->address, huge_page_size(h));
            if (likely(vmf->pte &&
                   pte_same(huge_ptep_get(mm, vmf->address, vmf->pte), pte)))
                goto retry_avoidcopy;
            /* race occurs while re-acquiring page table
             * lock, and our job is done. */
            delayacct_wpcopy_end();
            return 0;
        }

        ret = vmf_error(PTR_ERR(new_folio));
        goto out_release_old;
    }

    /* When the original hugepage is shared one, it does not have
     * anon_vma prepared. */
    ret = __vmf_anon_prepare(vmf) {
        struct vm_area_struct *vma = vmf->vma;
        vm_fault_t ret = 0;

        if (likely(vma->anon_vma))
            return 0;
        if (vmf->flags & FAULT_FLAG_VMA_LOCK) {
            if (!mmap_read_trylock(vma->vm_mm))
                return VM_FAULT_RETRY;
        }
        /* attach an anon_vma to a memory region */
        if (__anon_vma_prepare(vma))
            ret = VM_FAULT_OOM;
        if (vmf->flags & FAULT_FLAG_VMA_LOCK)
            mmap_read_unlock(vma->vm_mm);
        return ret;
    }
    if (unlikely(ret))
        goto out_release_all;

    if (copy_user_large_folio(new_folio, old_folio, vmf->real_address, vma)) {
        ret = VM_FAULT_HWPOISON_LARGE | VM_FAULT_SET_HINDEX(hstate_index(h));
        goto out_release_all;
    }
    __folio_mark_uptodate(new_folio);

    mmu_notifier_range_init(&range, MMU_NOTIFY_CLEAR, 0, mm, vmf->address,
                vmf->address + huge_page_size(h));
    mmu_notifier_invalidate_range_start(&range);

    /* Retake the page table lock to check for racing updates
     * before the page tables are altered */
    spin_lock(vmf->ptl);
    vmf->pte = hugetlb_walk(vma, vmf->address, huge_page_size(h));
    if (likely(vmf->pte && pte_same(huge_ptep_get(mm, vmf->address, vmf->pte), pte))) {
        pte_t newpte = make_huge_pte(vma, new_folio, !unshare);

        /* Break COW or unshare */
        huge_ptep_clear_flush(vma, vmf->address, vmf->pte);
        hugetlb_remove_rmap(old_folio);
        hugetlb_add_new_anon_rmap(new_folio, vma, vmf->address);
        if (huge_pte_uffd_wp(pte))
            newpte = huge_pte_mkuffd_wp(newpte);
        set_huge_pte_at(mm, vmf->address, vmf->pte, newpte, huge_page_size(h));
        folio_set_hugetlb_migratable(new_folio);
        /* Make the old page be freed below */
        new_folio = old_folio;
    }
    spin_unlock(vmf->ptl);
    mmu_notifier_invalidate_range_end(&range);

out_release_all:
    /* No restore in case of successful pagetable update (Break COW or
     * unshare) */
    if (new_folio != old_folio)
        restore_reserve_on_error(h, vma, vmf->address, new_folio);
    folio_put(new_folio);

out_release_old:
    folio_put(old_folio);

    spin_lock(vmf->ptl); /* Caller expects lock to be held */

    delayacct_wpcopy_end();
    return ret;
}
```

### thp_vma_allowable_order

```c
#define thp_vma_allowable_order(vma, vm_flags, tva_flags, order) \
    (!!thp_vma_allowable_orders(vma, vm_flags, tva_flags, BIT(order)))

unsigned long thp_vma_allowable_orders(struct vm_area_struct *vma,
    vm_flags_t vm_flags, unsigned long tva_flags, unsigned long orders)
{
    return thp_vma_allowable_orders(vma, vm_flags, tva_flags, BIT(order)) {
        /* Optimization to check if required orders are enabled early. */
        if ((tva_flags & TVA_ENFORCE_SYSFS) && vma_is_anonymous(vma)) {
            unsigned long mask = READ_ONCE(huge_anon_orders_always);

            if (vm_flags & VM_HUGEPAGE)
                mask |= READ_ONCE(huge_anon_orders_madvise);
            if (hugepage_global_always() ||
                ((vm_flags & VM_HUGEPAGE) && hugepage_global_enabled()))
                mask |= READ_ONCE(huge_anon_orders_inherit);

            orders &= mask;
            if (!orders)
                return 0;
        }

        return __thp_vma_allowable_orders(vma, vm_flags, tva_flags, orders) {
            bool smaps = tva_flags & TVA_SMAPS;
            bool in_pf = tva_flags & TVA_IN_PF;
            bool enforce_sysfs = tva_flags & TVA_ENFORCE_SYSFS;
            unsigned long supported_orders;

            /* Check the intersection of requested and supported orders. */
            if (vma_is_anonymous(vma))
                supported_orders = THP_ORDERS_ALL_ANON;
            else if (vma_is_special_huge(vma))
                supported_orders = THP_ORDERS_ALL_SPECIAL;
            else
                supported_orders = THP_ORDERS_ALL_FILE_DEFAULT;

            orders &= supported_orders;
            if (!orders)
                return 0;

            if (!vma->vm_mm)        /* vdso */
                return 0;

            if (thp_disabled_by_hw() || vma_thp_disabled(vma, vm_flags))
                return 0;

            /* khugepaged doesn't collapse DAX vma, but page fault is fine. */
            if (vma_is_dax(vma))
                return in_pf ? orders : 0;

            /* khugepaged special VMA and hugetlb VMA.
            * Must be checked after dax since some dax mappings may have
            * VM_MIXEDMAP set. */
            if (!in_pf && !smaps && (vm_flags & VM_NO_KHUGEPAGED))
                return 0;

            /* Check alignment for file vma and size for both file and anon vma by
            * filtering out the unsuitable orders.
            *
            * Skip the check for page fault. Huge fault does the check in fault
            * handlers. */
            if (!in_pf) {
                int order = highest_order(orders) {
                    return fls_long(orders) - 1;
                }
                unsigned long addr;

                while (orders) {
                    addr = vma->vm_end - (PAGE_SIZE << order);
                    ret = thp_vma_suitable_order(vma, addr, order) {
                        unsigned long hpage_size = PAGE_SIZE << order;
                        unsigned long haddr;

                        /* Don't have to check pgoff for anonymous vma */
                        if (!vma_is_anonymous(vma)) {
                            if (!IS_ALIGNED((vma->vm_start >> PAGE_SHIFT) - vma->vm_pgoff,
                                    hpage_size >> PAGE_SHIFT))
                                return false;
                        }

                        haddr = ALIGN_DOWN(addr, hpage_size);

                        if (haddr < vma->vm_start || haddr + hpage_size > vma->vm_end)
                            return false;
                        return true;
                    }
                    if (ret)
                        break;

                    order = next_order(&orders, order) {
                        *orders &= ~BIT(prev);
                        return highest_order(*orders);
                    }
                }

                if (!orders)
                    return 0;
            }

            /* Enabled via shmem mount options or sysfs settings.
            * Must be done before hugepage flags check since shmem has its
            * own flags. */
            if (!in_pf && shmem_file(vma->vm_file))
                return orders & shmem_allowable_huge_orders(file_inode(vma->vm_file),
                    vma, vma->vm_pgoff, 0, !enforce_sysfs) {

                    unsigned long mask = READ_ONCE(huge_shmem_orders_always);
                    unsigned long within_size_orders = READ_ONCE(huge_shmem_orders_within_size);
                    vm_flags_t vm_flags = vma ? vma->vm_flags : 0;
                    unsigned int global_orders;

                    if (thp_disabled_by_hw() || (vma && vma_thp_disabled(vma, vm_flags)))
                        return 0;

                    global_orders = shmem_huge_global_enabled(inode, index, write_end, shmem_huge_force, vma, vm_flags) {
                        unsigned int maybe_pmd_order = HPAGE_PMD_ORDER > MAX_PAGECACHE_ORDER
                            ? 0 : BIT(HPAGE_PMD_ORDER);
                        unsigned long within_size_orders;

                        if (!S_ISREG(inode->i_mode))
                            return 0;
                        if (shmem_huge == SHMEM_HUGE_DENY)
                            return 0;
                        if (shmem_huge_force || shmem_huge == SHMEM_HUGE_FORCE)
                            return maybe_pmd_order;

                        /* The huge order allocation for anon shmem is controlled through
                        * the mTHP interface, so we still use PMD-sized huge order to
                        * check whether global control is enabled.
                        *
                        * For tmpfs mmap()'s huge order, we still use PMD-sized order to
                        * allocate huge pages due to lack of a write size hint.
                        *
                        * Otherwise, tmpfs will allow getting a highest order hint based on
                        * the size of write and fallocate paths, then will try each allowable
                        * huge orders. */
                        switch (SHMEM_SB(inode->i_sb)->huge) {
                        case SHMEM_HUGE_ALWAYS:
                            if (vma)
                                return maybe_pmd_order;

                            return shmem_mapping_size_orders(inode->i_mapping, index, write_end);
                        case SHMEM_HUGE_WITHIN_SIZE:
                            if (vma)
                                within_size_orders = maybe_pmd_order;
                            else {
                                within_size_orders = shmem_mapping_size_orders(inode->i_mapping, index, write_end) {
                                    unsigned int order;
                                    size_t size;

                                    if (!mapping_large_folio_support(mapping) || !write_end)
                                        return 0;

                                    /* Calculate the write size based on the write_end */
                                    size = write_end - (index << PAGE_SHIFT);
                                    order = filemap_get_order(size);
                                    if (!order)
                                        return 0;

                                    /* If we're not aligned, allocate a smaller folio */
                                    if (index & ((1UL << order) - 1))
                                        order = __ffs(index);

                                    order = min_t(size_t, order, MAX_PAGECACHE_ORDER);
                                    return order > 0 ? BIT(order + 1) - 1 : 0;
                                }
                            }

                            within_size_orders = shmem_get_orders_within_size(inode, within_size_orders, index, write_end) {
                                pgoff_t aligned_index;
                                unsigned long order;
                                loff_t i_size;

                                order = highest_order(within_size_orders);
                                while (within_size_orders) {
                                    aligned_index = round_up(index + 1, 1 << order);
                                    i_size = max(write_end, i_size_read(inode));
                                    i_size = round_up(i_size, PAGE_SIZE);
                                    if (i_size >> PAGE_SHIFT >= aligned_index)
                                        return within_size_orders;

                                    order = next_order(&within_size_orders, order);
                                }

                                return 0;
                            }
                            if (within_size_orders > 0)
                                return within_size_orders;

                            fallthrough;
                        case SHMEM_HUGE_ADVISE:
                            if (vm_flags & VM_HUGEPAGE)
                                return maybe_pmd_order;
                            fallthrough;
                        default:
                            return 0;
                        }
                    }
                    /* Tmpfs huge pages allocation */
                    if (!vma || !vma_is_anon_shmem(vma))
                        return global_orders;

                    /* Following the 'deny' semantics of the top level, force the huge
                    * option off from all mounts. */
                    if (shmem_huge == SHMEM_HUGE_DENY)
                        return 0;

                    /* Only allow inherit orders if the top-level value is 'force', which
                    * means non-PMD sized THP can not override 'huge' mount option now. */
                    if (shmem_huge == SHMEM_HUGE_FORCE)
                        return READ_ONCE(huge_shmem_orders_inherit);

                    /* Allow mTHP that will be fully within i_size. */
                    mask |= shmem_get_orders_within_size(inode, within_size_orders, index, 0);

                    if (vm_flags & VM_HUGEPAGE)
                        mask |= READ_ONCE(huge_shmem_orders_madvise);

                    if (global_orders > 0)
                        mask |= READ_ONCE(huge_shmem_orders_inherit);

                    return THP_ORDERS_ALL_FILE_DEFAULT & mask;
                }

            if (!vma_is_anonymous(vma)) {
                /* Enforce sysfs THP requirements as necessary. Anonymous vmas
                * were already handled in thp_vma_allowable_orders(). */
                if (enforce_sysfs &&
                    (!hugepage_global_enabled() || (!(vm_flags & VM_HUGEPAGE) && !hugepage_global_always())))
                    return 0;

                /* Trust that ->huge_fault() handlers know what they are doing
                * in fault path. */
                if (((in_pf || smaps)) && vma->vm_ops->huge_fault)
                    return orders;
                /* Only regular file is valid in collapse path */
                if (((!in_pf || smaps)) && file_thp_enabled(vma))
                    return orders;
                return 0;
            }

            if (vma_is_temporary_stack(vma))
                return 0;

            /* THPeligible bit of smaps should show 1 for proper VMAs even
            * though anon_vma is not initialized yet.
            *
            * Allow page fault since anon_vma may be not initialized until
            * the first page fault. */
            if (!vma->anon_vma)
                return (smaps || in_pf) ? orders : 0;

            return orders;
        }
    }
}
```

## hugetlbfs_file_operations

### hugetlbfs_file_mmap

```c
int hugetlbfs_file_mmap(struct file *file, struct vm_area_struct *vma)
{
    struct inode *inode = file_inode(file);
    loff_t len, vma_len;
    int ret;
    struct hstate *h = hstate_file(file);
    vm_flags_t vm_flags;

    /* vma address alignment (but not the pgoff alignment) has
     * already been checked by prepare_hugepage_range. */
    vm_flags_set(vma, VM_HUGETLB | VM_DONTEXPAND);
    vma->vm_ops = &hugetlb_vm_ops;

    /* page based offset in vm_pgoff could be sufficiently large to
     * overflow a loff_t when converted to byte offset.  This can
     * only happen on architectures where sizeof(loff_t) ==
     * sizeof(unsigned long).  So, only check in those instances. */
    if (sizeof(unsigned long) == sizeof(loff_t)) {
        if (vma->vm_pgoff & PGOFF_LOFFT_MAX)
            return -EINVAL;
    }

    /* must be huge page aligned */
    if (vma->vm_pgoff & (~huge_page_mask(h) >> PAGE_SHIFT))
        return -EINVAL;

    vma_len = (loff_t)(vma->vm_end - vma->vm_start);
    len = vma_len + ((loff_t)vma->vm_pgoff << PAGE_SHIFT);
    /* check for overflow */
    if (len < vma_len)
        return -EINVAL;

    inode_lock(inode);
    file_accessed(file);

    ret = -ENOMEM;

    vm_flags = vma->vm_flags;
    /* for SHM_HUGETLB, the pages are reserved in the shmget() call so skip
     * reserving here. Note: only for SHM hugetlbfs file, the inode
     * flag S_PRIVATE is set. */
    if (inode->i_flags & S_PRIVATE)
        vm_flags |= VM_NORESERVE;

    if (hugetlb_reserve_pages(inode, vma->vm_pgoff >> huge_page_order(h), len >> huge_page_shift(h), vma, vm_flags) < 0)
        goto out;

    ret = 0;
    if (vma->vm_flags & VM_WRITE && inode->i_size < len)
        i_size_write(inode, len);
out:
    inode_unlock(inode);

    return ret;
}
```

#### hugetlb_reserve_pages

Huge page reservations were added to prevent unexpected page allocation failures (OOM) at page fault time.

Reservations are consumed when huge pages associated with the reservations are allocated and instantiated in the corresponding mapping. The allocation is performed within the routine `alloc_hugetlb_folio()`

* For private mappings, the reservation map hangs off the VMA structure: `vma->vm_private_data`.
* For shared mappings, the reservation map hangs off the inode: `inode->i_mapping->private_data`.

One of the big differences between **PRIVATE** and **SHARED** mappings is the way **in which reservations are represented in the reservation map**:
* For shared mappings, **an entry** in the reservation map indicates a reservation exists or did exist for the corresponding page. As reservations are consumed, the reservation map is not modified.
* For private mappings, the **lack of an entry** in the reservation map indicates a reservation exists for the corresponding page. As reservations are consumed, entries are added to the reservation map. Therefore, the reservation map can also be used to determine which reservations have been consumed.

```c
struct resv_map {
    struct kref             refs;
    spinlock_t              lock;
    struct list_head        regions;
    long                    adds_in_progress;
    struct list_head        region_cache;
    long                    region_cache_count;
    struct rw_semaphore     rw_sema;
#ifdef CONFIG_CGROUP_HUGETLB
    struct page_counter         *reservation_counter;
    unsigned long               pages_per_hpage;
    struct cgroup_subsys_state  *css;
#endif
};

struct file_region {
    struct list_head            link;
    /* from, to: huge page indices into the mapping or file */
    long                        from;
    long                        to;
#ifdef CONFIG_CGROUP_HUGETLB
    struct page_counter         *reservation_counter;
    struct cgroup_subsys_state  *css;
#endif
};
```

```c
/* reserve’ huge pages at mmap() time to ensure that huge pages
 * would be available for page faults in that mapping. */
long hugetlb_reserve_pages(struct inode *inode,
        long from, long to,
        struct vm_area_struct *vma,
        vm_flags_t vm_flags)
{
    long chg = -1, add = -1, spool_resv, gbl_resv;
    struct hstate *h = hstate_inode(inode);
    struct hugepage_subpool *spool = subpool_inode(inode);
    struct resv_map *resv_map;
    struct hugetlb_cgroup *h_cg = NULL;
    long gbl_reserve, regions_needed = 0;

    /* This should never happen */
    if (from > to) {
        VM_WARN(1, "%s called with a negative range\n", __func__);
        return -EINVAL;
    }

    hugetlb_vma_lock_alloc(vma);

    if (vm_flags & VM_NORESERVE)
        return 0;

/* 1. calc charge */
    if (!vma || vma->vm_flags & VM_MAYSHARE) {
        resv_map = inode_resv_map(inode) {
            return (struct resv_map *)(&inode->i_data)->i_private_data;
        }

        chg = region_chg(resv_map, from, to, &regions_needed);
    } else {
        /* Private mapping. */
        resv_map = resv_map_alloc();
        if (!resv_map)
            goto out_err;

        chg = to - from;

        set_vma_resv_map(vma, resv_map) {
            set_vma_private_data(vma, (unsigned long)map);
        }
        set_vma_resv_flags(vma, HPAGE_RESV_OWNER) {
            set_vma_private_data(vma, get_vma_private_data(vma) | flags) {
                vma->vm_private_data = (void *)value;
            }
        }
    }

    if (chg < 0)
        goto out_err;

    if (hugetlb_cgroup_charge_cgroup_rsvd(hstate_index(h), chg * pages_per_huge_page(h), &h_cg) < 0)
        goto out_err;

    if (vma && !(vma->vm_flags & VM_MAYSHARE) && h_cg) {
        /* For private mappings, the hugetlb_cgroup uncharge info hangs
        * of the resv_map. */
        resv_map_set_hugetlb_cgroup_uncharge_info(resv_map, h_cg, h);
    }

/* 2. consult subpool */
    gbl_reserve = hugepage_subpool_get_pages(spool, chg) {
        long ret = delta;

        if (!spool)
            return ret;

        spin_lock_irq(&spool->lock);

        if (spool->max_hpages != -1) { /* maximum size accounting */
            if ((spool->used_hpages + delta) <= spool->max_hpages)
                spool->used_hpages += delta;
            else {
                ret = -ENOMEM;
                goto unlock_ret;
            }
        }

        /* minimum size accounting */
        if (spool->min_hpages != -1 && spool->rsv_hpages) {
            if (delta > spool->rsv_hpages) {
                ret = delta - spool->rsv_hpages;
                spool->rsv_hpages = 0;
            } else {
                ret = 0;    /* reserves already accounted for */
                spool->rsv_hpages -= delta;
            }
        }

    unlock_ret:
        spin_unlock_irq(&spool->lock);
        return ret;
    }
    if (gbl_reserve < 0)
        goto out_uncharge_cgroup;

/* 3. alloc enough huge page */
    /* Check enough hugepages are available for the reservation.
    * Hand the pages back to the subpool if there are not */
    if (hugetlb_acct_memory(h, gbl_reserve) < 0)
        goto out_put_pages;

    if (!vma || vma->vm_flags & VM_MAYSHARE) {
        add = region_add(resv_map, from, to, regions_needed, h, h_cg);

        if (unlikely(add < 0)) {
            hugetlb_acct_memory(h, -gbl_reserve);
            goto out_put_pages;
        } else if (unlikely(chg > add)) {
            /* pages in this range were added to the reserve
            * map between region_chg and region_add.  This
            * indicates a race with alloc_hugetlb_folio.  Adjust
            * the subpool and reserve counts modified above
            * based on the difference. */
            long rsv_adjust;

            /* hugetlb_cgroup_uncharge_cgroup_rsvd() will put the
            * reference to h_cg->css. See comment below for detail. */
            hugetlb_cgroup_uncharge_cgroup_rsvd(
                hstate_index(h),
                (chg - add) * pages_per_huge_page(h), h_cg);

            rsv_adjust = hugepage_subpool_put_pages(spool, chg - add);
            hugetlb_acct_memory(h, -rsv_adjust);
        } else if (h_cg) {
            /* The file_regions will hold their own reference to
            * h_cg->css. So we should release the reference held
            * via hugetlb_cgroup_charge_cgroup_rsvd() when we are
            * done. */
            hugetlb_cgroup_put_rsvd_cgroup(h_cg);
        }
    }
    return chg;

out_put_pages:
    spool_resv = chg - gbl_reserve;
    if (spool_resv) {
        /* put sub pool's reservation back, chg - gbl_reserve */
        gbl_resv = hugepage_subpool_put_pages(spool, spool_resv);
        /* subpool's reserved pages can not be put back due to race,
        * return to hstate. */
        hugetlb_acct_memory(h, -gbl_resv);
    }
out_uncharge_cgroup:
    hugetlb_cgroup_uncharge_cgroup_rsvd(hstate_index(h), chg * pages_per_huge_page(h), h_cg);
out_err:
    hugetlb_vma_lock_free(vma);
    if (!vma || vma->vm_flags & VM_MAYSHARE)
        /* Only call region_abort if the region_chg succeeded but the
        * region_add failed or didn't run. */
        if (chg >= 0 && add < 0)
            region_abort(resv_map, from, to, regions_needed);
    if (vma && is_vma_resv_set(vma, HPAGE_RESV_OWNER)) {
        kref_put(&resv_map->refs, resv_map_release);
        set_vma_resv_map(vma, NULL);
    }

    return chg < 0 ? chg : add < 0 ? add : -EINVAL;
}
```

#### region_chg

```c
/* Examine the existing reserve map and determine how many
 * huge pages in the specified range [f, t) are NOT currently
 * represented.
 *
 * Returns the number of huge pages that need to be added to the existing
 * reservation map for the range [f, t). */
static long region_chg(struct resv_map *resv, long f, long t,
               long *out_regions_needed) {
    long chg = 0;

    spin_lock(&resv->lock);

    /* Count how many hugepages in this range are NOT represented.
     * out_regions_needed == NULL do add
     * out_regions_needed != NULL just count */
    chg = add_reservation_in_range(resv, f, t, NULL, NULL, out_regions_needed) {
        long add = 0;
        struct list_head *head = &resv->regions;
        long last_accounted_offset = f;
        struct file_region *iter, *trg = NULL;
        struct list_head *rg = NULL;

        if (regions_needed)
            *regions_needed = 0;

        /* In this loop, we essentially handle an entry for the range
        * [last_accounted_offset, iter->from), at every iteration, with some
        * bounds checking. */
        list_for_each_entry_safe(iter, trg, head, link) {
            /* Skip irrelevant regions that start before our range. */
            if (iter->from < f) {
                /* If this region ends after the last accounted offset,
                * then we need to update last_accounted_offset. */
                if (iter->to > last_accounted_offset)
                    last_accounted_offset = iter->to;
                continue;
            }

            /* When we find a region that starts beyond our range, we've
            * finished. */
            if (iter->from >= t) {
                rg = iter->link.prev;
                break;
            }

            /* Add an entry for last_accounted_offset -> iter->from, and
            * update last_accounted_offset. */
            if (iter->from > last_accounted_offset)
                add += hugetlb_resv_map_add(resv, iter->link.prev,
                    last_accounted_offset,
                    iter->from, h, h_cg, regions_needed);
            last_accounted_offset = iter->to;
        }

        /* Handle the case where our range extends beyond
        * last_accounted_offset. */
        if (!rg)
            rg = head->prev;
        if (last_accounted_offset < t) {
            add += hugetlb_resv_map_add(
                resv, rg, last_accounted_offset, t, h, h_cg, regions_needed) {

                struct file_region *nrg;

                if (!regions_needed) {
                    nrg = get_file_region_entry_from_cache(map, from, to);
                    record_hugetlb_cgroup_uncharge_info(cg, h, map, nrg);
                    list_add(&nrg->link, rg);
                    coalesce_file_region(map, nrg);
                } else
                    *regions_needed += 1;

                return to - from;
            }
        }

        return add;
    }

    if (*out_regions_needed == 0)
        *out_regions_needed = 1;

    ret = allocate_file_region_entries(resv, *out_regions_needed) {
        LIST_HEAD(allocated_regions);
        int to_allocate = 0, i = 0;
        struct file_region *trg = NULL, *rg = NULL;

        cnt =  (resv->adds_in_progress + regions_needed);
        while (resv->region_cache_count < cnt) {
            to_allocate = resv->adds_in_progress + regions_needed
                - resv->region_cache_count;

            spin_unlock(&resv->lock);
            for (i = 0; i < to_allocate; i++) {
                trg = kmalloc(sizeof(*trg), GFP_KERNEL);
                if (!trg)
                    goto out_of_memory;
                list_add(&trg->link, &allocated_regions);
            }

            spin_lock(&resv->lock);

            list_splice(&allocated_regions, &resv->region_cache);
            resv->region_cache_count += to_allocate;
        }

        return 0;
    }
    if (ret)
        return -ENOMEM;

    resv->adds_in_progress += *out_regions_needed;

    spin_unlock(&resv->lock);
    return chg;
}

hugetlb_resv_map_add(struct resv_map *map, struct list_head *rg, long from,
        long to, struct hstate *h, struct hugetlb_cgroup *cg,
        long *regions_needed) {

    struct file_region *nrg;

    if (!regions_needed) {
        nrg = get_file_region_entry_from_cache(map, from, to) {
            struct file_region *nrg;

            VM_BUG_ON(resv->region_cache_count <= 0);

            resv->region_cache_count--;
            nrg = list_first_entry(&resv->region_cache, struct file_region, link);
            list_del(&nrg->link);

            nrg->from = from;
            nrg->to = to;

            return nrg;
        }

        record_hugetlb_cgroup_uncharge_info(cg, h, map, nrg) {
        #ifdef CONFIG_CGROUP_HUGETLB
            if (h_cg) {
                nrg->reservation_counter =
                    &h_cg->rsvd_hugepage[hstate_index(h)];
                nrg->css = &h_cg->css;

                css_get(&h_cg->css);
                if (!resv->pages_per_hpage)
                    resv->pages_per_hpage = pages_per_huge_page(h);
            } else {
                nrg->reservation_counter = NULL;
                nrg->css = NULL;
            }
        #endif
        }

        list_add(&nrg->link, rg);
        coalesce_file_region(map, nrg) {
            struct file_region *nrg, *prg;

            prg = list_prev_entry(rg, link);
            if (&prg->link != &resv->regions && prg->to == rg->from &&
                has_same_uncharge_info(prg, rg)) {
                prg->to = rg->to;

                list_del(&rg->link);
                put_uncharge_info(rg);
                kfree(rg);

                rg = prg;
            }

            nrg = list_next_entry(rg, link);
            if (&nrg->link != &resv->regions && nrg->from == rg->to &&
                has_same_uncharge_info(nrg, rg)) {
                nrg->from = rg->from;

                list_del(&rg->link);
                put_uncharge_info(rg);
                kfree(rg);
            }
        }
    } else
        *regions_needed += 1;

    return to - from;
}
```

#### region_add

```c
static long region_add(struct resv_map *resv, long f, long t,
               long in_regions_needed, struct hstate *h,
               struct hugetlb_cgroup *h_cg)
{
    long add = 0, actual_regions_needed = 0;

    spin_lock(&resv->lock);
retry:

    /* Count how many regions are actually needed to execute this add. */
    add_reservation_in_range(resv, f, t, NULL, NULL, &actual_regions_needed);

    /* Check for sufficient descriptors in the cache to accommodate
     * this add operation. Note that actual_regions_needed may be greater
     * than in_regions_needed, as the resv_map may have been modified since
     * the region_chg call. In this case, we need to make sure that we
     * allocate extra entries, such that we have enough for all the
     * existing adds_in_progress, plus the excess needed for this
     * operation. */
    if (actual_regions_needed > in_regions_needed &&
        resv->region_cache_count <
            resv->adds_in_progress + (actual_regions_needed - in_regions_needed)) {
        /* region_add operation of range 1 should never need to
         * allocate file_region entries. */
        VM_BUG_ON(t - f <= 1);

        if (allocate_file_region_entries(
                resv, actual_regions_needed - in_regions_needed)) {
            return -ENOMEM;
        }

        goto retry;
    }

    /* pass NULL to do actual add operation */
    add = add_reservation_in_range(resv, f, t, h_cg, h, NULL);

    resv->adds_in_progress -= in_regions_needed;

    spin_unlock(&resv->lock);
    return add;
}
```

## hugetlb_vm_ops

```c
static void hugetlb_vm_op_open(struct vm_area_struct *vma)
{
    struct resv_map *resv = vma_resv_map(vma);

    /* HPAGE_RESV_OWNER indicates a private mapping.
     * This new VMA should share its siblings reservation map if present.
     * The VMA will only ever have a valid reservation map pointer where
     * it is being copied for another still existing VMA.  As that VMA
     * has a reference to the reservation map it cannot disappear until
     * after this open call completes.  It is therefore safe to take a
     * new reference here without additional locking. */
    if (resv && is_vma_resv_set(vma, HPAGE_RESV_OWNER)) {
        resv_map_dup_hugetlb_cgroup_uncharge_info(resv);
        kref_get(&resv->refs);
    }

    /* vma_lock structure for sharable mappings is vma specific.
     * Clear old pointer (if copied via vm_area_dup) and allocate
     * new structure.  Before clearing, make sure vma_lock is not
     * for this vma. */
    if (vma->vm_flags & VM_MAYSHARE) {
        struct hugetlb_vma_lock *vma_lock = vma->vm_private_data;

        if (vma_lock) {
            if (vma_lock->vma != vma) {
                vma->vm_private_data = NULL;
                hugetlb_vma_lock_alloc(vma);
            } else
                pr_warn("HugeTLB: vma_lock already exists in %s.\n", __func__);
        } else
            hugetlb_vma_lock_alloc(vma);
    }
}

static void hugetlb_vm_op_close(struct vm_area_struct *vma)
{
    struct hstate *h = hstate_vma(vma);
    struct resv_map *resv;
    struct hugepage_subpool *spool = subpool_vma(vma);
    unsigned long reserve, start, end;
    long gbl_reserve;

    hugetlb_vma_lock_free(vma);

    resv = vma_resv_map(vma);
    if (!resv || !is_vma_resv_set(vma, HPAGE_RESV_OWNER))
        return;

    start = vma_hugecache_offset(h, vma, vma->vm_start);
    end = vma_hugecache_offset(h, vma, vma->vm_end);

    reserve = (end - start) - region_count(resv, start, end);
    hugetlb_cgroup_uncharge_counter(resv, start, end);
    if (reserve) {
        /* Decrement reserve counts.  The global reserve count may be
         * adjusted if the subpool has a minimum size. */
        gbl_reserve = hugepage_subpool_put_pages(spool, reserve);
        hugetlb_acct_memory(h, -gbl_reserve);
    }

    kref_put(&resv->refs, resv_map_release);
}
```

## remove_inode_hugepages

```c
void remove_inode_hugepages(struct inode *inode, loff_t lstart,
                   loff_t lend)
{
    struct hstate *h = hstate_inode(inode);
    struct address_space *mapping = &inode->i_data;
    const pgoff_t end = lend >> PAGE_SHIFT;
    struct folio_batch fbatch;
    pgoff_t next, index;
    int i, freed = 0;
    bool truncate_op = (lend == LLONG_MAX);

    folio_batch_init(&fbatch);
    next = lstart >> PAGE_SHIFT;
    while (filemap_get_folios(mapping, &next, end - 1, &fbatch)) {
        for (i = 0; i < folio_batch_count(&fbatch); ++i) {
            struct folio *folio = fbatch.folios[i];
            u32 hash = 0;

            index = folio->index >> huge_page_order(h);
            hash = hugetlb_fault_mutex_hash(mapping, index);
            mutex_lock(&hugetlb_fault_mutex_table[hash]);

            /* Remove folio that was part of folio_batch. */
            if (remove_inode_single_folio(h, inode, mapping, folio, index, truncate_op))
                freed++;

            mutex_unlock(&hugetlb_fault_mutex_table[hash]);
        }
        folio_batch_release(&fbatch);
        cond_resched();
    }

    if (truncate_op)
        (void)hugetlb_unreserve_pages(inode, lstart >> huge_page_shift(h), LONG_MAX, freed);
}

bool remove_inode_single_folio(struct hstate *h, struct inode *inode,
                    struct address_space *mapping,
                    struct folio *folio, pgoff_t index,
                    bool truncate_op)
{
    bool ret = false;

    /* If folio is mapped, it was faulted in after being
     * unmapped in caller.  Unmap (again) while holding
     * the fault mutex.  The mutex will prevent faults
     * until we finish removing the folio. */
    if (unlikely(folio_mapped(folio)))
        hugetlb_unmap_file_folio(h, mapping, folio, index);

    folio_lock(folio);
    /* We must remove the folio from page cache before removing
     * the region/ reserve map (hugetlb_unreserve_pages).  In
     * rare out of memory conditions, removal of the region/reserve
     * map could fail.  Correspondingly, the subpool and global
     * reserve usage count can need to be adjusted. */
    VM_BUG_ON_FOLIO(folio_test_hugetlb_restore_reserve(folio), folio);
    hugetlb_delete_from_page_cache(folio);
    ret = true;
    if (!truncate_op) {
        if (unlikely(hugetlb_unreserve_pages(inode, index, index + 1, 1)))
            hugetlb_fix_reserve_counts(inode);
    }

    folio_unlock(folio);
    return ret;
}

void hugetlb_unmap_file_folio(struct hstate *h,
                    struct address_space *mapping,
                    struct folio *folio, pgoff_t index)
{
    struct rb_root_cached *root = &mapping->i_mmap;
    struct hugetlb_vma_lock *vma_lock;
    unsigned long pfn = folio_pfn(folio);
    struct vm_area_struct *vma;
    unsigned long v_start;
    unsigned long v_end;
    pgoff_t start, end;

    start = index * pages_per_huge_page(h);
    end = (index + 1) * pages_per_huge_page(h);

    i_mmap_lock_write(mapping);
retry:
    vma_lock = NULL;
    vma_interval_tree_foreach(vma, root, start, end - 1) {
        v_start = vma_offset_start(vma, start);
        v_end = vma_offset_end(vma, end);

        if (!hugetlb_vma_maps_pfn(vma, v_start, pfn))
            continue;

        if (!hugetlb_vma_trylock_write(vma)) {
            vma_lock = vma->vm_private_data;
            /* If we can not get vma lock, we need to drop
             * immap_sema and take locks in order.  First,
             * take a ref on the vma_lock structure so that
             * we can be guaranteed it will not go away when
             * dropping immap_sema. */
            kref_get(&vma_lock->refs);
            break;
        }

        unmap_hugepage_range(vma, v_start, v_end, NULL, ZAP_FLAG_DROP_MARKER);
        hugetlb_vma_unlock_write(vma);
    }

    i_mmap_unlock_write(mapping);

    if (vma_lock) {
        /* Wait on vma_lock.  We know it is still valid as we have
         * a reference.  We must 'open code' vma locking as we do
         * not know if vma_lock is still attached to vma. */
        down_write(&vma_lock->rw_sema);
        i_mmap_lock_write(mapping);

        vma = vma_lock->vma;
        if (!vma) {
            /* If lock is no longer attached to vma, then just
             * unlock, drop our reference and retry looking for
             * other vmas. */
            up_write(&vma_lock->rw_sema);
            kref_put(&vma_lock->refs, hugetlb_vma_lock_release);
            goto retry;
        }

        /* vma_lock is still attached to vma.  Check to see if vma
         * still maps page and if so, unmap. */
        v_start = vma_offset_start(vma, start);
        v_end = vma_offset_end(vma, end);
        if (hugetlb_vma_maps_pfn(vma, v_start, pfn))
            unmap_hugepage_range(vma, v_start, v_end, NULL, ZAP_FLAG_DROP_MARKER);

        kref_put(&vma_lock->refs, hugetlb_vma_lock_release);
        hugetlb_vma_unlock_write(vma);

        goto retry;
    }
}
```

### unmap_hugepage_range

```c
void unmap_hugepage_range(struct vm_area_struct *vma, unsigned long start,
              unsigned long end, struct folio *folio,
              zap_flags_t zap_flags)
{
    struct mmu_notifier_range range;
    struct mmu_gather tlb;

    mmu_notifier_range_init(&range, MMU_NOTIFY_CLEAR, 0, vma->vm_mm, start, end);
    adjust_range_if_pmd_sharing_possible(vma, &range.start, &range.end);
    mmu_notifier_invalidate_range_start(&range);
    tlb_gather_mmu(&tlb, vma->vm_mm);

    __unmap_hugepage_range(&tlb, vma, start, end, folio, zap_flags) {
        struct mm_struct *mm = vma->vm_mm;
        const bool folio_provided = !!folio;
        unsigned long address;
        pte_t *ptep;
        pte_t pte;
        spinlock_t *ptl;
        struct hstate *h = hstate_vma(vma);
        unsigned long sz = huge_page_size(h);
        bool adjust_reservation;
        unsigned long last_addr_mask;
        bool force_flush = false;

        WARN_ON(!is_vm_hugetlb_page(vma));
        BUG_ON(start & ~huge_page_mask(h));
        BUG_ON(end & ~huge_page_mask(h));

        /* This is a hugetlb vma, all the pte entries should point
        * to huge page. */
        tlb_change_page_size(tlb, sz);
        tlb_start_vma(tlb, vma);

        last_addr_mask = hugetlb_mask_last_page(h);
        address = start;
        for (; address < end; address += sz) {
            ptep = hugetlb_walk(vma, address, sz);
            if (!ptep) {
                address |= last_addr_mask;
                continue;
            }

            ptl = huge_pte_lock(h, mm, ptep);
            if (huge_pmd_unshare(mm, vma, address, ptep)) {
                spin_unlock(ptl);
                tlb_flush_pmd_range(tlb, address & PUD_MASK, PUD_SIZE);
                force_flush = true;
                address |= last_addr_mask;
                continue;
            }

            pte = huge_ptep_get(mm, address, ptep);
            if (huge_pte_none(pte)) {
                spin_unlock(ptl);
                continue;
            }

            /* Migrating hugepage or HWPoisoned hugepage is already
            * unmapped and its refcount is dropped, so just clear pte here. */
            if (unlikely(!pte_present(pte))) {
                /* If the pte was wr-protected by uffd-wp in any of the
                * swap forms, meanwhile the caller does not want to
                * drop the uffd-wp bit in this zap, then replace the
                * pte with a marker. */
                if (pte_swp_uffd_wp_any(pte) &&
                    !(zap_flags & ZAP_FLAG_DROP_MARKER))
                    set_huge_pte_at(mm, address, ptep,
                            make_pte_marker(PTE_MARKER_UFFD_WP),
                            sz);
                else
                    huge_pte_clear(mm, address, ptep, sz);
                spin_unlock(ptl);
                continue;
            }

            /* If a folio is supplied, it is because a specific
            * folio is being unmapped, not a range. Ensure the folio we
            * are about to unmap is the actual folio of interest. */
            if (folio_provided) {
                if (folio != page_folio(pte_page(pte))) {
                    spin_unlock(ptl);
                    continue;
                }
                /* Mark the VMA as having unmapped its page so that
                * future faults in this VMA will fail rather than
                * looking like data was lost */
                set_vma_resv_flags(vma, HPAGE_RESV_UNMAPPED);
            } else {
                folio = page_folio(pte_page(pte));
            }

            pte = huge_ptep_get_and_clear(mm, address, ptep, sz);
            tlb_remove_huge_tlb_entry(h, tlb, ptep, address);
            if (huge_pte_dirty(pte))
                folio_mark_dirty(folio);
            /* Leave a uffd-wp pte marker if needed */
            if (huge_pte_uffd_wp(pte) &&
                !(zap_flags & ZAP_FLAG_DROP_MARKER))
                set_huge_pte_at(mm, address, ptep,
                        make_pte_marker(PTE_MARKER_UFFD_WP),
                        sz);
            hugetlb_count_sub(pages_per_huge_page(h), mm);
            hugetlb_remove_rmap(folio);
            spin_unlock(ptl);

            /* Restore the reservation for anonymous page, otherwise the
            * backing page could be stolen by someone.
            * If there we are freeing a surplus, do not set the restore
            * reservation bit. */
            adjust_reservation = false;

            spin_lock_irq(&hugetlb_lock);
            if (!h->surplus_huge_pages && __vma_private_lock(vma) &&
                folio_test_anon(folio)) {
                folio_set_hugetlb_restore_reserve(folio);
                /* Reservation to be adjusted after the spin lock */
                adjust_reservation = true;
            }
            spin_unlock_irq(&hugetlb_lock);

            /* Adjust the reservation for the region that will have the
            * reserve restored. Keep in mind that vma_needs_reservation() changes
            * resv->adds_in_progress if it succeeds. If this is not done,
            * do_exit() will not see it, and will keep the reservation
            * forever. */
            if (adjust_reservation) {
                int rc = vma_needs_reservation(h, vma, address);

                if (rc < 0)
                    /* Pressumably allocate_file_region_entries failed
                    * to allocate a file_region struct. Clear
                    * hugetlb_restore_reserve so that global reserve
                    * count will not be incremented by free_huge_folio.
                    * Act as if we consumed the reservation. */
                    folio_clear_hugetlb_restore_reserve(folio);
                else if (rc)
                    vma_add_reservation(h, vma, address);
            }

            tlb_remove_page_size(tlb, folio_page(folio, 0),
                        folio_size(folio));
            /* If we were instructed to unmap a specific folio, we're done. */
            if (folio_provided)
                break;
        }
        tlb_end_vma(tlb, vma);

        /* If we unshared PMDs, the TLB flush was not recorded in mmu_gather. We
        * could defer the flush until now, since by holding i_mmap_rwsem we
        * guaranteed that the last refernece would not be dropped. But we must
        * do the flushing before we return, as otherwise i_mmap_rwsem will be
        * dropped and the last reference to the shared PMDs page might be
        * dropped as well.
        *
        * In theory we could defer the freeing of the PMD pages as well, but
        * huge_pmd_unshare() relies on the exact page_count for the PMD page to
        * detect sharing, so we cannot defer the release of the page either.
        * Instead, do flush now. */
        if (force_flush)
            tlb_flush_mmu_tlbonly(tlb);
    }

    mmu_notifier_invalidate_range_end(&range);
    tlb_finish_mmu(&tlb);
}
```


# hugetlb_cgroup

```c
struct hugetlb_cgroup {
    struct cgroup_subsys_state css;

    /* the counter to account for hugepages from hugetlb. */
    struct page_counter hugepage[HUGE_MAX_HSTATE];

    /* the counter to account for hugepage reservations from hugetlb. */
    struct page_counter rsvd_hugepage[HUGE_MAX_HSTATE];

    atomic_long_t events[HUGE_MAX_HSTATE][HUGETLB_NR_MEMORY_EVENTS];
    atomic_long_t events_local[HUGE_MAX_HSTATE][HUGETLB_NR_MEMORY_EVENTS];

    /* Handle for "hugetlb.events" */
    struct cgroup_file events_file[HUGE_MAX_HSTATE];

    /* Handle for "hugetlb.events.local" */
    struct cgroup_file events_local_file[HUGE_MAX_HSTATE];

    struct hugetlb_cgroup_per_node *nodeinfo[];
};

struct hugetlb_cgroup_per_node {
    /* hugetlb usage in pages over all hstates. */
    unsigned long usage[HUGE_MAX_HSTATE];
};
```

## hugetlb_cgroup_charge_cgroup_rsvd

```c
int hugetlb_cgroup_charge_cgroup_rsvd(int idx, unsigned long nr_pages,
                      struct hugetlb_cgroup **ptr)
{
    return __hugetlb_cgroup_charge_cgroup(idx, nr_pages, ptr, true);
}

static int __hugetlb_cgroup_charge_cgroup(int idx, unsigned long nr_pages,
                      struct hugetlb_cgroup **ptr,
                      bool rsvd)
{
    int ret = 0;
    struct page_counter *counter;
    struct hugetlb_cgroup *h_cg = NULL;

    if (hugetlb_cgroup_disabled())
        goto done;
again:
    rcu_read_lock();
    h_cg = hugetlb_cgroup_from_task(current);
    if (!css_tryget(&h_cg->css)) {
        rcu_read_unlock();
        goto again;
    }
    rcu_read_unlock();

    if (!page_counter_try_charge(
            __hugetlb_cgroup_counter_from_cgroup(h_cg, idx, rsvd),
            nr_pages, &counter)) {
        ret = -ENOMEM;
        hugetlb_event(h_cg, idx, HUGETLB_MAX);
        css_put(&h_cg->css);
        goto done;
    }
    /* Reservations take a reference to the css because they do not get
     * reparented. */
    if (!rsvd)
        css_put(&h_cg->css);
done:
    *ptr = h_cg;
    return ret;
}
```

# debug

## CONFIG_DEBUG_KMEMLEAK

```sh
CONFIG_HAVE_DEBUG_KMEMLEAK=y
CONFIG_DEBUG_KMEMLEAK=y
CONFIG_DEBUG_KMEMLEAK_EARLY_LOG_SIZE=16000
CONFIG_DEBUG_KMEMLEAK_DEFAULT_OFF=y

# show memleak details
mount -t debugfs debugfs /sys/kernel/debug
echo scan > /sys/kernel/debug/kmemleak
cat /sys/kernel/debug/kmemleak
echo clear > /sys/kernel/debug/kmemleak
```

## CONFIG_KASAN

KASAN detects invalid memory accesses and use-after-free bugs, which can lead to leaks.

```sh
CONFIG_KASAN=y

dmesg | grep kasan
```

# fs_proc

```sh
/proc/          # Virtual filesystem for system and process information
├── meminfo     # Detailed memory usage statistics (total, free, buffers, cache, swap, etc.)
├── zoneinfo    # Per-memory-zone statistics (e.g., DMA, Normal, HighMem zones on each NUMA node)
├── buddyinfo   # Buddy allocator statistics for diagnosing memory fragmentation (free blocks by order per zone)
├── slabinfo    # Slab cache usage details (kernel object pools like inodes, dentries, etc.)
├── vmallocinfo # Information on vmalloc areas (virtual memory allocations, sizes, and callers)
├── iomem       # Physical memory map and I/O memory regions (addresses reserved for devices)
├── pagetypeinfo # Page type and migration info (free/used pages by type and migrate order for fragmentation analysis)
├── vmstat      # Virtual memory statistics (page ins/outs, interrupts, context switches, etc.)
├── swaps       # Swap space statistics (total, used, priority for each swap device)
├── kpagecount  # Page reference counts (number of times each physical page is mapped; indexed by PFN)
├── kpageflags  # Page flags (e.g., locked, referenced, dirty for each physical page; indexed by PFN)
└── <pid>/      # Process-specific directory (one per PID)
    ├── maps        # Virtual memory mappings (addresses, permissions, offsets for libraries, heap, stack)
    ├── smaps       # Detailed per-mapping memory stats (RSS, PSS, shared/clean/private for each map)
    ├── oom_adj     # Deprecated OOM adjustment score (legacy; use oom_score_adj instead)
    ├── oom_score   # OOM kill score (badness heuristic for process selection on out-of-memory)
    └── oom_score_adj # OOM adjustment value (-1000 to 1000; tunes oom_score for process priority)
```

```sh
/proc/sys/vm/                    # Virtual memory kernel parameters
├── admin_reserve_kbytes         # Memory reserved for admin tasks (kB)
├── compact_memory               # Trigger memory compaction (write-only: echo 1)
├── compact_unevictable_allowed  # whether unevictable pages (e.g., mlocked pages) can be compacted
├── dirty_background_bytes       # Bytes of dirty memory before background writeout
├── dirty_background_ratio       # % of memory for background writeout
├── dirty_bytes                  # Bytes of dirty memory before writeout
├── dirty_ratio                  # % of memory for dirty pages before writeout
├── dirty_expire_centisecs       # Time before dirty pages are written out (centisecs)
├── dirty_writeback_centisecs    # Interval for dirty page writeback (centisecs)
├── dirtytime_expire_seconds     # Maximum age (in seconds) for dirty pages on filesystems supporting dirtytime before writeback.
├── drop_caches                  # Drop pagecache, dentries, inodes (write-only: echo 1, 2, or 3)
├── extfrag_threshold            # Fragmentation threshold for memory compaction
├── hugepages_treat_as_movable   # Allow huge pages in movable zones (0 or 1)
├── hugetlb_shm_group            # Group ID allowed to use HugeTLB for shmem
├── laptop_mode                  # Optimize for laptop power saving (0-5)
├── legacy_va_layout             # Use legacy virtual address layout (0 or 1)
├── lowmem_reserve_ratio         # Memory reserve ratios for low memory zones
├── max_map_count                # Max number of memory map areas per process
├── memory_failure_early_kill    # Kill processes on memory failure (0 or 1)
├── memory_failure_recovery      # Enable memory failure recovery (0 or 1)
├── min_free_kbytes              # Minimum free memory for allocation (kB)
├── min_slab_ratio               # Min % of memory for slab caches
├── min_unmapped_ratio           # Min % of unmapped pages for compaction
├── mmap_min_addr                # Minimum address for mmap (security)
├── mmap_rnd_bits                # Bits for mmap address randomization
├── mmap_rnd_compat_bits         # Bits for compat mode address randomization
├── nr_hugepages                 # Number of HugeTLB pages (global)
├── nr_hugepages_mempolicy       # Number of HugeTLB pages for NUMA policy
├── nr_overcommit_hugepages      # Overcommit HugeTLB pages
├── numa_stat                    # NUMA memory statistics (read-only)
├── numa_zonelist_order          # NUMA zonelist order (e.g., "node", "zone")
├── oom_dump_tasks               # Dump tasks on OOM killer invocation (0 or 1)
├── oom_kill_allocating_task     # Kill allocating task on OOM (0 or 1)
├── overcommit_kbytes            # Fixed overcommit memory limit (kB)
├── overcommit_memory            # Memory overcommit policy (0, 1, or 2)
├── overcommit_ratio             # % of memory for overcommit calculation
├── page-cluster                 # Pages to read/write in one go for swapping
├── panic_on_oom                 # Panic on OOM (0, 1, or 2)
├── percpu_pagelist_fraction     # Fraction of pages for per-CPU lists
├── stat_interval                # Interval for updating VM statistics (seconds)
├── swappiness                   # Tendency to swap out memory (0-100)
├── user_reserve_kbytes          # Memory reserved for user tasks (kB)
├── vfs_cache_pressure           # Tendency to reclaim inode/dentry cache (0-100)
├── watermark_boost_factor       # Boost factor for memory watermarks
├── watermark_scale_factor       # Scale factor for memory watermarks
└── zone_reclaim_mode            # Reclaim mode for NUMA zones (0, 1, 2, or 3)
```

## /proc/buddyinfo

```c
proc_create_seq("buddyinfo", 0444, NULL, &fragmentation_op);

static const struct seq_operations fragmentation_op = {
    .start  = frag_start,
    .next   = frag_next,
    .stop   = frag_stop,
    .show   = frag_show,
};

static int frag_show(struct seq_file *m, void *arg)
{
    pg_data_t *pgdat = (pg_data_t *)arg;
    walk_zones_in_node(m, pgdat, true, false, frag_show_print) {
        struct zone *zone;
        struct zone *node_zones = pgdat->node_zones;
        unsigned long flags;

        for (zone = node_zones; zone - node_zones < MAX_NR_ZONES; ++zone) {
            if (assert_populated && !populated_zone(zone))
                continue;

            if (!nolock)
                spin_lock_irqsave(&zone->lock, flags);
            print(m, pgdat, zone);
            if (!nolock)
                spin_unlock_irqrestore(&zone->lock, flags);
        }
    }
    return 0;
}

static void frag_show_print(struct seq_file *m, pg_data_t *pgdat,
                        struct zone *zone)
{
    int order;

    seq_printf(m, "Node %d, zone %8s ", pgdat->node_id, zone->name);
    for (order = 0; order < NR_PAGE_ORDERS; ++order)
        /* Access to nr_free is lockless as nr_free is used only for
         * printing purposes. Use data_race to avoid KCSAN warning. */
        seq_printf(m, "%6lu ", data_race(zone->free_area[order].nr_free));
    seq_putc(m, '\n');
}

void zoneinfo_show_print(struct seq_file *m, pg_data_t *pgdat,
                            struct zone *zone)
{
    int i;
    seq_printf(m, "Node %d, zone %8s", pgdat->node_id, zone->name);
    if (is_zone_first_populated(pgdat, zone)) {
        seq_printf(m, "\n  per-node stats");
        for (i = 0; i < NR_VM_NODE_STAT_ITEMS; i++) {
            unsigned long pages = node_page_state_pages(pgdat, i);

            if (vmstat_item_print_in_thp(i))
                pages /= HPAGE_PMD_NR;
            seq_printf(m, "\n      %-12s %lu", node_stat_name(i),
                   pages);
        }
    }
    seq_printf(m,
           "\n  pages free     %lu"
           "\n        boost    %lu"
           "\n        min      %lu"
           "\n        low      %lu"
           "\n        high     %lu"
           "\n        promo    %lu"
           "\n        spanned  %lu"
           "\n        present  %lu"
           "\n        managed  %lu"
           "\n        cma      %lu",
           zone_page_state(zone, NR_FREE_PAGES),
           zone->watermark_boost,
           min_wmark_pages(zone),
           low_wmark_pages(zone),
           high_wmark_pages(zone),
           promo_wmark_pages(zone),
           zone->spanned_pages,
           zone->present_pages,
           zone_managed_pages(zone),
           zone_cma_pages(zone));

    seq_printf(m,
           "\n        protection: (%ld",
           zone->lowmem_reserve[0]);
    for (i = 1; i < ARRAY_SIZE(zone->lowmem_reserve); i++)
        seq_printf(m, ", %ld", zone->lowmem_reserve[i]);
    seq_putc(m, ')');

    /* If unpopulated, no other information is useful */
    if (!populated_zone(zone)) {
        seq_putc(m, '\n');
        return;
    }

    for (i = 0; i < NR_VM_ZONE_STAT_ITEMS; i++)
        seq_printf(m, "\n      %-12s %lu", zone_stat_name(i),
               zone_page_state(zone, i));

#ifdef CONFIG_NUMA
    fold_vm_zone_numa_events(zone);
    for (i = 0; i < NR_VM_NUMA_EVENT_ITEMS; i++)
        seq_printf(m, "\n      %-12s %lu", numa_stat_name(i),
               zone_numa_event_state(zone, i));
#endif

    seq_printf(m, "\n  pagesets");
    for_each_online_cpu(i) {
        struct per_cpu_pages *pcp;
        struct per_cpu_zonestat __maybe_unused *pzstats;

        pcp = per_cpu_ptr(zone->per_cpu_pageset, i);
        seq_printf(m,
               "\n    cpu: %i"
               "\n              count:    %i"
               "\n              high:     %i"
               "\n              batch:    %i"
               "\n              high_min: %i"
               "\n              high_max: %i",
               i,
               pcp->count,
               pcp->high,
               pcp->batch,
               pcp->high_min,
               pcp->high_max);
#ifdef CONFIG_SMP
        pzstats = per_cpu_ptr(zone->per_cpu_zonestats, i);
        seq_printf(m, "\n  vm stats threshold: %d",
                pzstats->stat_threshold);
#endif
    }
    seq_printf(m,
           "\n  node_unreclaimable:  %u"
           "\n  start_pfn:           %lu"
           "\n  reserved_highatomic: %lu"
           "\n  free_highatomic:     %lu",
           atomic_read(&pgdat->kswapd_failures) >= MAX_RECLAIM_RETRIES,
           zone->zone_start_pfn,
           zone->nr_reserved_highatomic,
           zone->nr_free_highatomic);
    seq_putc(m, '\n');
}
```

## /proc/meminfo

* NR_FILE_PAGES
    * Page cache for files
    * mmap’ed file pages
    * tmpfs/shmem file pages (some of these)
    * Includes swapcache
    * Includes pages that back block-device buffers

```c
static int __init proc_meminfo_init(void)
{
    struct proc_dir_entry *pde;

    pde = proc_create_single("meminfo", 0, NULL, meminfo_proc_show);
    pde_make_permanent(pde);
    return 0;
}
fs_initcall(proc_meminfo_init);

static int meminfo_proc_show(struct seq_file *m, void *v)
{
    struct sysinfo i;
    unsigned long committed;
    long cached;
    long available;
    unsigned long pages[NR_LRU_LISTS];
    unsigned long sreclaimable, sunreclaim;
    int lru;

    si_meminfo(&i) {
        val->totalram = totalram_pages() {
            return (unsigned long)atomic_long_read(&_totalram_pages);
        }
        val->sharedram = global_node_page_state(NR_SHMEM) {
            return global_node_page_state_pages(item) {
                long x = atomic_long_read(&vm_node_stat[item]);
                if (x < 0)
                    x = 0;
                return x;
            }
        }
        val->freeram = global_zone_page_state(NR_FREE_PAGES);
        val->bufferram = nr_blockdev_pages() {
            struct inode *inode;
            long ret = 0;

            spin_lock(&blockdev_superblock->s_inode_list_lock);
            list_for_each_entry(inode, &blockdev_superblock->s_inodes, i_sb_list)
                ret += inode->i_mapping->nrpages;
            spin_unlock(&blockdev_superblock->s_inode_list_lock);

            return ret;
        }
        val->totalhigh = totalhigh_pages() {
            return __totalhigh_pages() {
                unsigned long pages = 0;
                struct zone *zone;

                for_each_populated_zone(zone) {
                    if (is_highmem(zone))
                        pages += zone_managed_pages(zone);
                }

                return pages;
            }
        }
        val->freehigh = nr_free_highpages() {
            return __nr_free_highpages() {
                unsigned long pages = 0;
                struct zone *zone;

                for_each_populated_zone(zone) {
                    if (is_highmem(zone))
                        pages += zone_page_state(zone, NR_FREE_PAGES);
                }

                return pages;
            }
        }
        val->mem_unit = PAGE_SIZE;
    }

    si_swapinfo(&i) {
        unsigned int type;
        unsigned long nr_to_be_unused = 0;

        spin_lock(&swap_lock);
        for (type = 0; type < nr_swapfiles; type++) {
            struct swap_info_struct *si = swap_info[type];

            if ((si->flags & SWP_USED) && !(si->flags & SWP_WRITEOK)) {
                nr_to_be_unused += swap_usage_in_pages(si) {
                    return atomic_long_read(&si->inuse_pages) & SWAP_USAGE_COUNTER_MASK;
                }
            }
        }
        val->freeswap = atomic_long_read(&nr_swap_pages) + nr_to_be_unused;
        val->totalswap = total_swap_pages + nr_to_be_unused;
        spin_unlock(&swap_lock);
    }
    committed = vm_memory_committed() {
        return percpu_counter_sum_positive(&vm_committed_as) {
            s64 ret = __percpu_counter_sum(fbc) {
                s64 ret;
                int cpu;
                unsigned long flags;

                raw_spin_lock_irqsave(&fbc->lock, flags);
                ret = fbc->count;
                for_each_cpu_or(cpu, cpu_online_mask, cpu_dying_mask) {
                    s32 *pcount = per_cpu_ptr(fbc->counters, cpu);
                    ret += *pcount;
                }
                raw_spin_unlock_irqrestore(&fbc->lock, flags);
                return ret;
            }
	        return ret < 0 ? 0 : ret;
        }
    }

    cached = global_node_page_state(NR_FILE_PAGES) -
            total_swapcache_pages() {
                return global_node_page_state(NR_SWAPCACHE);
            } - i.bufferram;
    if (cached < 0)
        cached = 0;

    for (lru = LRU_BASE; lru < NR_LRU_LISTS; lru++)
        pages[lru] = global_node_page_state(NR_LRU_BASE + lru);

    available = si_mem_available() {
        long available;
        unsigned long pagecache;
        unsigned long wmark_low = 0;
        unsigned long reclaimable;
        struct zone *zone;

        for_each_zone(zone) {
            wmark_low += low_wmark_pages(zone) {
                return wmark_pages(z, WMARK_LOW) {
                    return z->_watermark[w] + z->watermark_boost;
                }
            }
        }

        /* Estimate the amount of memory available for userspace allocations,
        * without causing swapping or OOM. */
        available = global_zone_page_state(NR_FREE_PAGES) - totalreserve_pages;

        /* Not all the page cache can be freed, otherwise the system will
        * start swapping or thrashing. Assume at least half of the page
        * cache, or the low watermark worth of cache, needs to stay. */
        pagecache = global_node_page_state(NR_ACTIVE_FILE) +
            global_node_page_state(NR_INACTIVE_FILE);
        pagecache -= min(pagecache / 2, wmark_low);
        available += pagecache;

        /* Part of the reclaimable slab and other kernel memory consists of
        * items that are in use, and cannot be freed. Cap this estimate at the
        * low watermark. */
        reclaimable = global_node_page_state_pages(NR_SLAB_RECLAIMABLE_B) +
            global_node_page_state(NR_KERNEL_MISC_RECLAIMABLE);
        reclaimable -= min(reclaimable / 2, wmark_low);
        available += reclaimable;

        if (available < 0)
            available = 0;
        return available;
    }

    sreclaimable = global_node_page_state_pages(NR_SLAB_RECLAIMABLE_B);
    sunreclaim = global_node_page_state_pages(NR_SLAB_UNRECLAIMABLE_B);

    show_val_kb(m, "MemTotal:       ", i.totalram);
    show_val_kb(m, "MemFree:        ", i.freeram);
    show_val_kb(m, "MemAvailable:   ", available);
    show_val_kb(m, "Buffers:        ", i.bufferram);
    show_val_kb(m, "Cached:         ", cached);
    show_val_kb(m, "SwapCached:     ", total_swapcache_pages());
    show_val_kb(m, "Active:         ", pages[LRU_ACTIVE_ANON] + pages[LRU_ACTIVE_FILE]);
    show_val_kb(m, "Inactive:       ", pages[LRU_INACTIVE_ANON] + pages[LRU_INACTIVE_FILE]);
    show_val_kb(m, "Active(anon):   ", pages[LRU_ACTIVE_ANON]);
    show_val_kb(m, "Inactive(anon): ", pages[LRU_INACTIVE_ANON]);
    show_val_kb(m, "Active(file):   ", pages[LRU_ACTIVE_FILE]);
    show_val_kb(m, "Inactive(file): ", pages[LRU_INACTIVE_FILE]);
    show_val_kb(m, "Unevictable:    ", pages[LRU_UNEVICTABLE]);
    show_val_kb(m, "Mlocked:        ", global_zone_page_state(NR_MLOCK));

#ifdef CONFIG_HIGHMEM
    show_val_kb(m, "HighTotal:      ", i.totalhigh);
    show_val_kb(m, "HighFree:       ", i.freehigh);
    show_val_kb(m, "LowTotal:       ", i.totalram - i.totalhigh);
    show_val_kb(m, "LowFree:        ", i.freeram - i.freehigh);
#endif

#ifndef CONFIG_MMU
    show_val_kb(m, "MmapCopy:       ", (unsigned long)atomic_long_read(&mmap_pages_allocated));
#endif

    show_val_kb(m, "SwapTotal:      ", i.totalswap);
    show_val_kb(m, "SwapFree:       ", i.freeswap);
#ifdef CONFIG_ZSWAP
    show_val_kb(m, "Zswap:          ", zswap_total_pages());
    seq_printf(m,  "Zswapped:       %8lu kB\n", (unsigned long)atomic_long_read(&zswap_stored_pages) << (PAGE_SHIFT - 10));
#endif
    show_val_kb(m, "Dirty:          ", global_node_page_state(NR_FILE_DIRTY));
    show_val_kb(m, "Writeback:      ", global_node_page_state(NR_WRITEBACK));
    show_val_kb(m, "AnonPages:      ", global_node_page_state(NR_ANON_MAPPED));
    show_val_kb(m, "Mapped:         ", global_node_page_state(NR_FILE_MAPPED));
    show_val_kb(m, "Shmem:          ", i.sharedram);
    show_val_kb(m, "KReclaimable:   ", sreclaimable + global_node_page_state(NR_KERNEL_MISC_RECLAIMABLE));
    show_val_kb(m, "Slab:           ", sreclaimable + sunreclaim);
    show_val_kb(m, "SReclaimable:   ", sreclaimable);
    show_val_kb(m, "SUnreclaim:     ", sunreclaim);
    seq_printf(m, "KernelStack:    %8lu kB\n", global_node_page_state(NR_KERNEL_STACK_KB));
#ifdef CONFIG_SHADOW_CALL_STACK
    seq_printf(m, "ShadowCallStack:%8lu kB\n", global_node_page_state(NR_KERNEL_SCS_KB));
#endif
    show_val_kb(m, "PageTables:     ", global_node_page_state(NR_PAGETABLE));
    show_val_kb(m, "SecPageTables:  ", global_node_page_state(NR_SECONDARY_PAGETABLE));

    show_val_kb(m, "NFS_Unstable:   ", 0);
    show_val_kb(m, "Bounce:         ", 0);
    show_val_kb(m, "WritebackTmp:   ", 0);
    show_val_kb(m, "CommitLimit:    ", vm_commit_limit());
    show_val_kb(m, "Committed_AS:   ", committed);
    seq_printf(m, "VmallocTotal:   %8lu kB\n", (unsigned long)VMALLOC_TOTAL >> 10);
    show_val_kb(m, "VmallocUsed:    ", vmalloc_nr_pages());
    show_val_kb(m, "VmallocChunk:   ", 0ul);
    show_val_kb(m, "Percpu:         ", pcpu_nr_pages());

    memtest_report_meminfo(m);

#ifdef CONFIG_MEMORY_FAILURE
    seq_printf(m, "HardwareCorrupted: %5lu kB\n", atomic_long_read(&num_poisoned_pages) << (PAGE_SHIFT - 10));
#endif

#ifdef CONFIG_TRANSPARENT_HUGEPAGE
    show_val_kb(m, "AnonHugePages:  ", global_node_page_state(NR_ANON_THPS));
    show_val_kb(m, "ShmemHugePages: ", global_node_page_state(NR_SHMEM_THPS));
    show_val_kb(m, "ShmemPmdMapped: ", global_node_page_state(NR_SHMEM_PMDMAPPED));
    show_val_kb(m, "FileHugePages:  ", global_node_page_state(NR_FILE_THPS));
    show_val_kb(m, "FilePmdMapped:  ", global_node_page_state(NR_FILE_PMDMAPPED));
#endif

#ifdef CONFIG_CMA
    show_val_kb(m, "CmaTotal:       ", totalcma_pages);
    show_val_kb(m, "CmaFree:        ", global_zone_page_state(NR_FREE_CMA_PAGES));
#endif

#ifdef CONFIG_UNACCEPTED_MEMORY
    show_val_kb(m, "Unaccepted:     ", global_zone_page_state(NR_UNACCEPTED));
#endif
    show_val_kb(m, "Balloon:        ", global_node_page_state(NR_BALLOON_PAGES));

    hugetlb_report_meminfo(m);

    arch_report_meminfo(m);

    return 0;
}
```

## /proc/pagetypeinfo

```c
proc_create_seq("pagetypeinfo", 0400, NULL, &pagetypeinfo_op);
```

## /proc/vmstat

```c
proc_create_seq("vmstat", 0444, NULL, &vmstat_op);

static const struct seq_operations vmstat_op = {
    .start    = vmstat_start,
    .next    = vmstat_next,
    .stop    = vmstat_stop,
    .show    = vmstat_show,
};

```

## /proc/zoneinfo

```c
proc_create_seq("zoneinfo", 0444, NULL, &zoneinfo_op);

static const struct seq_operations zoneinfo_op = {
    .start    = frag_start, /* iterate over all zones. The same as in
                   * fragmentation. */
    .next    = frag_next,
    .stop    = frag_stop,
    .show    = zoneinfo_show,
};

static int zoneinfo_show(struct seq_file *m, void *arg)
{
    pg_data_t *pgdat = (pg_data_t *)arg;
    walk_zones_in_node(m, pgdat, false, false, zoneinfo_show_print);
    return 0;
}

static void zoneinfo_show_print(struct seq_file *m, pg_data_t *pgdat,
                            struct zone *zone)
{
    int i;
    seq_printf(m, "Node %d, zone %8s", pgdat->node_id, zone->name);
    if (is_zone_first_populated(pgdat, zone)) {
        seq_printf(m, "\n  per-node stats");
        for (i = 0; i < NR_VM_NODE_STAT_ITEMS; i++) {
            unsigned long pages = node_page_state_pages(pgdat, i);

            if (vmstat_item_print_in_thp(i))
                pages /= HPAGE_PMD_NR;
            seq_printf(m, "\n      %-12s %lu", node_stat_name(i),
                   pages);
        }
    }
    seq_printf(m,
           "\n  pages free     %lu"
           "\n        boost    %lu"
           "\n        min      %lu"
           "\n        low      %lu"
           "\n        high     %lu"
           "\n        promo    %lu"
           "\n        spanned  %lu"
           "\n        present  %lu"
           "\n        managed  %lu"
           "\n        cma      %lu",
           zone_page_state(zone, NR_FREE_PAGES),
           zone->watermark_boost,
           min_wmark_pages(zone),
           low_wmark_pages(zone),
           high_wmark_pages(zone),
           promo_wmark_pages(zone),
           zone->spanned_pages,
           zone->present_pages,
           zone_managed_pages(zone),
           zone_cma_pages(zone));

    seq_printf(m,
           "\n        protection: (%ld",
           zone->lowmem_reserve[0]);
    for (i = 1; i < ARRAY_SIZE(zone->lowmem_reserve); i++)
        seq_printf(m, ", %ld", zone->lowmem_reserve[i]);
    seq_putc(m, ')');

    /* If unpopulated, no other information is useful */
    if (!populated_zone(zone)) {
        seq_putc(m, '\n');
        return;
    }

    for (i = 0; i < NR_VM_ZONE_STAT_ITEMS; i++)
        seq_printf(m, "\n      %-12s %lu", zone_stat_name(i),
               zone_page_state(zone, i));

#ifdef CONFIG_NUMA
    fold_vm_zone_numa_events(zone);
    for (i = 0; i < NR_VM_NUMA_EVENT_ITEMS; i++)
        seq_printf(m, "\n      %-12s %lu", numa_stat_name(i),
               zone_numa_event_state(zone, i));
#endif

    seq_printf(m, "\n  pagesets");
    for_each_online_cpu(i) {
        struct per_cpu_pages *pcp;
        struct per_cpu_zonestat __maybe_unused *pzstats;

        pcp = per_cpu_ptr(zone->per_cpu_pageset, i);
        seq_printf(m,
               "\n    cpu: %i"
               "\n              count:    %i"
               "\n              high:     %i"
               "\n              batch:    %i"
               "\n              high_min: %i"
               "\n              high_max: %i",
               i,
               pcp->count,
               pcp->high,
               pcp->batch,
               pcp->high_min,
               pcp->high_max);
#ifdef CONFIG_SMP
        pzstats = per_cpu_ptr(zone->per_cpu_zonestats, i);
        seq_printf(m, "\n  vm stats threshold: %d",
                pzstats->stat_threshold);
#endif
    }
    seq_printf(m,
           "\n  node_unreclaimable:  %u"
           "\n  start_pfn:           %lu"
           "\n  reserved_highatomic: %lu"
           "\n  free_highatomic:     %lu",
           atomic_read(&pgdat->kswapd_failures) >= MAX_RECLAIM_RETRIES,
           zone->zone_start_pfn,
           zone->nr_reserved_highatomic,
           zone->nr_free_highatomic);
    seq_putc(m, '\n');
}
```

# Tuning

## Tunable Parameters

Option | Default | Description
:-- | :-: | :-:
vm.dirty_background_bytes | 0 | Amount of dirty memory to trigger pdflush background write-back
vm.dirty_background_ratio | 10 | Percentage of dirty system memory to trigger pdflush background write-back
vm.dirty_bytes | 0 | Amount of dirty memory that causes a writing process to start write-back
vm.dirty_ratio | 20 | Ratio of dirty system memory to cause a writing process to begin write-back
vm.dirty_expire_centisecs | 3,000 | Minimum time for dirty memory to be eligible for pdflush (promotes write cancellation)
vm.dirty_writeback_centisecs | 500 | pdflush wake-up interval (0 to disable)
vm.min_free_kbytes | dynamic | sets the desired free memory amount (some kernel atomic allocations can consume this)
vm.watermark_scale_factor | 10 | The distance between kswapd watermarks (min, low, high) that control waking up and sleeping (unit is fractions of 10000, such that 10 means 0.1% of system memory)
vm.watermark_boost_factor | 5000 | How far past the high watermark kswapd scans when memory is fragmented (recent fragmentation events occurred); unit is fractions of 10000, so 5000 means kswapd can boost up to 150% of the high watermark; 0 to disable
vm.percpu_pagelist_fraction | 0 | This can override the default max fraction of pages that can be allocated to per-cpu page lists (a value of 10 limits to 1/10th of pages)
vm.overcommit_memory | 0 | 0 = Use a heuristic to allow reasonable overcommits; 1 = always overcommit; 2 = don’t overcommit
vm.swappiness | 60 | The degree to favor swapping (paging) for freeing memory over reclaiming it from the page cache
vm.vfs_cache_pressure | 100 | The degree to reclaim cached directory and inode objects; lower values retain them more; 0 means never reclaim—can easily lead to out-of-memory conditions
kernel.numa_balancing | 1 |  Enables automatic NUMA page balancing
kernel.numa_balancing_scan_size_mb  | 256 | How many Mbytes of pages are scanned for each NUMA balancing scan

## Multiple Page Sizes

```sh
echo 50 > /proc/sys/vm/nr_hugepages
grep Huge /proc/meminfo
AnonHugePages: 0 kB
HugePages_Total: 50
HugePages_Free: 50
HugePages_Rsvd: 0
HugePages_Surp: 0
Hugepagesize: 2048 kB
```

```sh
mkdir /mnt/hugetlbfs
mount -t hugetlbfs none /mnt/hugetlbfs -o pagesize=2048K
```

## Allocators

```sh
export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
```

## NUMA Binding

bind processes to NUMA nodes.
```sh
numactl --membind=0 3161
```

## Resource Controls

* **memory.limit_in_bytes**: The maximum allowed user memory, including file cache usage, in bytes
* **memory.memsw.limit_in_bytes**: The maximum allowed memory and swap space, in bytes(when swap is in use)
* **memory.kmem.limit_in_bytes**: The maximum allowed kernel memory, in bytes
* **memory.tcp.limit_in_bytes**: The maximum tcp buffer memory, in bytes.
* **memory.swappiness**: Similar to vm.swappiness described earlier but can be set for a cgroup
* **memory.oom_control**: Can be set to 0, to allow the OOM killer for this cgroup, or 1, to disable it